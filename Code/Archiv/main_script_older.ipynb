{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings \n",
    "\n",
    "import time\n",
    "\n",
    "#import multiprocess as mp\n",
    "from multiprocess import Pool\n",
    "\n",
    "#from joblib import Parallel, delayed\n",
    "\n",
    "#from functools import partial  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "\n",
    "#from statsmodels.tsa.tsatools import lagmat\n",
    "import statsmodels.api as sm\n",
    "#from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "#from sklearn import random_projection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.linear_model import LarsCV\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from dimod import BinaryQuadraticModel\n",
    "from dwave.samplers import SimulatedAnnealingSampler\n",
    "#from dwave.samplers import SteepestDescentSolver\n",
    "#from dwave.preprocessing import roof_duality\n",
    "# from dwave.system import LeapHybridSampler\n",
    "# from dwave.system import DWaveSampler\n",
    "# from dwave.system import EmbeddingComposite\n",
    "# from dimod import ExactSolver\n",
    "# from dwave.samplers import TabuSampler\n",
    "# from dwave.samplers import TreeDecompositionSolver\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobi_optimods.qubo import solve_qubo\n",
    "gp.setParam('OutputFlag', 0)\n",
    "\n",
    "if os.name == 'nt':\n",
    "    import dill\n",
    "    dill.settings['recurse'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-01-09\n"
     ]
    }
   ],
   "source": [
    "###### File for Candidate Model Functions ######\n",
    "### Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import random_projection\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gurobipy as gp\n",
    "gp.setParam('OutputFlag', 0)\n",
    "import os\n",
    "if os.name == 'nt':\n",
    "    import dill\n",
    "    dill.settings['recurse'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _cm import lambda_path, eln, crr, dtr, candidate_models, candidate_models_kf\n",
    "from _fm import bssf, bssf_cv, csr_cv, avg_best_cv, peLASSO_cv, psgd_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate X\n",
    "y = np.random.normal(0, 1, 1000)\n",
    "X = np.random.normal(0, 1, (1000, 100))\n",
    "\n",
    "x_train = np.random.normal(0, 1, (250, 100))\n",
    "x_pred = np.random.normal(0, 1, (250, 100))\n",
    "y_train = np.sum(x_train, axis = 1) + np.random.normal(0, 5, 250)\n",
    "\n",
    "lambda_vec = np.linspace(0.1, 1, 10)\n",
    "alpha_vec = np.linspace(0.1, 1, 5)\n",
    "n_jobs = 1\n",
    "\n",
    "comp_vec = np.array([1, 2, 3, 4, 5])\n",
    "rep_range = range(1, 100)\n",
    "\n",
    "vec_depth = np.array([3, 4, 5])\n",
    "\n",
    "kfolds = 5\n",
    "ran_st = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_params = [\n",
    "    (\"eln\", {\"n_lambda\": 10, \"alpha_vec\": np.linspace(0.1, 1, 5), \"n_jobs\": 1}),\n",
    "    (\"crr\", {\"comp_vec\": np.array([1, 2, 3, 4, 5]), \"rep_range\": np.arange(1, 500), \"n_jobs\": 1}),\n",
    "    (\"dtr\", {\"vec_depth\": np.array([3, 4, 5]), \"n_jobs\": 1}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytmp, ptemp = candidate_models_kf(y, X, kfolds, models_params, ran_st, n_jobs = 5)\n",
    "ytmp.shape\n",
    "ptemp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 1, 1000)\n",
    "\n",
    "alpha = 100000.0\n",
    "timeout = 10.0\n",
    "method = \"gurobi\"\n",
    "k = 10\n",
    "vec_k = np.arange(1, 21)\n",
    "kfolds = 5\n",
    "ran_st = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n",
      "8\n",
      "10\n",
      "9\n",
      "9\n",
      "10\n",
      "6\n",
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Repeat X times and save the third element of the tuple\n",
    "for i in range(10):\n",
    "    \n",
    "    cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "    cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "    y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 1000)\n",
    "    \n",
    "    _, _, res = bssf_cv(y_train, cf_train, cf_pred, alpha, vec_k, 1000, \"dwave\", kfolds, ran_st)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "10.0\n",
      "3.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# Repeat X times and save the third element of the tuple\n",
    "for i in range(10):\n",
    "    \n",
    "    cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "    cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "    y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 1000)\n",
    "    \n",
    "    _, _, res = bssf_cv(y_train, cf_train, cf_pred, alpha, vec_k, timeout, \"qcbo\", kfolds, ran_st)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "3.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# Repeat X times and save the third element of the tuple\n",
    "for i in range(10):\n",
    "    \n",
    "    cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "    cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "    y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 1000)\n",
    "    \n",
    "    _, _, res = bssf_cv(y_train, cf_train, cf_pred, alpha, vec_k, timeout, \"gurobi\", kfolds, ran_st)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.40004012,  2.12652452,  2.20705825, -0.37040808,  0.46301774,\n",
       "         0.38656105, -0.17330651, -2.77591771, -0.54067072,  3.00135041,\n",
       "        -0.99663277,  4.05175266,  0.91668638,  2.6897929 , -0.5425837 ,\n",
       "         0.54689144,  2.25107761, -0.05494685, -0.91605699,  0.64594409,\n",
       "         1.3539885 ,  0.61635737, -2.64987197,  2.98113764,  0.17139907,\n",
       "        -0.92923178,  1.00710263, -1.2507955 ,  5.1030572 , -2.60170425,\n",
       "         5.11305087, -1.64234187, -0.09297338, -1.25679768,  1.49545315,\n",
       "         4.15780615,  3.3976011 ,  0.37283797, -0.5386995 ,  0.10172649,\n",
       "         0.95888073,  2.39005283,  0.59151579,  2.79706773, -0.86911969,\n",
       "        -0.40876665,  1.91150542,  0.91558946, -0.20908597,  1.19303609,\n",
       "         1.90675021, -0.75114428,  2.92558859, -3.36556777,  1.79995153,\n",
       "         0.16105071,  2.0007541 , -0.40409813,  4.07750536,  5.2057992 ,\n",
       "         2.43324853,  4.11552116,  3.12624258,  0.43948895, -0.56868746,\n",
       "         1.9099868 , -0.221567  , -1.13897882,  0.53979534,  1.87947425,\n",
       "         3.90798623,  1.19871939,  1.81722844, -0.38660792, -1.8900606 ,\n",
       "         1.56039499,  1.9125897 ,  5.20269188,  0.38916501,  1.74082571,\n",
       "         0.15263645,  1.07396592,  2.69764084,  0.86411562, -2.28936063,\n",
       "         4.98432739,  0.84808459, -0.44068625,  0.68815314,  3.84309932,\n",
       "         2.8856255 , -0.85880862, -1.41129827, -2.75901719,  1.09496695,\n",
       "        -2.29112577, -0.26573001,  2.9403468 , -0.21077769,  1.68986474,\n",
       "        -0.2021075 ,  1.3178923 , -1.04197668,  0.06661889,  2.32878278,\n",
       "        -4.73507593,  1.64545839,  2.75085127, -1.58672089,  3.37163958,\n",
       "         0.53247627, -0.29595676,  0.38580268,  1.48508578,  0.73287999,\n",
       "        -2.54390096,  0.92029693, -1.00062373,  1.66800444, -1.78123613,\n",
       "        -3.98536993,  0.17485352, -0.06389505,  1.33280016,  3.13412425,\n",
       "         0.5820631 , -1.17032262,  0.9607974 , -2.40970987, -0.12231168,\n",
       "        -1.34407914, -3.22758151,  3.55740766, -0.75873552, -3.73738657,\n",
       "         6.85946155,  0.79240865, -2.00687994,  0.17159604,  2.72676515,\n",
       "        -0.59413054, -0.96931949,  1.47483538,  4.58121664,  4.09250697,\n",
       "         2.92346477,  0.26145834,  1.82114202,  0.87963383, -0.97388763,\n",
       "        -1.03803258,  0.86937444,  0.03207104,  0.38093422,  2.89605666,\n",
       "         0.13594255,  1.47166307,  0.27742853, -0.74384028,  0.36915856,\n",
       "        -0.21693773, -3.4996282 ,  2.58636822,  1.61395741, -0.01463108,\n",
       "        -1.31619224, -4.23437994,  1.9827919 , -1.17760822, -0.79885917,\n",
       "         1.4755551 ,  2.69810067, -2.84140822, -3.37160546,  1.48199306,\n",
       "        -1.43780909,  0.61499638,  2.2655249 ,  4.43548603, -4.3278245 ,\n",
       "        -4.87788636,  2.10354426,  1.22227579, -0.75456218,  5.01679789,\n",
       "         2.97882863,  1.60186086,  3.03346211,  1.45534603,  3.38108933,\n",
       "         4.95713095,  1.13288173,  0.27931451, -1.49633295,  0.8289054 ,\n",
       "         3.01151286,  1.83694459,  1.0414807 ,  4.8179109 ,  2.39488242,\n",
       "        -0.97267958, -1.69998824,  1.73687383,  0.45397866,  3.17384447,\n",
       "         3.40439324,  1.70427775, -3.65441415, -3.13715211,  3.28619706,\n",
       "         1.88837632, -2.35199153,  1.66823177,  2.65132761,  0.75386205,\n",
       "         2.19055829,  2.94578341, -0.53531133,  3.10763066, -2.92164494,\n",
       "        -0.80061839, -1.49919429, -1.75899167,  2.40235757,  1.03564928,\n",
       "         3.86387369, -1.03536056, -7.84934252,  0.83578872,  0.75735411,\n",
       "         1.9290471 ,  0.4989397 , -1.06701085, -0.57431748,  0.01954641,\n",
       "         1.58741963, -3.55874843, -1.0247273 ,  1.19522521,  3.24407743,\n",
       "         3.0654157 , -2.00111272, -2.70098677, -1.93005079, -2.43410422,\n",
       "         3.35713239,  2.01448348, -0.01396582, -3.70242777,  2.43946604]),\n",
       " 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.random.normal(0, 1, (250, 100))\n",
    "x_pred = np.random.normal(0, 1, (250, 100))\n",
    "y_train = np.sum(x_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 250)\n",
    "\n",
    "vec_k = np.arange(1, 21)\n",
    "sampling = True\n",
    "\n",
    "csr_cv(y_train, x_train, x_pred, vec_k, sampling, kfolds, ran_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kfolds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(cf_train[:, :\u001b[38;5;241m10\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m250\u001b[39m)\n\u001b[1;32m      5\u001b[0m vec_k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m avg_best_cv(y_train, cf_train, cf_pred, vec_k, \u001b[43mkfolds\u001b[49m, ran_st)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kfolds' is not defined"
     ]
    }
   ],
   "source": [
    "cf_train = np.random.normal(0, 1, (250, 100))\n",
    "cf_pred = np.random.normal(0, 1, (250, 100))\n",
    "y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 250)\n",
    "\n",
    "vec_k = np.arange(1, 21)\n",
    "kfolds = 5\n",
    "\n",
    "avg_best_cv(y_train, cf_train, cf_pred, vec_k, kfolds, ran_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-6.67128545e+00,  2.43977881e-01, -3.86148063e+00,  6.45891251e+00,\n",
       "        -1.14195068e+00, -3.44267475e+00,  2.07649525e-01,  2.02386123e+00,\n",
       "        -4.10569493e+00, -4.33981622e+00,  4.24252587e+00, -1.62541923e-01,\n",
       "        -2.39377466e+00,  2.68896678e+00, -8.01676105e-02,  5.15823895e-01,\n",
       "         4.38948808e+00, -3.05990012e+00, -2.80934817e+00, -5.53419613e+00,\n",
       "         5.51675269e+00,  3.50849958e+00, -2.01758678e+00,  8.49603264e-01,\n",
       "        -7.87505841e-01, -8.30926390e+00, -2.61800869e+00,  1.00424231e-01,\n",
       "        -1.02357986e+00,  4.85788906e-01, -2.83100575e+00, -3.30147559e+00,\n",
       "        -6.12832780e-01, -4.41792478e+00,  2.35726146e-01, -3.43467342e+00,\n",
       "         5.77925749e+00,  2.90882266e+00,  6.12044957e+00, -1.36783991e+00,\n",
       "         8.08589610e-01,  2.44373952e+00,  4.96084511e-01,  1.40801441e-01,\n",
       "        -4.70638046e+00, -3.14178752e+00, -6.35969340e+00,  7.13693886e+00,\n",
       "        -4.90515325e-01,  1.31393155e+00,  1.61525494e+00, -4.75638881e-01,\n",
       "        -3.33895773e+00, -2.21662054e+00,  1.40019474e+00, -2.35017924e+00,\n",
       "         4.48737615e+00,  4.25039537e+00, -1.01647008e+00,  1.16710197e+00,\n",
       "         3.01554713e+00, -9.52519639e+00,  3.29110790e+00,  6.23093315e-01,\n",
       "         2.25510819e+00,  3.37263200e+00,  8.87412838e-01, -3.58079976e-02,\n",
       "         1.05887292e+00, -9.80463871e-02,  3.21140055e+00,  2.64765199e+00,\n",
       "        -2.68097279e-01, -4.66848674e+00, -6.53460304e-01, -1.64679061e-01,\n",
       "        -1.25757466e+00, -5.44609792e-01,  3.87778578e+00,  1.10365136e+00,\n",
       "        -5.13300602e+00, -3.88971759e+00, -5.72951154e-01,  4.79210090e+00,\n",
       "         4.29086791e-02, -1.31706877e+00,  1.79529222e-01,  1.46414856e-02,\n",
       "         4.03256318e+00,  2.18352833e+00, -3.81216950e+00, -2.91191814e+00,\n",
       "         1.25623554e+00,  4.75722494e+00, -2.54960252e+00,  2.12428710e+00,\n",
       "         3.99535351e+00, -1.18618470e+00,  3.51241689e+00, -4.66251116e-01,\n",
       "        -1.86420180e+00, -4.01406723e+00, -4.33528901e+00,  2.52046820e+00,\n",
       "         3.18192882e+00, -5.70959153e-01,  1.36246436e-01, -1.78941323e+00,\n",
       "         4.81170538e+00, -1.70542040e+00, -2.64231579e+00,  8.81021798e-01,\n",
       "        -8.30596726e-01,  2.30949508e-01,  1.18020658e+00, -3.35241971e+00,\n",
       "        -2.81662909e+00,  6.13955209e+00, -1.20189575e-01,  3.61163451e+00,\n",
       "         1.25334177e+00, -7.91148174e+00,  1.13317551e+00, -4.00926416e+00,\n",
       "         1.46655697e+00, -6.97721121e+00,  3.41599268e+00,  9.62513258e-01,\n",
       "        -1.72553727e+00,  2.51916269e+00,  3.37685413e+00, -3.24750614e-01,\n",
       "         1.12000302e+00, -4.93804659e+00, -2.79935617e+00, -2.79883555e+00,\n",
       "        -8.63134234e-01, -7.23389511e+00, -9.92436497e-01,  3.17646054e+00,\n",
       "         1.27982751e+00,  2.60927518e-01,  4.61362738e-01, -4.98966944e+00,\n",
       "        -1.97540654e+00,  9.93278799e+00,  6.51671990e+00, -5.29463209e+00,\n",
       "         1.67176066e+00, -2.17868402e+00,  2.49858871e+00, -2.24965510e+00,\n",
       "        -9.41149471e+00, -4.83920257e+00,  9.41251752e-02, -6.10609932e-01,\n",
       "         2.79122533e+00,  1.87434610e-01,  1.21687149e+00, -2.86372617e+00,\n",
       "         4.46468152e+00,  4.53417475e+00, -1.57760843e+00,  1.63001318e+00,\n",
       "        -2.58333241e-01,  2.20137660e+00, -6.93978045e+00, -6.49273766e+00,\n",
       "        -3.79023037e+00,  1.29042412e+00, -1.76119214e+00,  9.82680407e-01,\n",
       "        -2.15883538e+00,  1.32151002e+00, -3.87706311e+00,  2.53730494e-01,\n",
       "        -1.23858418e-03,  3.40304834e+00, -2.49552416e+00, -4.29615951e+00,\n",
       "        -1.76903166e+00, -3.46552653e+00, -6.55953591e-01, -2.13119777e+00,\n",
       "        -6.70618594e+00, -3.93425812e+00,  3.95588567e-01, -2.39484181e+00,\n",
       "         1.24632427e+00,  2.09271463e+00,  4.30495588e-01,  8.37584454e-01,\n",
       "        -1.08034935e+00,  1.01684304e+00,  2.12251677e+00, -1.80483929e+00,\n",
       "         2.90252907e+00, -3.46943690e+00, -3.92092945e+00,  2.56855901e+00,\n",
       "        -2.82662397e+00, -3.21685931e+00,  2.06224763e+00,  6.51073283e-01,\n",
       "        -1.56000964e+00, -6.99751431e-01, -1.47192301e+00, -1.47563299e+00,\n",
       "         4.06101608e+00,  2.81879033e-01, -4.84148683e+00,  3.05516331e+00,\n",
       "         1.84949364e+00,  2.73468997e+00, -3.88567791e+00, -3.49022542e+00,\n",
       "         2.92279410e+00, -1.33208190e+00, -2.38598751e-01,  3.23127664e+00,\n",
       "        -2.19529448e+00,  3.34961381e+00, -8.16648551e-01, -2.60548369e+00,\n",
       "         9.28767698e-01,  2.35241935e-01,  7.40206170e-01,  5.19904517e+00,\n",
       "         6.49226262e+00, -8.05078475e-01,  4.97548199e+00,  2.31634124e-01,\n",
       "        -2.72797161e+00, -1.24727459e+00,  2.74264403e+00,  2.54679350e+00,\n",
       "        -1.16375262e-01, -5.99392249e-01, -1.37464216e+00,  5.35817003e-01,\n",
       "         1.15572474e+00, -1.00893925e+00,  4.08056717e+00, -3.92950015e+00,\n",
       "         1.22322199e+00,  3.71338342e-01, -1.83545611e+00,  8.86178551e+00,\n",
       "         2.80424780e+00,  4.91209452e-01,  1.57278242e+00,  1.74881587e+00,\n",
       "         3.52361529e+00, -2.84142649e+00, -2.21808762e+00,  4.64244373e+00,\n",
       "        -1.37660307e+00, -4.81475924e-01,  3.54850780e+00, -7.52026438e-01,\n",
       "        -2.79164203e+00, -1.36727126e+00,  1.41373658e-01,  2.97257615e+00,\n",
       "        -5.08029389e+00,  1.28228260e+00,  3.29369143e+00,  1.01462042e+00,\n",
       "        -2.07872206e+00,  1.84441954e+00,  2.23418941e+00, -1.78034618e+00,\n",
       "         1.45912511e-01, -1.28624728e+00, -2.16678379e+00, -1.95182148e+00,\n",
       "         1.97347000e+00, -6.24277853e+00,  4.76847351e+00, -3.11409968e+00,\n",
       "        -3.15371409e+00, -5.10061871e-01,  3.40673838e+00, -4.92873168e-01,\n",
       "         2.38903610e+00,  2.47353894e+00,  7.32507627e-01, -1.78716238e+00,\n",
       "         2.26557274e+00,  4.23889187e+00, -9.91781737e-01, -4.24560260e+00,\n",
       "        -1.00987816e+00,  3.06985935e+00,  3.21391514e+00, -6.40706589e+00,\n",
       "         2.59310550e+00,  4.81422601e-01,  4.76995492e-03, -4.77181213e+00,\n",
       "         8.70218417e-01, -1.40236379e+00,  4.65386267e+00, -9.39567749e-01,\n",
       "        -1.75647093e+00, -3.88154359e+00,  2.28824923e+00, -1.86031643e+00,\n",
       "        -4.51357916e+00,  4.40682228e+00, -2.75530416e+00,  2.53486496e+00,\n",
       "        -8.90614008e+00,  1.19260158e-01, -7.74330357e-01, -7.45201899e-01,\n",
       "         8.70757329e-01,  5.17214151e-01,  1.10521752e+00,  2.38071349e+00,\n",
       "         1.40297622e+00, -3.23963420e+00,  3.11653453e+00,  4.64062404e+00,\n",
       "        -1.14228223e-01,  1.62494458e+00,  2.14727026e+00,  9.43082262e-01,\n",
       "        -3.18574636e-02,  1.51477287e+00,  1.49359515e+00,  2.89134789e+00,\n",
       "         2.75664255e+00,  4.30531746e+00, -1.04975686e+00, -4.07931096e+00,\n",
       "         1.48244355e+00,  1.75460530e+00,  1.56319095e+00,  1.82925363e-01,\n",
       "        -3.71535764e+00,  4.32939902e-01,  3.86745317e+00,  5.91168845e-01,\n",
       "         2.57811888e-01,  2.83033306e+00, -2.31767497e+00,  3.49541802e+00,\n",
       "         4.95062370e+00,  1.47314796e+00, -1.32143367e+00,  2.81230822e+00,\n",
       "        -2.97942496e-01,  6.74642911e-01, -3.00607283e-02,  3.04162158e+00,\n",
       "        -4.51912344e+00, -4.69267563e+00, -2.53951920e-01,  1.58388366e+00,\n",
       "        -3.81956226e+00, -4.09369397e-01,  2.82149917e+00, -2.54184634e+00,\n",
       "         2.24903257e-01,  2.49476865e+00,  2.87845671e+00,  2.05759399e+00,\n",
       "        -4.36855621e+00,  2.59746106e+00,  3.46779617e+00,  2.55864250e+00,\n",
       "        -2.68668254e+00, -3.04997197e+00, -9.36443332e-01, -5.79549888e-01,\n",
       "         2.24207725e+00, -1.38610995e+00,  3.94020521e+00, -1.10981328e+00,\n",
       "        -5.57416135e-01, -9.23139976e-01, -2.36321303e-01,  2.06447580e+00,\n",
       "        -1.77517936e+00,  2.51778592e+00,  4.31592341e+00, -5.11089761e-01,\n",
       "        -2.45440094e+00, -6.61421318e-01, -8.12558510e-01,  8.31505003e-01,\n",
       "         1.38781575e+00,  3.15898348e-01, -1.81594047e+00,  1.20297492e+00,\n",
       "        -3.13375342e+00, -6.81395759e+00,  5.03691585e-01,  4.27243374e+00,\n",
       "         3.73000943e+00,  4.90414486e-01,  2.15960684e+00, -2.79976070e+00,\n",
       "        -1.68686786e+00, -1.47254572e+00,  2.74766654e+00, -1.67226695e+00,\n",
       "         8.02588619e+00,  9.09740507e-01, -3.62393776e+00,  8.53839467e-01,\n",
       "         9.11529743e-01,  1.49877673e+00,  3.74282151e+00, -3.09477520e-01,\n",
       "         7.54727220e-02, -4.03170283e+00,  5.26863771e+00,  3.43818898e+00,\n",
       "        -4.57532975e+00, -1.75053157e+00,  1.95094198e+00,  2.29528311e+00,\n",
       "        -9.24742277e-01, -3.00245477e-02, -3.18362151e-02, -3.60751525e+00,\n",
       "         4.53582929e+00, -4.53224578e+00, -3.91826179e+00, -2.03742642e+00,\n",
       "         3.90284635e+00, -6.67755116e+00, -1.89312314e+00,  2.33149211e+00,\n",
       "         2.58673735e+00, -1.32834204e-01, -1.11276761e+00,  1.01905846e+00,\n",
       "        -3.25480270e+00, -3.11329610e+00,  3.30935608e+00, -5.27247251e-02,\n",
       "         1.31638439e+00, -1.13058257e+00,  4.60063353e+00,  2.45936972e+00,\n",
       "         1.98658117e-02,  1.79173726e-02, -4.31944352e+00,  5.18232120e+00,\n",
       "        -1.06943501e+00,  9.37594634e-02,  5.57797745e+00,  4.08304001e+00,\n",
       "        -3.00986085e+00,  7.11162287e+00, -2.45822831e+00, -3.00904219e+00,\n",
       "         1.00468735e-01, -4.03717127e-01,  3.77292695e+00,  1.15331797e+00,\n",
       "         6.88566187e+00,  4.09591288e+00, -2.86031787e+00, -1.85507218e+00,\n",
       "        -1.28252288e+00, -3.85861596e-01, -1.79242647e+00, -7.80093812e+00,\n",
       "        -1.21283699e+00, -1.66594697e+00, -6.70599231e-01, -2.97259733e+00,\n",
       "         2.02803592e-01, -4.85700587e+00,  2.49026807e+00, -2.62046211e+00,\n",
       "        -1.07821914e+00, -4.27558233e-01, -1.08432385e+00,  1.05835301e+01,\n",
       "        -1.13173220e+00,  2.16697983e+00,  2.41999340e+00, -1.48736603e+00,\n",
       "         3.63816392e+00, -8.34991776e-01,  9.07655936e-01,  1.19410763e+00,\n",
       "        -4.12987994e+00,  1.00802176e+00,  2.55823778e+00,  3.45322727e+00,\n",
       "        -2.26961619e+00,  2.53459889e+00, -1.90813401e+00,  1.54590814e+00,\n",
       "        -2.97801879e+00, -5.60317648e+00, -5.69650216e+00,  2.26012747e+00,\n",
       "        -1.46469895e+00,  1.71534994e+00,  2.27588769e-01, -3.97874477e+00,\n",
       "         4.33382373e+00,  9.55933973e+00, -1.98007513e-02,  1.16852755e+00,\n",
       "        -1.38865932e+00,  3.34868728e-01,  6.15008891e-02,  2.22506439e+00,\n",
       "         2.61029597e+00,  3.67619271e+00, -4.86116603e+00, -1.50553632e+00,\n",
       "         6.64507340e-01,  3.34584372e+00, -1.75491678e-01, -4.94072227e+00,\n",
       "        -4.53876678e+00, -1.65970714e+00,  1.86909900e+00, -9.10099689e-01,\n",
       "        -2.82550101e-01, -2.85070963e+00,  4.13354539e+00,  1.46468676e+00,\n",
       "         6.58932609e-01, -4.71084975e+00, -4.10642321e-01,  5.99429160e+00,\n",
       "         2.61593920e+00,  5.08869454e+00,  2.14321654e+00,  2.63130095e+00,\n",
       "         7.91143613e-01,  3.18559201e+00, -3.12347859e+00,  1.04462576e+00,\n",
       "         4.30915333e+00, -4.36337801e-01, -3.64337432e+00,  2.84243963e+00,\n",
       "        -6.52674760e-01, -2.13507481e+00, -2.69448451e+00, -2.18499332e-01,\n",
       "         3.37065374e+00,  5.15654760e+00, -1.94658402e+00,  3.17093761e+00,\n",
       "         4.55549767e+00,  3.76446778e+00,  9.10563309e-02, -8.32603087e-01,\n",
       "        -5.43207466e-01, -3.26351044e+00, -3.65831396e+00,  8.40398447e+00,\n",
       "        -2.08005840e+00,  1.26422839e+00,  2.58280016e-01,  5.45289217e+00,\n",
       "         8.13498918e-01, -4.18628365e+00,  1.07444182e+00, -3.52298310e-01,\n",
       "        -5.52689585e+00,  6.71013356e-01,  6.50460409e+00, -2.08927895e+00,\n",
       "        -2.69209059e+00, -6.04577214e+00, -2.41656756e+00,  2.75981818e+00,\n",
       "         6.29444836e-01,  6.87872894e+00, -1.27753355e+00, -4.57570409e+00,\n",
       "         3.04349638e+00, -2.75442973e+00, -3.37474695e+00,  2.37729121e+00,\n",
       "         1.05219785e+00, -1.84185246e+00,  5.83902542e-01, -4.22650406e+00,\n",
       "         2.57850249e+00, -1.04271562e+00, -4.77734103e+00,  9.40007284e-01,\n",
       "        -2.21594228e+00,  5.27719540e+00,  3.52122064e+00, -5.05436929e+00,\n",
       "        -2.33576013e+00, -4.25343488e+00,  1.84775988e+00, -2.39794494e+00,\n",
       "        -1.05760771e+00, -4.92469628e+00, -1.04355336e+00, -5.44010518e+00,\n",
       "        -5.49413182e+00,  1.43546727e+00,  8.77835212e-02,  3.62637064e+00,\n",
       "        -2.27348974e+00, -6.02714455e-02, -2.80792020e+00,  6.36522098e+00,\n",
       "         3.37181624e+00,  3.43614738e+00,  4.87306694e+00,  1.12820246e+00,\n",
       "        -8.24335959e-01,  2.84499460e+00, -3.34011118e+00, -2.38660843e+00,\n",
       "         6.43246126e+00, -9.76981609e-01,  2.78291532e+00,  5.89332922e-01,\n",
       "         4.95611936e+00, -2.96073508e+00,  5.71112448e-01, -1.39097566e+00,\n",
       "         4.03414374e+00,  5.24929949e-01, -5.47641996e+00, -2.36852858e+00,\n",
       "        -3.79782260e+00,  5.66126924e-01,  2.79911304e-01,  3.80750458e+00,\n",
       "         7.70827508e-01,  7.34677047e-01,  3.94820744e-01, -4.91101422e-01,\n",
       "         3.68360390e+00, -4.24820853e+00, -2.00552443e+00, -1.59851259e+00,\n",
       "        -2.48770467e-01, -5.54320682e+00,  1.16532943e+00, -1.75752334e+00,\n",
       "         2.83076955e+00,  1.51226905e+00,  1.03454567e-01, -4.71457841e-01,\n",
       "         7.09308832e+00, -1.46512737e-01, -1.78013495e+00,  3.11874454e+00,\n",
       "         4.77961118e+00, -2.23682524e+00,  1.70884077e+00, -1.46047051e+00,\n",
       "        -3.83923522e+00,  3.42651734e+00, -4.66753494e+00,  2.28682641e+00,\n",
       "        -1.61835452e+00,  1.56768069e+00, -1.34680841e-01,  1.60508331e+00,\n",
       "         4.67747390e+00, -8.24855980e-01,  3.27778045e+00, -2.37796961e+00,\n",
       "        -7.06785743e-01, -2.21504265e+00,  2.85201059e+00, -1.00047949e+00,\n",
       "        -3.88396590e+00,  1.94673421e-01,  2.52725672e+00, -2.82151736e+00,\n",
       "         2.06130633e+00,  5.83030760e+00,  1.75778464e+00, -1.15250755e-01,\n",
       "         4.94401344e+00,  3.38017480e+00,  9.09172761e-01, -1.41013029e+00,\n",
       "         1.19046072e+00, -1.72945050e+00,  3.06680004e+00, -1.23054918e+00,\n",
       "        -6.11069563e-01, -2.00761609e+00,  8.19269056e+00,  3.55743290e+00,\n",
       "         3.39320780e+00, -1.66422919e+00,  4.63132984e+00,  1.60973997e+00,\n",
       "        -7.35751559e-01, -1.90123298e+00,  5.65522549e+00, -1.80248858e-01,\n",
       "        -1.41379821e-01,  2.08864267e+00, -7.67774563e-01,  4.09944325e+00,\n",
       "         6.37446361e-01, -2.56168441e+00,  3.82656174e+00,  4.37466468e+00,\n",
       "        -1.51918659e+00, -1.69796885e+00,  3.71380953e+00, -2.87832444e+00,\n",
       "         6.86689558e+00,  7.17974563e-01,  2.08020241e+00,  5.00268730e+00,\n",
       "         1.76578223e+00,  2.56081764e+00, -7.11726799e-01, -1.23201644e+00,\n",
       "         3.19671425e+00, -4.14925768e+00, -3.90582677e+00,  1.59660382e+00,\n",
       "        -7.85576624e-01,  6.34701540e-01, -3.61169533e-01,  5.18723806e-01,\n",
       "         1.77797343e-02,  7.71487988e+00, -1.50057143e+00, -2.70852429e-01,\n",
       "        -1.99508088e+00,  7.33873415e+00,  7.96785464e-01, -5.42507270e-01,\n",
       "        -6.29238264e+00,  8.26149270e-01, -1.17953285e+00,  3.51574298e+00,\n",
       "        -4.66623700e-01,  3.04420181e+00, -3.57184890e+00,  6.97323546e-01,\n",
       "        -3.21851758e+00,  6.38919685e+00,  2.11231797e+00, -5.12684645e+00,\n",
       "        -1.73044977e+00, -3.17414182e+00, -1.57062595e+00, -2.00548536e+00,\n",
       "         2.52532592e+00,  2.92053327e+00, -1.33357099e+00, -3.89438798e-01,\n",
       "         5.52826505e-01,  1.66099883e+00,  8.94707898e-01,  4.98195640e-02,\n",
       "         2.82438971e+00, -1.44799792e+00,  1.82152380e+00,  3.99584755e+00,\n",
       "         5.31297485e-01,  2.33658664e+00, -2.53852839e+00,  2.90054766e-01,\n",
       "        -6.66976636e-01, -2.06435215e+00,  9.81367280e-01,  1.78379669e+00,\n",
       "         7.91477711e-01, -3.79184583e+00,  3.42808669e-01,  1.63116198e+00,\n",
       "         4.55453836e+00, -5.71525755e+00,  1.59940696e+00, -3.95527410e+00,\n",
       "        -6.17372335e+00, -2.05108403e+00, -1.73029684e+00, -4.32549411e+00,\n",
       "         7.57647893e-01, -1.09018776e+00,  2.99246000e+00, -3.18488173e+00,\n",
       "        -2.99254892e+00, -4.09871448e+00,  2.07294363e-02, -3.15001987e+00,\n",
       "         4.61855993e+00, -4.59881217e+00, -1.38260715e+00, -1.78903931e+00,\n",
       "        -8.88585534e-01, -2.68882778e+00,  1.76300624e+00,  3.55611388e+00,\n",
       "         1.14511908e+00,  3.35858622e+00, -2.20004903e+00, -1.87020272e+00,\n",
       "         1.13296429e+00, -2.65588230e+00,  5.40605348e+00,  1.48936428e+00,\n",
       "         3.13588631e+00,  1.08792755e+00, -2.82037698e+00,  3.36233079e+00,\n",
       "        -1.67060548e-01,  2.19262671e+00, -5.83472363e+00, -1.59531115e+00,\n",
       "        -3.12687216e+00, -2.27753334e+00,  9.74972755e-01, -2.14449448e+00,\n",
       "         9.15556805e-01,  3.05026190e+00,  4.26984095e+00, -1.40827480e+00,\n",
       "        -1.99349632e+00, -3.22953026e+00,  9.99124695e-01,  2.53085887e-01,\n",
       "         1.22532523e+00, -3.97153572e+00,  1.03948236e+00,  3.32802878e+00,\n",
       "        -2.18688735e+00, -6.37028717e+00,  2.52633898e+00, -8.90214211e-01,\n",
       "        -4.35917634e+00, -1.89304423e+00, -4.50968385e-03, -1.71560495e-01,\n",
       "         4.72193063e+00,  2.88184432e+00,  1.18463058e-01, -7.36883978e-01,\n",
       "         1.39068123e+00, -2.33230933e+00, -4.23603205e+00,  3.00382339e+00,\n",
       "         5.40447569e+00,  1.64815088e-01,  2.89917255e+00,  1.76737100e+00,\n",
       "        -4.89337911e+00, -4.03121240e+00, -6.18485453e-01, -2.31047100e+00,\n",
       "         5.18089286e+00,  7.48153417e+00,  1.21351881e+00,  1.30199784e+00,\n",
       "        -1.04349831e+00,  1.78318231e+00,  3.50019731e+00,  2.87325209e+00,\n",
       "         2.91195797e+00, -2.31052485e+00,  1.17289638e+00,  4.53222917e+00,\n",
       "        -4.56417201e+00, -4.50384951e+00,  1.63954614e+00, -3.74794217e+00,\n",
       "        -5.18164154e-01, -7.87210478e-01, -9.19618898e-01, -6.40511902e-01,\n",
       "        -2.01860755e+00,  2.92159593e+00,  4.60499582e+00, -3.08874356e+00,\n",
       "        -3.75202643e+00, -1.50150538e-02, -6.67753300e+00,  2.57732719e+00,\n",
       "         4.79511313e-01,  4.37148103e+00, -1.61049405e+00,  7.68465210e+00,\n",
       "         6.43748577e-01,  1.29255513e+00, -2.39439341e+00, -6.97374309e-01,\n",
       "        -4.96522255e+00, -6.66310934e+00, -4.29385266e+00, -7.36635085e-01,\n",
       "        -4.04260078e+00, -1.67320480e+00, -2.05382526e+00, -2.44047818e-02,\n",
       "        -4.10134845e-01,  9.74815155e-01, -1.74800329e+00, -2.30281122e+00,\n",
       "         2.52122536e+00,  2.34560267e+00,  3.93686083e+00,  1.93788793e+00,\n",
       "        -1.96831760e+00, -1.93730608e+00,  4.49625788e+00, -2.67406622e+00,\n",
       "         5.97923894e+00, -2.78880683e+00,  3.25532321e+00, -5.02675857e+00,\n",
       "         4.17443654e-01,  5.82211607e+00, -1.92182081e+00,  3.98062188e+00,\n",
       "         7.09833352e+00,  2.05895467e+00,  6.63096578e+00, -3.68713951e-01,\n",
       "        -4.03748334e+00, -4.30765749e+00,  4.64374469e+00,  3.26197702e-01,\n",
       "         8.50733424e-02,  1.15764816e+00,  3.42865178e+00,  4.41107432e+00,\n",
       "         2.28489313e+00, -1.90514497e-01,  2.67115170e+00,  1.36708351e+00,\n",
       "        -3.81652439e-01,  1.06539482e+00,  1.99560472e-01,  1.91214244e-01,\n",
       "        -3.24588649e+00,  4.69932167e+00,  5.40881615e+00, -2.36188069e+00,\n",
       "         1.81194749e+00,  2.38464341e+00, -1.18078909e+00, -3.29901932e+00,\n",
       "         3.14041101e-01,  2.51187940e+00,  4.87844838e+00,  3.97769575e+00,\n",
       "        -1.90298036e+00,  6.32540616e-01,  3.17058691e+00,  2.46050987e+00,\n",
       "        -3.16362152e-01,  7.66472931e-01, -1.29464413e-01, -1.51967996e+00,\n",
       "         1.35619990e+00,  2.09774102e+00,  1.59770526e+00,  3.49513549e-01,\n",
       "        -1.01491139e+00,  3.16086626e+00, -6.14609612e+00,  2.25652142e+00,\n",
       "        -1.08386751e+00,  4.20896515e+00, -3.34308791e+00,  3.24771822e+00,\n",
       "         2.11838327e+00, -1.38220851e+00,  4.50113319e-01, -3.07666372e+00,\n",
       "         2.47530692e+00,  2.00198348e+00,  1.77823677e+00, -3.35805011e+00,\n",
       "         3.51139026e-01, -7.89966406e+00,  3.19623343e+00,  4.00977499e+00,\n",
       "         5.55569136e+00, -1.40486414e+00, -1.65805756e+00,  2.11610351e-01,\n",
       "         6.30752251e-01, -4.81431713e-01,  1.12820661e+00,  3.74443251e-01]),\n",
       " array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True, False, False, False,  True,  True,  True, False,  True,\n",
       "        False, False, False, False, False,  True,  True, False, False,\n",
       "        False, False, False,  True,  True,  True, False, False, False,\n",
       "        False, False, False,  True,  True,  True, False,  True, False,\n",
       "        False,  True, False, False,  True, False, False, False,  True,\n",
       "         True, False, False, False, False,  True, False, False,  True,\n",
       "        False,  True, False, False, False,  True, False, False,  True,\n",
       "         True, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False,  True, False,  True, False, False,\n",
       "        False, False, False,  True,  True, False, False,  True, False,\n",
       "        False]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 1000)\n",
    "cf_train\n",
    "kfolds = 5\n",
    "ran_st = 42\n",
    "n_jobs = 1\n",
    "\n",
    "peLASSO_cv(y_train, cf_train, cf_pred, kfolds, ran_st, n_jobs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.normal(0, 1, (200, 100))\n",
    "x_pred = np.random.normal(0, 1, (200, 100))\n",
    "y_train = np.sum(x_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 200)\n",
    "\n",
    "n_models = 5\n",
    "split_grid = np.arange(1, 5)\n",
    "size_grid = np.array([10, 15, 20])\n",
    "kfolds = 5\n",
    "n_jobs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds, coefs = psgd_cv(y_train, x_train, x_pred, n_models, split_grid, size_grid, kfolds, n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.07703981, -4.89383573,  0.11783898,  3.75500092,  3.08550239,\n",
       "        3.41571229, -1.28221727, -6.51912643,  0.66380337,  0.75643027,\n",
       "       -3.08163963, -0.95581095, -0.7400876 , -0.69218145,  3.069736  ,\n",
       "       -1.55774335, -1.6671057 ,  4.48066325, -1.3869967 , -1.93908023,\n",
       "       -1.01943208, -2.37057906,  0.40994832, -1.00170536,  2.36942938,\n",
       "       -3.59267985, -0.90897479,  0.28668936, -0.3603516 ,  0.88773281,\n",
       "        3.10340832,  0.9926245 ,  0.21529504,  4.54091058, -1.55401121,\n",
       "       -4.11845788, -1.72545022,  1.85063187,  0.18986745, -0.37645526,\n",
       "       -0.06182412, -2.25887745,  0.55347854,  2.3889278 ,  0.02357924,\n",
       "       -3.17362332, -4.10023334,  0.35053314,  1.96855553, -0.27525086,\n",
       "        3.8403013 , -3.55874741, -2.37449421, -0.6530038 , -3.95276458,\n",
       "       -2.52083802,  4.07615065, -0.46934731,  1.0177259 , -0.43118357,\n",
       "       -1.49140558,  0.51236085,  0.18787418,  0.5031356 , -1.53089143,\n",
       "        3.11752673, -1.19500989, -5.60389785,  0.16938076,  6.18564969,\n",
       "        2.99127716,  3.05376483,  0.18772607,  1.67087164,  2.74716003,\n",
       "        0.71733137,  1.64680555,  5.23461675,  0.77087006,  1.43418763,\n",
       "        2.07992364,  1.69365257, -1.19477027,  6.04649799, -1.39264789,\n",
       "        0.0650264 ,  1.36245582,  0.34584009,  1.31152965,  2.77310999,\n",
       "       -3.2176378 , -0.6177601 ,  0.80323654,  3.41571109, -1.5521408 ,\n",
       "        2.01769835, -0.56797922,  0.38985849,  2.35123922,  3.41182999,\n",
       "        3.76188187,  1.20105028,  2.93780014, -2.16630651,  0.35541132,\n",
       "        1.07915041,  0.86475058,  2.87950669, -5.4831527 , -0.76680338,\n",
       "        2.24759544,  1.89308956, -1.02263298, -1.0515915 ,  0.24928691,\n",
       "        2.20283588, -1.44129532,  3.10598084, -2.42971679, -7.47573788,\n",
       "        2.21500847, -0.77498964, -2.33601786,  1.46492061,  3.86253645,\n",
       "       -2.58650797, -0.67016088,  0.84760534,  1.42941968, -1.14288448,\n",
       "       -1.23772154, -0.06213192, -0.23485333,  3.32554873, -1.51832396,\n",
       "        2.23651124, -0.37114274, -2.84002645,  4.24025004, -0.66726413,\n",
       "        0.49104714,  1.34215592, -0.18117579,  0.93684295, -2.34653413,\n",
       "       -4.26009063,  3.57131611,  0.5992419 ,  1.00883693, -1.4156684 ,\n",
       "       -2.17255102, -3.27355369, -0.77690891, -4.68419027, -1.64034802,\n",
       "        1.64372452, -0.75183542, -0.42786776, -1.37149823,  1.93585124,\n",
       "        1.42961945, -1.16308086,  1.59734337,  1.38936823,  2.39798372,\n",
       "        2.83062911,  0.40582006, -2.33776089,  1.28150433, -0.27229101,\n",
       "       -5.75677591, -3.03864803,  4.46994187,  1.33116031,  2.50264156,\n",
       "        5.16290455,  0.57338512,  4.47065774, -0.58397314,  3.03166963,\n",
       "       -0.3894096 ,  0.5331659 ,  1.49023878,  1.77378083, -1.70456967,\n",
       "       -3.36450158,  1.8997638 , -0.67105208,  0.54434985,  1.42027942,\n",
       "       -2.57082266,  2.67850121, -1.45640091, -0.8120472 ,  0.5046806 ,\n",
       "       -1.23798828,  3.81441798,  1.14911385, -0.75663933,  1.92618772])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import numpy2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Load the PSGD R package\n",
    "    psgd = importr('PSGD')\n",
    "\n",
    "    # Access the cv.PSGD function from the PSGD package\n",
    "    cv_psgd = psgd.cv_PSGD\n",
    "\n",
    "    # Convert to R objects\n",
    "    y_train_r = numpy2ri.py2rpy(y_train)\n",
    "    x_train_r = numpy2ri.py2rpy(x_train)\n",
    "    x_pred_r = numpy2ri.py2rpy(x_pred)\n",
    "    split_grid_r = numpy2ri.py2rpy(split_grid)\n",
    "    size_grid_r = numpy2ri.py2rpy(size_grid)\n",
    "\n",
    "    # Ensure Y is a matrix\n",
    "    y_train_r = ro.r.matrix(y_train_r, nrow=y_train.shape[0], ncol = 1)\n",
    "    \n",
    "    # Convert to Vector\n",
    "    split_grid_r = ro.FloatVector(split_grid)\n",
    "    size_grid_r = ro.FloatVector(size_grid)\n",
    "    group_index = ro.IntVector(range(1, n_models + 1))\n",
    "\n",
    "    # Fast-Best-Split-Selection\n",
    "    output = cv_psgd(x = x_train_r, y = y_train_r, n_models = float(n_models),\n",
    "                     model_type = \"Linear\", include_intercept = True,\n",
    "                     split_grid = split_grid_r, size_grid = size_grid_r,\n",
    "                     max_iter = float(100), cycling_iter = float(0), n_folds = float(kfolds), n_threads = float(n_jobs))\n",
    "\n",
    "    # Extract coefficients and make predictions as per your original R code\n",
    "    psgd_coef = ro.r['coef'](output, group_index = group_index)\n",
    "    psgd_predictions = ro.r['predict'](output, newx = x_pred_r, group_index = group_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Path\n",
    "path  =  os.path.dirname(os.getcwd()) # os.path.dirname(os.getcwd()) #r'/Users/slehmann/Library/CloudStorage/Dropbox/QUBO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "def sim_data(N_obs, n_obs, n_preds, non_zero, p, rho, scenario, snr, random_state):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to simulate data for Monte Carlo Study according to Fan and Lv (2008)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set Seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Simulating nonzero betas\n",
    "    b_nonzero = (-1) ** np.random.binomial(1, p, non_zero) * ((4 * np.log(n_obs) / np.sqrt(n_obs)) + abs(np.random.standard_normal(non_zero)))\n",
    "    \n",
    "    # Simulate all Betas\n",
    "    b = np.append(b_nonzero, np.repeat(0, n_preds - non_zero))\n",
    "    \n",
    "    ## Shuffle\n",
    "    #random.shuffle(b)\n",
    "    \n",
    "    # Indice of actice Betas\n",
    "    active = np.where(b != 0)[0]\n",
    "    \n",
    "    # Simulate Covariance Matrix\n",
    "    if scenario == 1:\n",
    "        \n",
    "        # Set Covariance Matrix between Predictors\n",
    "        cov_mat = np.full((n_preds, n_preds), rho)\n",
    "        np.fill_diagonal(cov_mat, 1.0)\n",
    "        \n",
    "    if scenario == 2:\n",
    "        \n",
    "        # Set Covariance Matrix between Predictors\n",
    "        cov_mat = np.zeros((n_preds, n_preds))\n",
    "        cov_mat[:non_zero, :non_zero] = rho\n",
    "        np.fill_diagonal(cov_mat, 1.0)\n",
    "\n",
    "    # Simulate Predictor-Time-Series\n",
    "    X = np.random.multivariate_normal([0.0]*n_preds, cov_mat, N_obs)\n",
    "    pred_names = \"X\"+pd.Series(range(1, n_preds+1)).astype(str) \n",
    "    \n",
    "    # Simulate Noise\n",
    "    adj   = np.sqrt((b.transpose() @ cov_mat @ b) / snr)\n",
    "    error = np.random.standard_normal(N_obs)   \n",
    "\n",
    "    # Set Target Variable\n",
    "    y = X @ b + adj * error\n",
    "    \n",
    "    # Return\n",
    "    return(y, X, pred_names, adj * error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Pre-Process Data\n",
    "def prepro(X_train, X_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to add intercept and to standardize data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize Data\n",
    "    scaler  =  StandardScaler()   \n",
    "    X_train =  scaler.fit_transform(X_train)\n",
    "    X_pred  =  scaler.transform(X_pred)\n",
    "    \n",
    "    # ## Add Constant\n",
    "    # X_train =  sm.add_constant(X_train)\n",
    "    # X_pred  =  sm.add_constant(X_pred, has_constant = 'add')\n",
    "    \n",
    "    return X_train, X_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Candidate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Candidate Models in one Function\n",
    "def candidate_models_(y_train_,\n",
    "                      y_pred_,\n",
    "                      X_train_,\n",
    "                      X_pred_,\n",
    "                      lambda_vec_,\n",
    "                      alpha_vec_,\n",
    "                      n_iter_,\n",
    "                      dim_vec_,\n",
    "                      rep_range_):\n",
    "    \n",
    "    ### Pre-Process Data ###\n",
    "    X_train, X_pred = prepro(X_train_, X_pred_)\n",
    "        \n",
    "    ### Elastic-Net-Forecasts ###\n",
    "    if (len(lambda_vec_) == 0) or (len(alpha_vec_) == 0):\n",
    "        preds_eln = np.empty((0, X_pred.shape[0]), float) \n",
    "    else: \n",
    "        preds_eln = eln(y_train_, X_train, X_pred, lambda_vec_, alpha_vec_, n_iter_)\n",
    "    \n",
    "    ### Compressed Regressions ###\n",
    "    if (np.sum(dim_vec_) == 0) or (np.sum(rep_range_) == 0):\n",
    "        preds_cr = np.empty((0, X_pred.shape[0]), float) \n",
    "    else:\n",
    "        preds_cr = cr_reg(y_train_, X_train, X_pred, dim_vec, rep_range)\n",
    "            \n",
    "    ### Concatenate Predictions ###\n",
    "    cand_forecasts = np.concatenate([preds_eln, preds_cr])\n",
    "    \n",
    "    ### Transpose\n",
    "    cand_forecasts = cand_forecasts.transpose()\n",
    "    \n",
    "    return y_pred_, cand_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Candidate Models in Parallel\n",
    "def candidate_models_cv(y, X, kfolds, krepeats, n_lambda, alpha_range, cr_range, rep_range, n_core, ran_st):\n",
    "    \n",
    "    # Set up Repeated K-Fold\n",
    "    rkf = RepeatedKFold(n_splits=kfolds, n_repeats=krepeats, random_state=ran_st)\n",
    "    \n",
    "    ### Calculate Lambda-Path for Elastic-Net ###\n",
    "    lambdapath = lambda_path(y, X, n_lambda)\n",
    "    \n",
    "    ### Create Candidate Models ###\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        y_preds, cand_forecasts = zip(*list(pool_.map(lambda idx: candidate_models_n(y[idx[0]], y[idx[1]], X[idx[0]], X[idx[1]], k_range, cr_range, rep_range), rkf.split(X, y))))\n",
    "        pool_.close()\n",
    "        \n",
    "    ### Concatenate Predictions ###\n",
    "    cand_forecasts = np.concatenate(cand_forecasts)\n",
    "    \n",
    "    ### Concatenate Targets ###\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "        \n",
    "    # Return\n",
    "    return y_preds, cand_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using R inside Python\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.ipython.html\n",
    "rpy2.ipython.html.init_printing()\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.robjects.packages import importr\n",
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "base = rpackages.importr('base')\n",
    "\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Install R Packages\n",
    "utils.install_packages('PSGD')\n",
    "\n",
    "# Load R Packages\n",
    "PSGD = importr('PSGD')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = PSGD.cv_PSGD(x = base.matrix(cf_train, ncol = cf_train.shape[1]), y = base.matrix(targets_train), n_models = 5.0, \n",
    "                      model_type = \"Linear\", include_intercept = True,\n",
    "                      split_grid = robjects.FloatVector([1.0, 2.0, 3.0, 4.0]), size_grid = robjects.FloatVector([1.0, 2.0, 3.0, 4.0]),\n",
    "                      max_iter = 100.0, cycling_iter = 5.0, n_folds = 5.0, n_threads = 1.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(output)[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Angle Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and predict LARS-Models\n",
    "def lars_n(y_train, cf_train, cf_pred, n):    \n",
    "        \n",
    "    # Define Model\n",
    "    model = Lars(fit_intercept = True,\n",
    "                 fit_path = False,\n",
    "                 jitter = None,\n",
    "                 n_nonzero_coefs = n,\n",
    "                 random_state = 123)\n",
    "\n",
    "    # Fit Model\n",
    "    model.fit(cf_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(cf_pred)\n",
    "    \n",
    "    return(np.concatenate(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform LARS in Parallel\n",
    "def lars(y_train, cf_train, cf_pred, n_range, n_core):\n",
    "    \n",
    "    # Parallelize over Subset Size\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        predictions = np.array(pool_.map(lambda n: lars_n(y_train, cf_train, cf_pred, n), n_range))\n",
    "        pool_.close()\n",
    "\n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stepwise Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Forward Stepwise Selection\n",
    "def fss_n(y_train, cf_train, cf_pred, n):\n",
    "    \n",
    "        # Model\n",
    "        model = LinearRegression()\n",
    "    \n",
    "        # Sequential Forward Selection\n",
    "        sfs = SequentialFeatureSelector(model,\n",
    "                                        n_features_to_select = n,\n",
    "                                        direction = 'forward')\n",
    "    \n",
    "        # Select Features\n",
    "        active_set = sfs.fit(cf_train, y_train).get_support()\n",
    "    \n",
    "        # Fit Model\n",
    "        model.fit(cf_train[:, active_set], y_train)\n",
    "    \n",
    "        # Predict\n",
    "        pred = model.predict(cf_pred[:, active_set])\n",
    "        \n",
    "        # Return\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Forward Stepwise Selection in Parallel\n",
    "def fss(y_train, cf_train, cf_pred, n_range, n_core):\n",
    "    \n",
    "    # Parallelize over Subset Size\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        predictions = np.array(pool_.map(lambda n: fss_n(y_train, cf_train, cf_pred, n), n_range))\n",
    "        pool_.close()\n",
    "\n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially-Egalitarian Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and predict peLASSO-Models\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "def peLASSO(y_train, cf_train, cf_pred, n_alpha, n_iter, cv_splits, cv_repeats):\n",
    "    \n",
    "    ### Step 1: Select to zero\n",
    "    # Define Cross-Validation Method\n",
    "    cv = RepeatedKFold(n_splits = cv_splits,\n",
    "                       n_repeats = cv_repeats,\n",
    "                       random_state = 123)\n",
    "    \n",
    "    # Define Model\n",
    "    model_lasso = LassoCV(fit_intercept = True,\n",
    "                          n_alphas = n_alpha,\n",
    "                          max_iter = n_iter,\n",
    "                          cv = cv,\n",
    "                          verbose = 0,\n",
    "                          n_jobs=1)\n",
    "    \n",
    "    # Fit Model\n",
    "    model_lasso.fit(cf_train, y_train)\n",
    "    \n",
    "    # Get & select only active candidate models\n",
    "    active_cf_train = cf_train[:, model_lasso.coef_.astype(bool)]\n",
    "    active_cf_pred  = cf_pred[: , model_lasso.coef_.astype(bool)]\n",
    "\n",
    "    ### Step 2: Shrink towards equality\n",
    "    # Check if active candidate models exist\n",
    "    if active_cf_train.shape[1] > 0:\n",
    "        \n",
    "        mean_cf = active_cf_train.mean(axis = 1)\n",
    "    \n",
    "        # Define Cross-Validation Method\n",
    "        cv = RepeatedKFold(n_splits = cv_splits,\n",
    "                           n_repeats = cv_repeats,\n",
    "                           random_state = 1)\n",
    "    \n",
    "        # Define Model\n",
    "        model_elasso = LassoCV(fit_intercept = True,\n",
    "                               n_alphas = n_alpha,\n",
    "                               max_iter = n_iter,\n",
    "                               cv = cv,\n",
    "                               verbose = 0,\n",
    "                               n_jobs=1)\n",
    "    \n",
    "        # Fit Model\n",
    "        model_elasso.fit(active_cf_train, (y_train-mean_cf))\n",
    "        \n",
    "        # Coefficients\n",
    "        coefs = model_elasso.coef_ + 1.0 / active_cf_train.shape[1]\n",
    "    \n",
    "        # Predict\n",
    "        pred = active_cf_pred @ coefs\n",
    "            \n",
    "    else:\n",
    "        print(\"peLASSO: No active candidate models\")\n",
    "        \n",
    "        # Set Prediction to mean\n",
    "        pred = [y_train.mean()] * cf_pred.shape[0]\n",
    "        \n",
    "    # Return Prediction\n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate X \n",
    "X = np.random.normal(0, 1, (250, 5))\n",
    "Y = np.sum(X, axis = 1) + np.random.normal(0, 5, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha = .5,\n",
    "              max_iter = 1000,\n",
    "              fit_intercept = False)\n",
    "model.fit(X, Y)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha = 2.0,\n",
    "              max_iter = 1000,\n",
    "              fit_intercept = False)\n",
    "model.fit(X, Y - np.mean(X, axis = 1))\n",
    "model.coef_ + 1.0 / X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average-Best Forecast Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual-based average-best forecast combination\n",
    "def avg_best(y_train, cf_train, cf_pred, n_range):\n",
    "    \n",
    "    # Set up Array\n",
    "    predictions = np.full((len(n_range), cf_pred.shape[0]), np.nan)\n",
    "    \n",
    "    # Calculate Squared Errors\n",
    "    se = ([[value] for value in y_train] - cf_train) ** 2\n",
    "\n",
    "    # Mean-Squared-Error\n",
    "    mse = np.mean(se, axis = 0)\n",
    "\n",
    "    # Get indices of the average-best N candidate models\n",
    "    ind = np.argsort(mse)\n",
    "\n",
    "    # Init Counter\n",
    "    i = 0\n",
    "    \n",
    "    # Loop over Subset Size\n",
    "    for n in n_range:\n",
    "        \n",
    "        # Predict\n",
    "        pred = np.mean(cf_pred[:, ind[:n]], axis = 1)\n",
    "        \n",
    "        # Append Prediction\n",
    "        predictions[i] = pred\n",
    "        \n",
    "        # Update Counter\n",
    "        i += 1\n",
    "        \n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Best Subset Selection of Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(y_train, cf_train, cf_pred, alpha, n_sub, bssf_timeout, method):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    cf_train  =  cf_train / n_sub\n",
    "    cf_pred   =  cf_pred  / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(cf_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(y_train.transpose() @ cf_train + alpha * n_sub)\n",
    "    Q         =  - 2 * np.diag(aux_mat) + cf_train.transpose() @ cf_train + alpha * ivec @ ivec.transpose()\n",
    "    \n",
    "    if method == \"dwave\":\n",
    "\n",
    "        # Initialize BQM\n",
    "        bqm  =  BinaryQuadraticModel('BINARY')\n",
    "        bqm  =  bqm.from_qubo(Q)\n",
    "\n",
    "        # Normalize\n",
    "        bqm.normalize()\n",
    "\n",
    "        # Preprocess (?)\n",
    "        #roof_duality(bqm)    \n",
    "\n",
    "        # Select Solver\n",
    "        solver_qpu  =  SimulatedAnnealingSampler() #LeapHybridSampler() SimulatedAnnealingSampler() EmbeddingComposite(DWaveSampler())\n",
    "        #solver_pp   =  SteepestDescentSolver()    #SteepestDescentSolver()\n",
    "\n",
    "        # Submit for Solution\n",
    "        sampleset  =  solver_qpu.sample(bqm, \n",
    "                                        num_reads = n_times,\n",
    "                                        #time_limit = 90,\n",
    "                                        label = \"Best Subset Selection of Forecasts\",\n",
    "                                        seed = 123) # f'Best Subset Selection of Forecasts{t}'\n",
    "\n",
    "        ## Postprocess Problem\n",
    "        #sampleset_pp = solver_pp.sample(bqm,\n",
    "        #                                initial_states = sampleset.lowest())\n",
    "\n",
    "        # Get Solution\n",
    "        solution    =  np.array(list(sampleset.first[0].values()))\n",
    "    \n",
    "    if method == \"qubo\":\n",
    "    \n",
    "        # Set up Model\n",
    "        model = gp.Model()\n",
    "        model.Params.TimeLimit = bssf_timeout\n",
    "        \n",
    "        # Decision Variables\n",
    "        b = model.addMVar(shape=Q.shape[0], vtype=gp.GRB.BINARY, name=\"b\")\n",
    "        \n",
    "        # Objective Function\n",
    "        model.setObjective(b @ Q @ b, gp.GRB.MINIMIZE)\n",
    "        \n",
    "        # Optimize\n",
    "        model.optimize()\n",
    "        solution = np.array(model.x)\n",
    "        \n",
    "    if method == \"qcbo\":\n",
    "        \n",
    "        # Set up Model\n",
    "        model = gp.Model()\n",
    "        model.params.timelimit = bssf_timeout\n",
    "\n",
    "        # Decision Variables\n",
    "        b = model.addMVar(shape=cf_train.shape[1], vtype=gp.GRB.BINARY, name=\"b\")\n",
    "        norm_0 = model.addVar(lb=n_sub, ub=n_sub, name=\"norm\")\n",
    "\n",
    "        # Objective Function\n",
    "        model.setObjective(b.T @ cf_train.T @ cf_train @ b\n",
    "                           - 2*y_train.T @ cf_train @ b\n",
    "                           + np.dot(y_train, y_train), gp.GRB.MINIMIZE)\n",
    "\n",
    "        # L0-Norm Constraint\n",
    "        model.addGenConstrNorm(norm_0, b, which=0, name=\"budget\")\n",
    "\n",
    "        # Optimize\n",
    "        model.optimize()\n",
    "        solution = np.array(model.x)[:-1]\n",
    "    \n",
    "    # Test Solution\n",
    "    if np.sum(solution) != n_sub:\n",
    "        print(f\"Warning: Number of selected features does not match --- {np.sum(solution)} instead of {n_sub}!\")\n",
    "    \n",
    "    # Prediction \n",
    "    pred = solution @ cf_pred.transpose()\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_st = 0\n",
    "train = 90\n",
    "n_obs = 100\n",
    "bernoulli_p = 0.2\n",
    "n_preds = 10\n",
    "kfolds = 5\n",
    "krepeats = 1\n",
    "k_range_ssf = [1, 2]\n",
    "k_range_cr = [1, 2]\n",
    "rep_range_cr = range(5)\n",
    "n_core = 1\n",
    "\n",
    "### Simulate Data ###\n",
    "y, X, pred_names, error = sim_data(n_obs, train, n_preds, 2, bernoulli_p, 0.2, 1, 0.3, 0)\n",
    "\n",
    "### Split in Train and Test Data ###\n",
    "y_train, y_pred, X_train, X_pred = y[:train].copy(), y[train:].copy(), X[:train].copy(), X[train:].copy()\n",
    "### Create Train Candidate Models ###\n",
    "#targets_train, cf_train = candidate_models(y_train, X_train, kfolds, krepeats, k_range_ssf, k_range_cr, rep_range_cr, n_core, ran_st)\n",
    "    \n",
    "### Create Test Candidate Models ###\n",
    "#targets_test, cf_test = candidate_models_n(y_train, y_pred, X_train, X_pred, k_range_ssf, k_range_cr, rep_range_cr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MC-Parameter ######\n",
    "# Number of MC-Runs\n",
    "n_mc   =  10\n",
    "n_core =  6\n",
    "\n",
    "# Set Parameter\n",
    "n_obs       =  1050\n",
    "train       =  50\n",
    "n_preds     =  10\n",
    "b_range     =  [2, 5, 8]\n",
    "bernoulli_p =  0.2\n",
    "corr_range  =  [0.2, 0.5, 0.8]\n",
    "scenario_range = [1, 2]\n",
    "snr_range   =  [0.1, 0.3, 0.5]  # --> PVE = SNR / (1 + SNR)\n",
    "\n",
    "### Cross Validation ###\n",
    "kfolds   = 5\n",
    "krepeats = 1\n",
    "\n",
    "###### Parameter Subset Forecasts ######\n",
    "# Subset Lengths\n",
    "k_range_ssf = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  \n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "if np.sum(k_range_ssf) == 0:\n",
    "    n_sub  =  0\n",
    "else:\n",
    "    n_sub  =  int(sum([min(n_models(n_preds, k), n_max) for k in k_range_ssf]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "k_range_cr  =  [0] #[1, 2, 3, 4] \n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range_cr  =  [0] #range(0, 60) \n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "if (np.sum(k_range_cr) == 0) or (len(rep_range_cr) == 0):\n",
    "    n_cr = 0\n",
    "else: \n",
    "    n_cr  =  len(k_range_cr) * len(rep_range_cr)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) \n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter LARS ######\n",
    "k_range_lars = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "###### Parameter Forward Stepwise Selection ######\n",
    "k_range_fss = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  ### -> klein machen\n",
    "\n",
    "###### Parameter peLasso ######\n",
    "n_alpha        = 200\n",
    "n_iter_peL     = 1000\n",
    "cv_splits_peL  = 5\n",
    "cv_repeats_peL = 1\n",
    "\n",
    "###### Parameter Average-Best ######\n",
    "k_range_avg_best = [1, 5, 10, 25, 50, 100, 250, 500, 750, 1023] \n",
    "\n",
    "###### Parameter BSSF ######\n",
    "bssf_alpha   =  2000\n",
    "bssf_timeout =  1.0\n",
    "k_range_bssf =  [1, 5, 10, 25, 50, 100, 250, 500, 750, 1023] \n",
    "n_bssf       =  len(k_range_bssf)\n",
    "bssf_method  =  \"qubo\"\n",
    "\n",
    "######## Objects ########\n",
    "# Set up Matrices for Results\n",
    "cf_train = np.full((train, (n_sub + n_cr)), np.nan)  \n",
    "cf_test  = np.full((n_obs - train, (n_sub + n_cr)), np.nan)  \n",
    "\n",
    "benchmark        = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc), np.nan) ####\n",
    "se_benchmark     = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "\n",
    "cf_weights       = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, (n_sub + n_cr)), np.nan)\n",
    "bssf_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, n_obs - train), np.nan)\n",
    "se_bssf_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, n_obs - train), np.nan)\n",
    "\n",
    "csr_forecast     = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_ssf), n_obs - train), np.nan)\n",
    "se_csr_forecast  = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_ssf), n_obs - train), np.nan)\n",
    "\n",
    "lars_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_lars), n_obs - train), np.nan)\n",
    "se_lars_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_lars), n_obs - train), np.nan)\n",
    "\n",
    "fss_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_fss), n_obs - train), np.nan)\n",
    "se_fss_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_fss), n_obs - train), np.nan)\n",
    "\n",
    "pelasso_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "se_pelasso_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "\n",
    "avg_best_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_avg_best), n_obs - train), np.nan)\n",
    "se_avg_best_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_avg_best), n_obs - train), np.nan)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Monte-Carlo Runs\n",
    "program_starts = time.time()\n",
    "for r, ran_st in enumerate(tqdm(range(n_mc))):\n",
    "    \n",
    "    # Loop over Covariance-Scenario\n",
    "    for c, sce in enumerate(scenario_range):\n",
    "        \n",
    "        # Loop over Signal-to-Noise-Ratio\n",
    "        for s, snr in enumerate(snr_range):\n",
    "    \n",
    "            # Loop over Covariance-Sets\n",
    "            for p, rho in enumerate(corr_range):\n",
    "        \n",
    "                # Loop over Coefficient-Sets\n",
    "                for b, beta in enumerate(b_range):\n",
    "    \n",
    "                    ### Simulate Data ###\n",
    "                    y, X, pred_names, error = sim_data(n_obs, train, n_preds, beta, bernoulli_p, rho, sce, snr, ran_st)\n",
    "                    \n",
    "                    ### Split in Train and Test Data ###\n",
    "                    y_train, y_pred, X_train, X_pred = y[:train].copy(), y[train:].copy(), X[:train].copy(), X[train:].copy()\n",
    "\n",
    "                    ### Create Train Candidate Models ###\n",
    "                    targets_train, cf_train = candidate_models(y_train, X_train, kfolds, krepeats, k_range_ssf, k_range_cr, rep_range_cr, n_core, ran_st)\n",
    "                        \n",
    "                    ### Create Test Candidate Models ###\n",
    "                    targets_test, cf_test = candidate_models_n(y_train, y_pred, X_train, X_pred, k_range_ssf, k_range_cr, rep_range_cr) \n",
    "\n",
    "                    ### Benchmark: PHM ###\n",
    "                    benchmark[c][s][p][b][r] =  targets_train.mean()\n",
    "                    se_benchmark[c][s][p][b][r] = (targets_test - targets_train.mean()) ** 2\n",
    "\n",
    "                    ### Benchmark: Complete Subset Regression ###\n",
    "                    idx_ss = np.cumsum([0] + [len(list(combinations(range(n_preds), k))) for k in k_range_ssf])\n",
    "                    csr_forecast[c][s][p][b][r] = [np.mean(cf_test[:, :n_sub][:, idx_ss[i]:idx_ss[i+1]], axis = 1) for i in range(len(idx_ss)-1)]\n",
    "                    se_csr_forecast[c][s][p][b][r] = (targets_test - csr_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: LARS ###\n",
    "                    lars_forecast[c][s][p][b][r] = lars(targets_train, cf_train, cf_test, k_range_lars, n_core)\n",
    "                    se_lars_forecast[c][s][p][b][r] = (targets_test - lars_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: Forward Stepwise Selection ###\n",
    "                    fss_forecast[c][s][p][b][r] = fss(targets_train, cf_train, cf_test, k_range_fss, n_core)\n",
    "                    se_fss_forecast[c][s][p][b][r] = (targets_test - fss_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: peLASSO ###\n",
    "                    pelasso_forecast[c][s][p][b][r] = peLASSO(targets_train, cf_train, cf_test, n_alpha, n_iter_peL, cv_splits_peL, cv_repeats_peL)\n",
    "                    se_pelasso_forecast[c][s][p][b][r] = (targets_test - pelasso_forecast[c][s][p][b][r]) ** 2                    \n",
    "\n",
    "                    ### Benchmark: Average-Best ###\n",
    "                    avg_best_forecast[c][s][p][b][r] = avg_best(targets_train, cf_train, cf_test, k_range_avg_best)\n",
    "                    se_avg_best_forecast[c][s][p][b][r] = (targets_test - avg_best_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Best Selection of Forecast ###\n",
    "                    bssf_forecast[c][s][p][b][r], cf_weights[c][s][p][b][r] = zip(*list(map(lambda k_bssf: bssf(targets_train, cf_train, cf_test, bssf_alpha, k_bssf, bssf_timeout, bssf_method), k_range_bssf)))\n",
    "                    se_bssf_forecast[c][s][p][b][r] = (targets_test - bssf_forecast[c][s][p][b][r]) ** 2\n",
    "    \n",
    "    ### Save \n",
    "    # ...\n",
    "    \n",
    "# Time                \n",
    "program_ends = time.time()\n",
    "         \n",
    "# Loop over all combinations\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        for s in range(len(snr_range)):\n",
    "            for c in range(len(scenario_range)):\n",
    "                \n",
    "\n",
    "                # Calculate Forecast Combination Method Performances  \n",
    "                mse_bssf = np.sum(np.sum(se_bssf_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_csr  = np.sum(np.sum( se_csr_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_lars = np.sum(np.sum(se_lars_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_fss       =  np.sum(np.sum(se_fss_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_pelasso   =  np.sum(np.sum(se_pelasso_forecast[c][s][p][b], axis = 1), axis = 0)\n",
    "                mse_avg_best  =  np.sum(np.sum(se_avg_best_forecast[c][s][p][b], axis = 2), axis = 0)                \n",
    "\n",
    "                # Calculate Benchmark Performance  \n",
    "                mse_phm  =  np.sum(np.sum(se_benchmark[c][s][p][b], axis = 1), axis = 0)\n",
    "\n",
    "                # Create Rows\n",
    "                new_row_bbsf = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"BSSF\",       \"OOS-R2\": np.array2string(100 * (1 - mse_bssf / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_csr  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"CSR\",        \"OOS-R2\": np.array2string(100 * (1 - mse_csr  / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_lars = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"LARS\",       \"OOS-R2\": np.array2string(100 * (1 - mse_lars / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_fss  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"FSS\",        \"OOS-R2\": np.array2string(100 * (1 - mse_fss  / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_peL  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"peLASSO\",    \"OOS-R2\": np.array2string(100 * (1 - mse_pelasso / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_avgB = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"Avg_Best_N\", \"OOS-R2\": np.array2string(100 * (1 - mse_avg_best / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "\n",
    "                # Add Rows\n",
    "                new_rows = pd.DataFrame.from_dict([new_row_bbsf,\n",
    "                                                   new_row_csr,\n",
    "                                                   new_row_lars,\n",
    "                                                   new_row_fss,\n",
    "                                                   new_row_peL,\n",
    "                                                   new_row_avgB\n",
    "                                                   ])\n",
    "                \n",
    "                ## Add to CSV\n",
    "                #output_path = path + \"/Results/my_csv.csv\"\n",
    "                #new_rows.to_csv(output_path, mode='a', header=not os.path.exists(output_path))\n",
    "                \n",
    "                # Print Results\n",
    "                print(f\"BSSF:       Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_bssf / mse_phm), 2)) + \"%\")\n",
    "                print(f\"CSR:        Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_csr  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"LARS:       Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_lars / mse_phm), 2)) + \"%\")\n",
    "                print(f\"FSS:        Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_fss  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"peLASSO:    Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_pelasso  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"Avg-Best N: Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_avg_best / mse_phm), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ### Simulate Data ###\n",
    "                    y, X, pred_names, error = sim_data(n_obs, train, n_preds, 2, bernoulli_p, 0.2, 1, 0.3, 0)\n",
    "                    \n",
    "                    ### Split in Train and Test Data ###\n",
    "                    y_train, y_pred, X_train, X_pred = y[:train].copy(), y[train:].copy(), X[:train].copy(), X[train:].copy()\n",
    "\n",
    "                    ### Create Train Candidate Models ###\n",
    "                    targets_train, cf_train = candidate_models(y_train, X_train, kfolds, krepeats, k_range_ssf, k_range_cr, rep_range_cr, n_core, 0)\n",
    "                        \n",
    "                    ### Create Test Candidate Models ###\n",
    "                    targets_test, cf_test = candidate_models_n(y_train, y_pred, X_train, X_pred, k_range_ssf, k_range_cr, rep_range_cr) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up DataFrame\n",
    "dataframe_plot = pd.DataFrame()\n",
    "\n",
    "# Create DataFrame\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        # Subset Weights\n",
    "        chosen_cm = cf_weights[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "        # Aggregated\n",
    "        chosen_cm = np.sum(chosen_cm, axis = 0)\n",
    "\n",
    "        # Replace Zero with NaN\n",
    "        chosen_cm[chosen_cm == 0] = np.nan\n",
    "        chosen_cm_agg = chosen_cm.copy()\n",
    "\n",
    "        # Set to One\n",
    "        chosen_cm[chosen_cm > 1 ] = 1\n",
    "\n",
    "        # Adapt Column-Values\n",
    "        chosen_cm = chosen_cm * np.arange(1, chosen_cm.shape[1]+1)\n",
    "\n",
    "        # Create DataFrame\n",
    "        chosen_cm = pd.DataFrame(chosen_cm, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "        chosen_cm_agg = pd.DataFrame(chosen_cm_agg, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "\n",
    "        # Index\n",
    "        chosen_cm = chosen_cm.reset_index(names=\"INDEX\")\n",
    "        chosen_cm_agg = chosen_cm_agg.reset_index(names=\"INDEX\")\n",
    "\n",
    "        # Melt Data\n",
    "        chosen_cm = chosen_cm.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Value')\n",
    "        chosen_cm_agg = chosen_cm_agg.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Weight')\n",
    "\n",
    "        # Concatenate\n",
    "        chosen_cm = pd.concat([chosen_cm, chosen_cm_agg])\n",
    "        \n",
    "        # Transform Candidate Model Names\n",
    "        chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.split('_').str[0]\n",
    "        \n",
    "        # Add Column for Correlation\n",
    "        chosen_cm['Correlation'] = corr_range[p] \n",
    "        \n",
    "        # Add Column for Betas\n",
    "        chosen_cm['Betas'] = str(b_range[b])\n",
    "        \n",
    "        # Append\n",
    "        dataframe_plot = pd.concat([dataframe_plot, chosen_cm])\n",
    "        \n",
    "# Plot Theme\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Plot Data\n",
    "g = sns.relplot(\n",
    "    data = dataframe_plot,\n",
    "    y = 'Value', x = 'INDEX',\n",
    "    hue = \"Candidate_Model\", size = \"Weight\", \n",
    "    col = \"Betas\", row = \"Correlation\",\n",
    "    sizes = (3, 75), height = 5.0, aspect = 1.5,\n",
    "    alpha = 0.75, palette = \"muted\",\n",
    "    #legend = True\n",
    "    )\n",
    "\n",
    "# Legend\n",
    "#g._legend.remove()\n",
    "#h, l = g.ax.get_legend_handles_labels()\n",
    "#g.ax.legend(h[0:13], l[0:13], bbox_to_anchor=(1.0, 0.75), loc=2, fontsize=10, frameon=False)\n",
    "\n",
    "# Title & Axis\n",
    "g.set(xlabel='Combination Size',\n",
    "      ylabel='Candidate Models')\n",
    "      #title=f\"Selected Candidate Models: \\n Correlation {corr_range[p]} & Betas {b_range[b]}\")\n",
    "\n",
    "# Margins\n",
    "#g.ax.margins(x = 0.05, y = 0)\n",
    "\n",
    "# Set number of ticks for y-axis\n",
    "idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "#g.ax.set_yticks(idx)\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "#g.ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "## iterate over axes of FacetGrid\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_yticks(idx)\n",
    "    ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "# Tick-Label Size\n",
    "g.set_yticklabels(size = 8)\n",
    "g.set_xticklabels(size = 10)\n",
    "\n",
    "# Add Horizontal Lines\n",
    "#for i in idx:\n",
    "#    g.ax.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "#for label in g.ax.get_xticklabels():\n",
    "#    label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        # Subset Weights\n",
    "        chosen_cm = cf_weights[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "        # Aggregated\n",
    "        chosen_cm = np.sum(chosen_cm, axis = 0)\n",
    "\n",
    "        # Replace Zero with NaN\n",
    "        chosen_cm[chosen_cm == 0] = np.nan\n",
    "        chosen_cm_agg = chosen_cm.copy()\n",
    "\n",
    "        # Set to One\n",
    "        chosen_cm[chosen_cm > 1 ] = 1\n",
    "\n",
    "        # Adapt Column-Values\n",
    "        chosen_cm = chosen_cm * np.arange(1, chosen_cm.shape[1]+1)\n",
    "\n",
    "        # Create DataFrame\n",
    "        chosen_cm = pd.DataFrame(chosen_cm, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "        chosen_cm_agg = pd.DataFrame(chosen_cm_agg, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "\n",
    "        # Index\n",
    "        chosen_cm = chosen_cm.reset_index(names=\"INDEX\")\n",
    "        chosen_cm_agg = chosen_cm_agg.reset_index(names=\"INDEX\")\n",
    "\n",
    "        # Melt Data\n",
    "        chosen_cm = chosen_cm.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Value')\n",
    "        chosen_cm_agg = chosen_cm_agg.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Weight')\n",
    "\n",
    "        # Concatenate\n",
    "        chosen_cm = pd.concat([chosen_cm, chosen_cm_agg])\n",
    "\n",
    "        # Transform Candidate Model Names\n",
    "        chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.split('_').str[0]\n",
    "        #chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.startswith(\"SSF\").replace({True: \"SSF\", False: \"CR\"})\n",
    "\n",
    "        # Plot Theme\n",
    "        sns.set_theme(style=\"ticks\")\n",
    "\n",
    "        # Draw each cell as a scatter point with varying size and color\n",
    "        g = sns.relplot(\n",
    "            data = chosen_cm,\n",
    "            y = 'Value', x = 'INDEX', hue = \"Candidate_Model\", \n",
    "            size = \"Weight\", sizes = (3, 75),\n",
    "            height = 6.5, alpha = 0.75, palette=\"muted\",\n",
    "            #legend = True\n",
    "            )\n",
    "\n",
    "        # Legend\n",
    "        g._legend.remove()\n",
    "        h, l = g.ax.get_legend_handles_labels()\n",
    "        g.ax.legend(h[0:13], l[0:13], bbox_to_anchor=(1.0, 0.75), loc=2, fontsize=10, frameon=False)\n",
    "\n",
    "        # Title & Axis\n",
    "        g.set(xlabel='Combination Size',\n",
    "              ylabel='Candidate Models',\n",
    "              title=f\"Selected Candidate Models: \\n Correlation {corr_range[p]} & Betas {b_range[b]}\")\n",
    "\n",
    "        # Margins\n",
    "        g.ax.margins(x = 0.05, y = 0)\n",
    "\n",
    "        # Set number of ticks for y-axis\n",
    "        idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "        g.ax.set_yticks(idx)\n",
    "\n",
    "        # Set ticks labels for x-axis\n",
    "        g.ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "        # Tick-Label Size\n",
    "        g.set_yticklabels(size = 8)\n",
    "        g.set_xticklabels(size = 10)\n",
    "\n",
    "        # Add Horizontal Lines\n",
    "        #for i in idx:\n",
    "        #    g.ax.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "        # Rotate x-axis labels\n",
    "        for label in g.ax.get_xticklabels():\n",
    "            label.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Plot\n",
    "fig, ax = plt.subplots(1,1) \n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "# Plot \n",
    "colors = {'SSF':'blue', 'CR':'green'}\n",
    "ax.scatter(x = chosen_cm['INDEX'], y = chosen_cm['Weight'], c=chosen_cm['Candidate_Model'].map(colors), s = 1)\n",
    "#ax.plot(chosen_cm['INDEX'], chosen_cm.iloc[:, 1:], marker = \"o\", lw = 0, ms = 4)\n",
    "\n",
    "# Add title and axis names\n",
    "plt.title('Selected Candidate Models')\n",
    "plt.ylabel('Candidate Models')\n",
    "plt.xlabel('Combination Size')\n",
    "\n",
    "# Legend\n",
    "#plt.legend(loc = \"upper right\")\n",
    "\n",
    "# Margins\n",
    "plt.margins(x=0.10, y=0)\n",
    "\n",
    "# Set number of ticks for x-axis\n",
    "idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "ax.set_yticks(idx)\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "ax.set_yticklabels([[[*ssf_names, *cr_names]][0][i] for i in idx], rotation='horizontal', fontsize=6)\n",
    "\n",
    "# Add horizontal lines\n",
    "for i in idx:\n",
    "    plt.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot: Welches K hat was ausgewhlt\n",
    "### Benchmark: Complete Subset Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is:\" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is:\" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = benchmark[np.arange(0, n_mc * len(corr_range) * len(b_range), len(b_range))]\n",
    "b2 = benchmark[np.arange(1, n_mc * len(corr_range) * len(b_range), len(b_range))]\n",
    "\n",
    "b1_p1 = b1[np.arange(0, n_mc * len(corr_range), len(corr_range))]\n",
    "b1_p2 = b1[np.arange(1, n_mc * len(corr_range), len(corr_range))]\n",
    "b1_p3 = b1[np.arange(2, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "b2_p1 = b2[np.arange(0, n_mc * len(corr_range), len(corr_range))]\n",
    "b2_p2 = b2[np.arange(1, n_mc * len(corr_range), len(corr_range))]\n",
    "b2_p3 = b2[np.arange(2, n_mc * len(corr_range), len(corr_range))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Goyal Welch Data\n",
    "data  =  pd.read_csv(path + r'/Data/PredictorData2022.xlsx - Quarterly.csv', thousands=',')\n",
    "\n",
    "# Equity Premium\n",
    "data['equity_premium'] = data['CRSP_SPvw'] - data['Rfree']\n",
    "\n",
    "# Dividend Price Ratio \n",
    "data['dp'] = np.log(data['D12']) - np.log(data['Index'])\n",
    "\n",
    "# Dividend Yield \n",
    "data['dy'] = np.log(data['D12'])- np.log(data['Index'].shift(1))\n",
    "\n",
    "# Earnings Price Ratio \n",
    "data['ep'] = np.log(data['E12']) - np.log(data['Index'])\n",
    "\n",
    "# Dividend Payout Ratio \n",
    "data['dpayr'] = np.log(data['D12']) - np.log(data['E12'])\n",
    "\n",
    "# Book to Market Ratio\n",
    "data['bmr'] = data['b/m']\n",
    "\n",
    "# # Net Equity Expansion\n",
    "data['ntis'] = data['ntis']\n",
    "\n",
    "# Treasury Bill Rate\n",
    "data['tbl'] = data['tbl']\n",
    "\n",
    "# Long Term Rate\n",
    "data['ltr'] = data['ltr']\n",
    "\n",
    "# Term Spread \n",
    "data['tsp'] = data['lty'] - data['tbl']\n",
    "\n",
    "# Default Return Spread \n",
    "data['dfr'] = data['corpr'] - data['ltr']\n",
    "\n",
    "# Inflation\n",
    "data['infl'] = data['infl']\n",
    "\n",
    "# Investment of Capital Ratio\n",
    "data['ik']  = data['ik']\n",
    "\n",
    "# Default Yield Spread\n",
    "data['dfy'] = data['BAA'] - data['AAA']\n",
    "\n",
    "# Realized Volatility\n",
    "data['rvol'] = data['svar']\n",
    "\n",
    "# reorganize the dataframe\n",
    "data = data[['yyyyq', \"equity_premium\", \"dp\", \"dy\", \"ep\", \"dpayr\", \"bmr\", \"ntis\", \"tbl\", \"ltr\", \"tsp\", \"dfr\", \"infl\", \"ik\"]]\n",
    "\n",
    "# Convert Date\n",
    "data['yyyyq'] = data['yyyyq'].astype(str)\n",
    "data['yyyyq'] = data.apply(lambda x: x['yyyyq'][:4]+'-Q'+x['yyyyq'][4:], axis=1)\n",
    "data['yyyyq'] = pd.to_datetime(data['yyyyq'])\n",
    "\n",
    "# Resetting the index\n",
    "data.set_index('yyyyq', inplace=True)\n",
    "data.index = data.index.to_period('Q')\n",
    "\n",
    "# Lag all Predictors\n",
    "data.iloc[:,1:]  =  data.iloc[:,1:].shift(1)\n",
    "\n",
    "# Drop Na\n",
    "data = data.loc[\"1946Q1\":, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Set Seed ######\n",
    "#random.seed(123)\n",
    "\n",
    "###### Data ######\n",
    "# Set Target Variable\n",
    "y  =  data.loc[:, [\"equity_premium\"]]\n",
    "X  =  data.drop(\"equity_premium\", axis = 1)\n",
    "\n",
    "# Get Predictor Names\n",
    "pred_names = list(X.columns)\n",
    "\n",
    "# Number of AR-Terms to include\n",
    "mlags =  2\n",
    "\n",
    "# Create Lags\n",
    "X  =  create_lags(y, X, mlags)\n",
    "\n",
    "# Drop Missing Values\n",
    "y  =  y.loc[\"1947Q2\":, ] #y[mlags:]\n",
    "X  =  X.loc[\"1947Q2\":, ] #X[mlags:]\n",
    "\n",
    "# Check NA\n",
    "any(X.isna().any())\n",
    "\n",
    "###### Parameter Subset Forecasts\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  # 20000\n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "n_sub  =  int(sum([min(n_models((X.shape[1]-mlags), k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2, 3] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 100) # 10000\n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) # 300000\n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  0.5\n",
    "n_times     =  50\n",
    "bssf_range  =  [1, 2, 3] # range(1, 5)\n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "### General Parameter ######\n",
    "# Initial Training-Period\n",
    "init       =  4 * 50 #4 * 10\n",
    "\n",
    "# Total Length\n",
    "total =  len(y) \n",
    "\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts  =  np.full((total, (n_sub + n_cr)), np.nan)     #np.full((total, (n_sub + n_cr + n_dt)), np.nan)\n",
    "cf_weights      =  np.full((total, (n_sub + n_cr)), np.nan)\n",
    "benchmark       =  np.full(total, np.nan)\n",
    "bssf_forecast   =  np.full(total, np.nan)\n",
    "bssf_opt        =  np.full(total, np.nan)\n",
    "sse_bssf        =  np.zeros(n_bssf)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Time\n",
    "for t in tqdm(range(init, total)):\n",
    "        \n",
    "    # Pre-Process Data\n",
    "    y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "    \n",
    "    ### Benchmark: AR(X) Model\n",
    "    pred          =  ar_mod(y_train, lags = mlags)\n",
    "    benchmark[t]  =  pred.iloc[0]\n",
    "    \n",
    "    ### Subset Forecasts\n",
    "    # Set up List to store Subset-Forecasts\n",
    "    preds_ssf =  np.full(n_sub, np.nan)\n",
    "    idx_sub   =  0\n",
    "    \n",
    "    # Loop over Subset Size \n",
    "    for k in k_range:\n",
    "    \n",
    "        # Get all possible Subset of length k\n",
    "        col_idx   =  list(range(mlags+1, X_train.shape[1]))\n",
    "        subs_idx  =  complete_sub(col_idx, k)\n",
    "\n",
    "        # Randomly select n_upper Subsets\n",
    "        feature_set  =  subs_idx #random_select(subs_idx, n_max, random_state = 123)\n",
    "\n",
    "        # Loop over Subsets\n",
    "        for feature in feature_set:\n",
    "\n",
    "            # Compute Subset-Regression-Forecast\n",
    "            pred  =  ssf(y_train, X_train, X_pred, feature, mlags)\n",
    "            preds_ssf[idx_sub] = pred\n",
    "            idx_sub += 1\n",
    "            \n",
    "    ### Compressed Regressions\n",
    "    # Set up List to store Compressed-Regression-Forecasts\n",
    "    preds_cr = np.full(n_cr, np.nan)\n",
    "    idx_cr   = 0\n",
    "    \n",
    "    # Loop over number of Components\n",
    "    for n_comp in cr_range:\n",
    "\n",
    "        # Loop over n repetitions\n",
    "        for r in rep_range:\n",
    "        \n",
    "            # Compute Compressed-Regression-Forecasts\n",
    "            pred  =  cr_reg(y_train, X_train, X_pred, n_comp, mlags, r)\n",
    "            preds_cr[idx_cr] = pred\n",
    "            idx_cr += 1\n",
    "            \n",
    "    # ### Decision Tree Regressions\n",
    "    # # Set up Matrix to store Decision-Tree-Forecasts\n",
    "    # preds_dt   = np.full(n_dt, np.nan)\n",
    "    # \n",
    "    # # Loop over number of Components\n",
    "    # for idx_dt, r in enumerate(dt_range):\n",
    "    #     \n",
    "    #     # Compute Decision-Tree-Forecasts\n",
    "    #     pred  =  dt_reg(y_train, X_train, X_pred, r)\n",
    "    #     preds_dt[idx_dt] = pred[0]\n",
    "\n",
    "    # Append Results\n",
    "    cand_forecasts[t][:n_sub]             =  preds_ssf \n",
    "    cand_forecasts[t][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "    #cand_forecasts[t][(n_sub+n_cr):]      =  preds_dt\n",
    "\n",
    "    ### Best Selection of Forecast\n",
    "    if t > init:\n",
    "    \n",
    "        # Set up Matrix to store Forecasts\n",
    "        bssf_forecasts  =  np.full(n_bssf, np.nan)\n",
    "        bssf_weights    =  np.zeros([n_bssf, n_sub + n_cr]) \n",
    "           \n",
    "        # Get \"best\" Subset-Size until now (lowest Sum of Squared Errors)\n",
    "        s_opt  =  np.argmin(sse_bssf)\n",
    "    \n",
    "        # Loop over Subset Sizes\n",
    "        for idx_bssf, s in enumerate(bssf_range):\n",
    "    \n",
    "            # Compute Best-Subset-Selection-of-Forecasts\n",
    "            pred  =  bssf(y_train[init:], cand_forecasts[init:t], cand_forecasts[t], alpha, s, n_times)\n",
    "            bssf_forecasts[idx_bssf]  =  pred[0]\n",
    "            bssf_weights[idx_bssf]    =  pred[1]\n",
    "    \n",
    "            # Compute Sum of Squared Errors\n",
    "            sse_bssf[idx_bssf] =  sse_bssf[idx_bssf] + (y_pred.iloc[0,0] - pred[0]) ** 2\n",
    "    \n",
    "        # Select Forecast \n",
    "        bssf_forecast[t] =  bssf_forecasts[s_opt]\n",
    "        cf_weights[t]    =  bssf_weights[s_opt]\n",
    "        bssf_opt[t]      =  bssf_range[s_opt]\n",
    "        \n",
    "# Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(len(pred_names)), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "#dt_names = [f\"DT_{idx_dt}\" for idx_dt in dt_range]\n",
    "        \n",
    "# Convert Results to DataFrame\n",
    "cand_forecasts  =  pd.DataFrame(cand_forecasts, index = y.index, columns = [*ssf_names, *cr_names]) #, *dt_names])\n",
    "benchmark       =  pd.DataFrame(benchmark,      index = y.index, columns = [\"AR\"])\n",
    "bssf_forecast   =  pd.DataFrame(bssf_forecast,  index = y.index, columns = [\"BSSF\"])\n",
    "cf_weights      =  pd.DataFrame(cf_weights,     index = y.index, columns = [*ssf_names, *cr_names]) #, *dt_names])\n",
    "bssf_opt        =  pd.DataFrame(bssf_opt,     index = y.index, columns = [\"Subset_Size\"])\n",
    "\n",
    "# Cut off initial Training-Period\n",
    "sub_y              =  y.iloc[init:].copy()\n",
    "sub_cand_forecasts =  cand_forecasts.iloc[init:].copy()\n",
    "sub_benchmark      =  benchmark.iloc[init:].copy()\n",
    "sub_bssf_forecast  =  bssf_forecast.iloc[init:].copy()\n",
    "sub_cf_weights     =  cf_weights.iloc[init:].copy()\n",
    "sub_bssf_opt       =  bssf_opt.iloc[init:].copy()\n",
    "\n",
    "# OOS-Period\n",
    "oos_start  =  \"1999Q4\"\n",
    "oos_end    =  \"2022Q4\" \n",
    "oos_y             =  sub_y.loc[oos_start:oos_end].copy()\n",
    "oos_cand_forecast =  sub_cand_forecasts.loc[oos_start:oos_end].copy()\n",
    "oos_benchmark     =  sub_benchmark.loc[oos_start:oos_end].copy()\n",
    "oos_bssf_forecast =  sub_bssf_forecast.loc[oos_start:oos_end].copy()\n",
    "oos_cf_weights    =  sub_cf_weights.loc[oos_start:oos_end].copy()\n",
    "oos_bssf_opt      =  sub_bssf_opt.loc[oos_start:oos_end].copy()\n",
    "\n",
    "# Evaluation\n",
    "np.sum((oos_y.iloc[:,0] - oos_bssf_forecast.iloc[:,0]) ** 2) / np.sum((oos_y.iloc[:,0] - oos_benchmark.iloc[:,0]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Zero with NaN\n",
    "oos_cf_weights.replace({0:np.nan}, inplace=True)\n",
    "\n",
    "# Adapt Column-Values\n",
    "vec = list(range(1, oos_cf_weights.shape[1]+1))\n",
    "tmp = oos_cf_weights * vec\n",
    "\n",
    "# Dates\n",
    "tmp = tmp.reset_index(names=\"date\")\n",
    "\n",
    "# Plot \n",
    "# tmp.plot(x='date', y = tmp.columns[1:],\n",
    "#          figsize=(10, 5), legend=False,\n",
    "#          marker=\"o\", ms = 1, \n",
    "#          title=\"Selected Candidate Models\", ylabel=\"Selected Candidate Models\")\n",
    "#          #yticks = (np.arange(98), list(tmp.columns[1:])))\n",
    "\n",
    "tmp_long = pd.melt(tmp, id_vars = \"date\")\n",
    "tmp_long['variable'] = tmp_long['variable'].str.startswith(\"SSF\").replace({True: \"SSF\", False: \"CR\"})\n",
    "tmp_long.set_index(\"date\", inplace = True)\n",
    "tmp_long.groupby(\"variable\")[\"value\"].plot(legend=True, figsize = (10, 5),\n",
    "                                           marker=\"o\", ms = 2, lw = 0,\n",
    "                                           ylim = [-1, cand_forecasts.shape[1]+1],\n",
    "                                           title=\"Selected Candidate Models\", ylabel=\"Selected Candidate Models\")\n",
    "plt.show()\n",
    "\n",
    "# Subset Size\n",
    "oos_bssf_opt.plot(figsize=(10, 5), legend=False, \n",
    "                  color = \"black\", marker=\"o\", ms = 1, lw = 0,\n",
    "                  title = \"Subset Size\", xlabel=\"date\", ylabel=\"Selected Subset Size\",\n",
    "                  ylim  =  [min(bssf_range)-0.5, max(bssf_range)+0.5],\n",
    "                  yticks = np.arange(min(bssf_range), max(bssf_range)+1, step=1.0))\n",
    "plt.show()\n",
    "\n",
    "# CSSED\n",
    "cssed = np.cumsum(((oos_y.iloc[:,0] - oos_benchmark.iloc[:,0]) ** 2) - ((oos_y.iloc[:,0] - oos_bssf_forecast.iloc[:,0]) ** 2))\n",
    "cssed.plot(figsize=(10, 5),\n",
    "            xlabel = \"date\", ylabel = \"CSSED\", title = \"Cumulated Sum of Squared Error Differences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MC-Parameter ######\n",
    "# Number of MC-Runs\n",
    "n_mc  =  2\n",
    "\n",
    "# Set Parameter\n",
    "n_obs       =  100\n",
    "n_preds     =  8\n",
    "init        =  50\n",
    "mlags       =  0\n",
    "corr_range  =  [0.5, 0.95]\n",
    "b_range     =  np.array([[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "###### Parameter Subset Forecasts ######\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3, 4, 5, 6, 7, 8] \n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  \n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "n_sub  =  int(sum([min(n_models(n_preds, k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2] \n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 50) \n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) \n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  10.0\n",
    "n_times     =  1\n",
    "bssf_range  =  [1, 2, 3] \n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "######## Objects ########\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts   = np.full((len(range(init, n_obs)), (n_sub + n_cr)), np.nan)     \n",
    "benchmark        = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "cf_weights       = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf, (n_sub + n_cr)), np.nan)\n",
    "bssf_forecast    = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "csr_forecast     = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "se_benchmark     = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "se_bssf_forecast = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "se_csr_forecast  = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Monte-Carlo Runs\n",
    "i = 0\n",
    "for r in tqdm(range(n_mc)):\n",
    "    \n",
    "    # Loop over Covariance-Sets\n",
    "    for p in corr_range:\n",
    "        \n",
    "        # Loop over Coefficient-Sets\n",
    "        for b in b_range:\n",
    "    \n",
    "            ### Simulate Data ###\n",
    "            y, X, pred_names = sim(n_obs, n_preds, b, p, r)\n",
    "            \n",
    "            ### Benchmark: PHM ###\n",
    "            benchmark[i]    = y.iloc[:-1].mean().iloc[0]\n",
    "            se_benchmark[i] = (y.iloc[-1,0] - benchmark[i]) ** 2\n",
    "\n",
    "            # Loop over t / Create Candidate Models\n",
    "            for t in range(init, n_obs):\n",
    "            \n",
    "                ### Pre-Process Data ###\n",
    "                y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "\n",
    "                ### Subset Forecasts ###\n",
    "                feature_set  =  list(chain(*list(map(lambda k: complete_sub(list(range(1, X_train.shape[1])), k), k_range))))\n",
    "                preds_ssf    =  np.array(list(map(lambda feature: ssf(y_train, X_train, X_pred, feature, 0), feature_set)))\n",
    "            \n",
    "                ## Set up List to store Subset-Forecasts\n",
    "                #preds_ssf = np.full(n_sub, np.nan)\n",
    "                #idx_sub   = 0\n",
    "                #\n",
    "                ## Loop over Subset Size \n",
    "                #for k in k_range:\n",
    "                #\n",
    "                #    # Get all possible Subsets of length k\n",
    "                #    col_idx  = list(range(1, X_train.shape[1]))\n",
    "                #    subs_idx = complete_sub(col_idx, k)\n",
    "                #\n",
    "                #    # Randomly select n_upper Subsets\n",
    "                #    feature_set  =  subs_idx #random_select(subs_idx, n_max, random_state = 123)\n",
    "                #\n",
    "                #    # Loop over Subsets\n",
    "                #    for feature in feature_set:\n",
    "                #    \n",
    "                #        # Compute Subset-Regression-Forecast\n",
    "                #        pred  =  ssf(y_train, X_train, X_pred, feature, 0)\n",
    "                #        preds_ssf[idx_sub] = pred\n",
    "                #        idx_sub += 1\n",
    "\n",
    "                ### Compressed Regressions ###\n",
    "                preds_cr = np.array(list(chain(*[list(map(lambda rep: cr_reg(y_train, X_train, X_pred, n_comp, 0, rep), rep_range)) for n_comp in cr_range])))\n",
    "                \n",
    "                # # Set up List to store Compressed-Regression-Forecasts\n",
    "                # preds_cr   = np.full(n_cr, np.nan)\n",
    "                # idx_cr     = 0\n",
    "                # \n",
    "                # # Loop over number of Components\n",
    "                # for n_comp in cr_range:\n",
    "                # \n",
    "                #     # Loop over n repetitions\n",
    "                #     for rep in rep_range:\n",
    "                #     \n",
    "                #         # Compute Compressed-Regression-Forecasts\n",
    "                #         pred  =  cr_reg(y_train, X_train, X_pred, n_comp, 0, rep)\n",
    "                #         preds_cr[idx_cr] = pred\n",
    "                #         idx_cr += 1\n",
    "\n",
    "                # ### Decision Tree Regressions\n",
    "                # preds_dt = np.array(list(map(lambda r: dt_reg(y_train, X_train, X_pred, r), dt_range)))\n",
    "                \n",
    "                # # Set up Matrix to store Decision-Tree-Forecasts\n",
    "                # preds_dt   = np.full(n_dt, np.nan)\n",
    "                # \n",
    "                # # Loop over number of Components\n",
    "                # for idx_dt, r in enumerate(dt_range):\n",
    "                #     \n",
    "                #     # Compute Decision-Tree-Forecasts\n",
    "                #     pred  =  dt_reg(y_train, X_train, X_pred, r)\n",
    "                #     preds_dt[idx_dt] = pred[0]\n",
    "\n",
    "                # Append Results\n",
    "                cand_forecasts[t-init][:n_sub]             =  preds_ssf \n",
    "                cand_forecasts[t-init][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "                #cand_forecasts[t-init][(n_sub+n_cr):]     =  preds_dt\n",
    "                \n",
    "            ### Benchmark: Complete Subset Regression ###\n",
    "            tmp_ = np.cumsum([0] + [len(list(combinations(range(n_preds), k))) for k in k_range])\n",
    "            csr_forecast[i]     =  [np.mean(preds_ssf[tmp_[i]:tmp_[i+1]]) for i in range(len(tmp_)-1)]\n",
    "            se_csr_forecast[i]  =  (y_pred.iloc[0,0] - csr_forecast[i]) ** 2\n",
    "\n",
    "            ### Best Selection of Forecast ###\n",
    "            bssf_forecast[i], cf_weights[i] = zip(*list(map(lambda s: bssf(y_train[init:], cand_forecasts[:-1], cand_forecasts[-1], alpha, s, n_times), bssf_range)))\n",
    "            se_bssf_forecast[i] = (y_pred.values[0] - bssf_forecast[i]) ** 2\n",
    "            \n",
    "            # # Set up Matrix to store Forecasts\n",
    "            # kssf_forecasts  =  np.full(n_bssf, np.nan)\n",
    "            # kssf_weights    =  np.zeros([n_bssf, n_sub + n_cr]) \n",
    "\n",
    "            # # Loop over Subset Sizes\n",
    "            # for idx_bssf, s in enumerate(bssf_range):\n",
    "            #     \n",
    "            #     # Compute Best-Subset-Selection-of-Forecasts\n",
    "            #     pred = bssf(y_train[init:], cand_forecasts[:-1], cand_forecasts[-1], alpha, s, n_times)\n",
    "            #     bssf_forecast[i][idx_bssf] = pred[0]\n",
    "            #     cf_weights[i][idx_bssf]    = pred[1]\n",
    "            #     se_bssf_forecast[i][idx_bssf] = (y_pred.iloc[0,0] - pred[0]) ** 2\n",
    "             \n",
    "            # Update index   \n",
    "            i += 1\n",
    "            \n",
    "# # Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(n_preds), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "\n",
    "### Evaluation ###\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Fred-MD-Data stationary\n",
    "def transform_tcode(data):\n",
    "    \n",
    "    # Get Transformation-Code\n",
    "    tcode = data[0]\n",
    "    \n",
    "    # Get Data\n",
    "    data = data[1:]\n",
    "\n",
    "    if tcode == 1:\n",
    "        output = data\n",
    "    elif tcode == 2:\n",
    "        output = data - np.roll(data, 1)\n",
    "    elif tcode == 3:\n",
    "        output = (data - np.roll(data, 1)) - (np.roll(data, 1) - np.roll(data, 2))\n",
    "    elif tcode == 4:\n",
    "        output = np.log(data)\n",
    "    elif tcode == 5:\n",
    "        output = np.log(data) - np.roll(np.log(data), 1)\n",
    "    elif tcode == 6:\n",
    "        output = (np.log(data) - np.roll(np.log(data), 1)) - (np.roll(np.log(data), 1) - np.roll(np.log(data), 2))\n",
    "    else:\n",
    "        output = (data / np.roll(data, 1) - 1) - (np.roll(data, 1) / np.roll(data, 2) - 1)\n",
    "\n",
    "    return np.concatenate(([tcode], output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x_dataset  =  pd.read_csv(path + r'/fred_md_202306.csv')\n",
    "\n",
    "# Drop Variables with too many missing values\n",
    "x_dataset  =  x_dataset.drop([\"PERMIT\", \"PERMITNE\", \"PERMITMW\", \"PERMITS\",\n",
    "                              \"PERMITW\", \"ACOGNO\", \"ANDENOx\", \"CP3Mx\",\n",
    "                              \"COMPAPFFx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VIXCLSx\"],\n",
    "                              axis=1)\n",
    "\n",
    "# Transform remaining Columns\n",
    "x_dataset.iloc[:, 1:]  =  x_dataset.iloc[:,1:].apply(lambda x: transform_tcode(x))\n",
    "\n",
    "# Drop First Row with Transformation-Code\n",
    "x_dataset  =  x_dataset.iloc[1:,:]\n",
    "\n",
    "# Lag Data\n",
    "x_dataset.iloc[:,1:]  =  x_dataset.iloc[:,1:].shift(1)\n",
    "\n",
    "# Convert Date\n",
    "x_dataset['sasdate']  =  pd.to_datetime(x_dataset['sasdate'])\n",
    "\n",
    "# Filter Data\n",
    "x_dataset  =  x_dataset[(x_dataset['sasdate'] >= '1959-04-01') & (x_dataset['sasdate'] <= '2023-03-01')]\n",
    "\n",
    "# Resetting the index\n",
    "x_dataset.set_index('sasdate', inplace=True)\n",
    "x_dataset.index = x_dataset.index.to_period('M')\n",
    "\n",
    "# Load Data\n",
    "y_dataset  =  pd.read_csv(path + r'/fred_md_202306.csv')\n",
    "\n",
    "# Select and Rename Variables\n",
    "y_dataset  =  y_dataset.loc[:, [\"sasdate\", \"CPIAUCSL\", \"INDPRO\", \"UNRATE\"]]\n",
    "y_dataset  =  y_dataset.rename(columns={'CPIAUCSL': 'CPIAUCSL_h1', 'INDPRO': 'INDPRO_h1', 'UNRATE': 'UNRATE_h1'})\n",
    "\n",
    "# Transform Variables\n",
    "y_dataset[[\"CPIAUCSL_h1\"]]  =  y_dataset[[\"CPIAUCSL_h1\"]].apply(lambda x: 4 * 100 * (np.log(x) - np.roll(np.log(x), 1)))\n",
    "y_dataset[[\"INDPRO_h1\"]]    =  y_dataset[[\"INDPRO_h1\"]].apply(lambda x: 4 * 100 * (np.log(x) - np.roll(np.log(x), 1)))\n",
    "y_dataset[[\"UNRATE_h1\"]]    =  y_dataset[[\"UNRATE_h1\"]]\n",
    "\n",
    "# Drop First Row with Transformation-Code\n",
    "y_dataset  =  y_dataset.iloc[1:,:]\n",
    "\n",
    "# Convert Date\n",
    "y_dataset['sasdate']  =  pd.to_datetime(y_dataset['sasdate'])\n",
    "\n",
    "# Filter Data\n",
    "y_dataset  =  y_dataset[(y_dataset['sasdate'] >= '1959-04-01') & (y_dataset['sasdate'] <= '2023-03-01')]\n",
    "\n",
    "# Resetting the index\n",
    "y_dataset.set_index('sasdate', inplace=True)\n",
    "y_dataset.index = y_dataset.index.to_period('M')\n",
    "\n",
    "# Set Target Variable\n",
    "y  =  y_dataset.loc[:, [\"CPIAUCSL_h1\"]]\n",
    "X  =  x_dataset.drop(\"CPIAUCSL\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Selection of Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, X_train, X_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    X_train  =  X_train / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(X_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ X_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat[0])\n",
    "    Q         =  - 2 * diag_mat + X_train.transpose() @ X_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    # Initialize BQM\n",
    "    bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    bqm  =  bqm.from_qubo(Q)\n",
    "    \n",
    "    # Solve\n",
    "    solver     =  SimulatedAnnealingSampler()\n",
    "    #solver     =  SteepestDescentSolver()\n",
    "    #solver     =  TabuSampler()\n",
    "    #solver     =  TreeDecompositionSolver()\n",
    "\n",
    "    sampleset  =  solver.sample(bqm, num_reads = n_times)\n",
    "    solution   =  list(sampleset.first[0].values())\n",
    "    \n",
    "    # Prediction \n",
    "    pred       = solution @ X_pred\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series-Cross-Validation to Tune Parameter k\n",
    "def cv_ts(Y, X, splits, k_seq, alpha, n_times):\n",
    "\n",
    "    # Set up TS-Split\n",
    "    tscv    =  TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Set up Result-Matrix\n",
    "    cv_res  =  np.zeros((splits, len(k_seq)))\n",
    "\n",
    "    # Loop over Train-Test-Splits\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        # Split Data\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index],   Y.iloc[test_index]\n",
    "\n",
    "        # Cross-Validation k\n",
    "        for k in k_seq:\n",
    "\n",
    "            # Selected Predictors\n",
    "            solution    =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[1]\n",
    "\n",
    "            # Prediction\n",
    "            prediction  =  solution @ X_test.transpose()\n",
    "\n",
    "            # MSE\n",
    "            cv_res[i, k-1]  =  np.mean((Y_test.squeeze() - prediction) ** 2)\n",
    "\n",
    "            # Select k with smalltest average MSE\n",
    "            k  =  cv_res.mean(axis=0).argmin() + 1\n",
    "            \n",
    "    # Return \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate Forecasts\n",
    "cand_forecasts  =  results\n",
    "\n",
    "# Target Variable\n",
    "target_var  =  y_dataset.loc[:, [\"CPIAUCSL_h1\"]]\n",
    "\n",
    "# Get Dates\n",
    "dates  =  cand_forecasts.index.values\n",
    "\n",
    "# Match Time Frames\n",
    "target_var  =  target_var.loc[dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "alpha    =  1\n",
    "n_mods   =  cand_forecasts.shape[1]\n",
    "\n",
    "# Set Vector & Matrices\n",
    "X     =  cand_forecasts.copy()\n",
    "Y     =  target_var.copy()\n",
    "\n",
    "# Set Time Parameter\n",
    "init    =  int(12 * 5)\n",
    "final   =  len(Y)\n",
    "\n",
    "# Set up Empty Array\n",
    "predictions  =  np.zeros(final)\n",
    "predictions.fill(np.nan)\n",
    "\n",
    "# Loop\n",
    "for t in tqdm(range(init, final)):\n",
    "    \n",
    "    # Train Data\n",
    "    X_train  =  X.iloc[:t, ]\n",
    "    Y_train  =  Y.iloc[:t, ]\n",
    "    \n",
    "    # Prediction Data\n",
    "    X_pred   =  X.iloc[t, ]\n",
    "    \n",
    "    # Cross-Validation\n",
    "    splits   =  5\n",
    "    n_times  =  500\n",
    "    k_seq    =  [1, 2, 3, 4, 5]\n",
    "    k_opt    =  cv_ts(Y_train, X_train, splits, k_seq, alpha, n_times)\n",
    "    \n",
    "    # Prediction \n",
    "    n_times  =  500\n",
    "    predictions[t]  =  bssf(Y_train, X_train, X_pred, alpha, k_opt, n_times)[0]\n",
    "\n",
    "# Convert to Pandas Series\n",
    "predictions  =  pd.Series(predictions, name = \"qubo\", index = Y.index).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Target Variable\n",
    "target_var      =  pyreadr.read_r(path + '/Data/Results/Target_Var/target_var.RDS')[None]\n",
    "\n",
    "# Load all Candidate Forecasts\n",
    "cand_forecasts  =  pd.DataFrame()\n",
    "files           =  os.scandir(path + '/Data/Results/Candidate_Forecasts')\n",
    "\n",
    "# Loop\n",
    "for file in files:\n",
    "    if (file.path.endswith(\".RDS\")):\n",
    "        aux  =  pyreadr.read_r(file)[None]\n",
    "        cand_forecasts  =  pd.concat([cand_forecasts, aux], axis = 1)\n",
    "        \n",
    "# Drop Na\n",
    "cand_forecasts  =  cand_forecasts.dropna()\n",
    "\n",
    "# Get Dates\n",
    "dates           =  cand_forecasts.index.values\n",
    "\n",
    "# Match Time Frames\n",
    "target_var      =  target_var.loc[dates]\n",
    "\n",
    "# Dimensions\n",
    "print(cand_forecasts.shape)\n",
    "print(target_var.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, X_train, X_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    X_train  =  X_train / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(X_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ X_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat[0])\n",
    "    Q         =  - 2 * diag_mat + X_train.transpose() @ X_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    # Initialize BQM\n",
    "    bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    bqm  =  bqm.from_qubo(Q)\n",
    "    \n",
    "    # Solve\n",
    "    solver     =  SimulatedAnnealingSampler()\n",
    "    #solver     =  SteepestDescentSolver()\n",
    "    #solver     =  TabuSampler()\n",
    "    #solver     =  TreeDecompositionSolver()\n",
    "\n",
    "    sampleset  =  solver.sample(bqm, num_reads = n_times)\n",
    "    solution   =  list(sampleset.first[0].values())\n",
    "    \n",
    "    # Prediction \n",
    "    pred       = solution @ X_pred\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series-Cross-Validation to Tune Parameter k\n",
    "def cv_ts(Y, X, splits, k_seq, alpha, n_times):\n",
    "\n",
    "    # Set up TS-Split\n",
    "    tscv    =  TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Set up Result-Matrix\n",
    "    cv_res  =  np.zeros((splits, len(k_seq)))\n",
    "\n",
    "    # Loop over Train-Test-Splits\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        # Split Data\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index],   Y.iloc[test_index]\n",
    "\n",
    "        # Cross-Validation k\n",
    "        for k in k_seq:\n",
    "\n",
    "            # Selected Predictors\n",
    "            solution    =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[1]\n",
    "\n",
    "            # Prediction\n",
    "            prediction  =  solution @ X_test.transpose()\n",
    "\n",
    "            # MSE\n",
    "            cv_res[i, k-1]  =  np.mean((Y_test.squeeze() - prediction) ** 2)\n",
    "\n",
    "            # Select k with smalltest average MSE\n",
    "            k  =  cv_res.mean(axis=0).argmin() + 1\n",
    "            \n",
    "    # Return \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "alpha    =  1\n",
    "n_mods   =  cand_forecasts.shape[1]\n",
    "\n",
    "# Set Vector & Matrices\n",
    "X     =  cand_forecasts.copy()\n",
    "Y     =  target_var.copy()\n",
    "\n",
    "# Set Time Parameter\n",
    "init    =  int(12 * 5)\n",
    "final   =  len(Y)\n",
    "\n",
    "# Set up Empty Array\n",
    "predictions  =  np.zeros(final)\n",
    "predictions.fill(np.nan)\n",
    "\n",
    "# Loop\n",
    "for t in range(init, final):\n",
    "    \n",
    "    # Train Data\n",
    "    X_train  =  X.iloc[:t, ]\n",
    "    Y_train  =  Y.iloc[:t, ]\n",
    "    \n",
    "    # Prediction Data\n",
    "    X_pred   =  X.iloc[t, ]\n",
    "    \n",
    "    # Cross-Validation\n",
    "    splits   =  5\n",
    "    n_times  =  500\n",
    "    k_seq    =  [1, 2, 3, 4, 5]\n",
    "    k        =  cv_ts(Y_train, X_train, splits, k_seq, alpha, n_times)\n",
    "    \n",
    "    # Prediction \n",
    "    n_times  =  500\n",
    "    predictions[t]  =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[0]\n",
    "\n",
    "# Convert to Pandas Series\n",
    "predictions  =  pd.Series(predictions, name = \"qubo\", index = Y.index).to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Eval-Function (OOS-R2)\n",
    "def oos_r2(observation, prediction, benchmark):\n",
    "    \n",
    "    # Squared Error Model\n",
    "    se1  =  (observation - prediction) ** 2\n",
    "    \n",
    "    # Squared Error Benchmark\n",
    "    se2  =  (observation - benchmark) ** 2\n",
    "    \n",
    "    # Out-of-Sample R2\n",
    "    oos_r2  =  (1 - sum(se1) / sum(se2)) * 100\n",
    "    \n",
    "    # Return \n",
    "    return(oos_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OOS-Period\n",
    "eval_start  =  \"1974-12-01\"\n",
    "eval_end    =  \"2020-12-01\"\n",
    "\n",
    "# Keep only OOS-Period\n",
    "oos_target_var      =  target_var.loc[eval_start:eval_end].squeeze()\n",
    "oos_benchmark       =  cand_forecasts[eval_start:eval_end][\"pred_hist_mean\"]\n",
    "oos_bssf            =  predictions.loc[eval_start:eval_end].squeeze()\n",
    "oos_cand_forecasts  =  cand_forecasts[eval_start:eval_end]\n",
    "\n",
    "# Evaluate Best Subset Selection of Forecasts\n",
    "oos_r2(oos_target_var, oos_bssf, oos_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Candidate Models\n",
    "eval_cand_mods  =  oos_cand_forecasts.apply(lambda x: oos_r2(oos_target_var, x, oos_benchmark), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cand_mods.sort_values(ascending = False).to_frame(\"OOS-R2\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OOS-Period\n",
    "eval_start  =  \"1974-12-01\"\n",
    "eval_end    =  \"2020-12-01\"\n",
    "\n",
    "# Keep only OOS-Period\n",
    "oos_target_var      =  target_var.loc[eval_start:eval_end].squeeze()\n",
    "oos_benchmark       =  cand_forecasts[eval_start:eval_end][\"pred_hist_mean\"]\n",
    "oos_cand_forecasts  =  cand_forecasts[eval_start:eval_end]\n",
    "\n",
    "# Evaluate Candidate Models\n",
    "eval_cand_mods  =  oos_cand_forecasts.apply(lambda x: oos_r2(oos_target_var, x, oos_benchmark), axis = 0)\n",
    "\n",
    "# Show Results\n",
    "print(eval_cand_mods.filter(like = \"XGB\"))\n",
    "print(eval_cand_mods.filter(like = \"GBM\"))\n",
    "print(eval_cand_mods.filter(like = \"pcr\"))\n",
    "print(eval_cand_mods.filter(like = \"glm\"), n =)\n",
    "#eval_cand_mods.sort_values(ascending = False).to_frame(\"OOS-R2\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BQM\n",
    "qubo  =  { (i,j) : Q.iloc[i,j] for i in range(0, 11) for j in range(0, 11) }\n",
    "bqm   =  BinaryQuadraticModel('BINARY')\n",
    "bqm   =  bqm.from_qubo(Q)\n",
    "\n",
    "# Initialize BQM\n",
    "bqm = BinaryQuadraticModel('BINARY')\n",
    "\n",
    "# Add Linear Coefficients\n",
    "for i in range(0,11):\n",
    "    lin_coef  =  Q.iloc[i,i]\n",
    "    bqm.add_linear(i, lin_coef)\n",
    "    \n",
    "# Add Quadratic Coefficients\n",
    "for i in range(0,11):\n",
    "    for j in range(0,11):\n",
    "        if i != j:\n",
    "            quad_coef  =  Q.iloc[i,j]\n",
    "            bqm.add_quadratic(i, j, quad_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
