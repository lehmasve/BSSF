{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings \n",
    "\n",
    "import time\n",
    "\n",
    "#import multiprocess as mp\n",
    "from multiprocess import Pool\n",
    "\n",
    "#from joblib import Parallel, delayed\n",
    "\n",
    "#from functools import partial  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "\n",
    "#from statsmodels.tsa.tsatools import lagmat\n",
    "import statsmodels.api as sm\n",
    "#from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "#from sklearn import random_projection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.linear_model import LarsCV\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "#from sklearn.linear_model import ElasticNet\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from dimod import BinaryQuadraticModel\n",
    "from dwave.samplers import SimulatedAnnealingSampler\n",
    "#from dwave.samplers import SteepestDescentSolver\n",
    "#from dwave.preprocessing import roof_duality\n",
    "# from dwave.system import LeapHybridSampler\n",
    "# from dwave.system import DWaveSampler\n",
    "# from dwave.system import EmbeddingComposite\n",
    "# from dimod import ExactSolver\n",
    "# from dwave.samplers import TabuSampler\n",
    "# from dwave.samplers import TreeDecompositionSolver\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobi_optimods.qubo import solve_qubo\n",
    "gp.setParam('OutputFlag', 0)\n",
    "\n",
    "if os.name == 'nt':\n",
    "    import dill\n",
    "    dill.settings['recurse'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-01-09\n"
     ]
    }
   ],
   "source": [
    "###### File for Candidate Model Functions ######\n",
    "### Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import random_projection\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import gurobipy as gp\n",
    "gp.setParam('OutputFlag', 0)\n",
    "import os\n",
    "if os.name == 'nt':\n",
    "    import dill\n",
    "    dill.settings['recurse'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _cm import lambda_path, eln, crr, dtr, candidate_models, candidate_models_kf\n",
    "from _fm import bssf, bssf_cv, csr_cv, avg_best_cv, peLASSO_cv, psgd_cv, avg_best_cv_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate X\n",
    "y = np.random.normal(0, 1, 1000)\n",
    "X = np.random.normal(0, 1, (1000, 100))\n",
    "\n",
    "x_train = np.random.normal(0, 1, (250, 100))\n",
    "x_pred = np.random.normal(0, 1, (250, 100))\n",
    "y_train = np.sum(x_train, axis = 1) + np.random.normal(0, 5, 250)\n",
    "\n",
    "lambda_vec = np.linspace(0.1, 1, 10)\n",
    "alpha_vec = np.linspace(0.1, 1, 5)\n",
    "n_jobs = 1\n",
    "\n",
    "comp_vec = np.array([1, 2, 3, 4, 5])\n",
    "rep_range = range(1, 100)\n",
    "\n",
    "vec_depth = np.array([3, 4, 5])\n",
    "\n",
    "kfolds = 5\n",
    "ran_st = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_params = [\n",
    "    (\"eln\", {\"n_lambda\": 10, \"alpha_vec\": np.linspace(0.1, 1, 5), \"n_jobs\": 1}),\n",
    "    (\"crr\", {\"comp_vec\": np.array([1, 2, 3, 4, 5]), \"rep_range\": np.arange(1, 500), \"n_jobs\": 1}),\n",
    "    (\"dtr\", {\"vec_depth\": np.array([3, 4, 5]), \"n_jobs\": 1}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytmp, ptemp = candidate_models_kf(y, X, kfolds, models_params, ran_st, n_jobs = 5)\n",
    "ytmp.shape\n",
    "ptemp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 1, 1000)\n",
    "\n",
    "alpha = 100000.0\n",
    "timeout = 10.0\n",
    "method = \"gurobi\"\n",
    "k = 10\n",
    "vec_k = np.arange(1, 21)\n",
    "kfolds = 5\n",
    "ran_st = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat X times and save the third element of the tuple\n",
    "for i in range(10):\n",
    "    \n",
    "    cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "    cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "    y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 1000)\n",
    "    \n",
    "    _, _, res = bssf_cv(y_train, cf_train, cf_pred, alpha, vec_k, 1000, \"dwave\", kfolds, ran_st)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat X times and save the third element of the tuple\n",
    "for i in range(10):\n",
    "    \n",
    "    cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "    cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "    y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 1000)\n",
    "    \n",
    "    _, _, res = bssf_cv(y_train, cf_train, cf_pred, alpha, vec_k, timeout, \"qcbo\", kfolds, ran_st)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat X times and save the third element of the tuple\n",
    "for i in range(10):\n",
    "    \n",
    "    cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "    cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "    y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 1000)\n",
    "    \n",
    "    _, _, res = bssf_cv(y_train, cf_train, cf_pred, alpha, vec_k, timeout, \"gurobi\", kfolds, ran_st)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.normal(0, 1, (250, 100))\n",
    "x_pred = np.random.normal(0, 1, (250, 100))\n",
    "y_train = np.sum(x_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 250)\n",
    "\n",
    "vec_k = np.arange(1, 21)\n",
    "sampling = True\n",
    "kfolds = 5\n",
    "ran_st = 42\n",
    "\n",
    "csr_cv(y_train, x_train, x_pred, vec_k, sampling, kfolds, ran_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_cv_par(y_train, x_train, x_pred, vec_k, sampling, kfolds, ran_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.48641548,  0.29823941,  0.96492874,  0.32104404, -0.22822607,\n",
       "         0.70788622, -0.45489162,  0.45330513,  0.3663949 ,  0.06558947,\n",
       "         0.46682904, -0.33942384, -0.90481044, -0.46999687, -0.58119514,\n",
       "        -0.03823994,  0.27605657, -0.32952389,  0.63292255, -0.64538509,\n",
       "         0.0105698 , -0.93284268, -1.51621305, -0.01825803, -1.0217201 ,\n",
       "        -0.70941155,  0.59485541, -0.39027506,  0.82431233, -0.45858792,\n",
       "        -0.86737943, -0.32517115, -0.03653052, -1.27254278, -0.69287906,\n",
       "         0.49078506, -0.16181475,  1.72806762, -0.22347091,  0.57226818,\n",
       "        -0.71094711,  0.23864258, -0.30012492, -0.38648707, -0.47470815,\n",
       "         0.64189526, -0.02070353, -0.26530292,  0.6092145 , -0.49301251,\n",
       "         0.91626174,  0.18134312, -0.4114115 ,  0.44283262, -0.72494622,\n",
       "         0.91120456,  0.39965308, -1.44484193,  0.31957598,  0.7633932 ,\n",
       "        -0.24296486,  0.8828982 ,  0.20987059, -0.86186946,  0.28615917,\n",
       "         0.2633723 , -0.54592841,  0.35925062, -0.20955206, -0.64184776,\n",
       "         0.19185155,  0.23259301, -0.52234257,  0.32317189,  0.76473147,\n",
       "         0.24442147, -0.05866518,  0.51270172, -0.77707898, -0.70293375,\n",
       "         0.7086032 ,  1.21724668, -0.54926811,  0.46352214,  0.15617753,\n",
       "        -0.63487536,  0.91278468,  0.81713561, -0.53970166, -0.13098173,\n",
       "         1.99056403, -0.32716631,  0.31296963,  0.29526583, -0.86016055,\n",
       "        -0.54188855,  0.43372013, -0.00303188,  0.00831647,  1.29895026,\n",
       "         0.87330757, -0.774454  , -0.69291408, -0.87148631, -0.9881397 ,\n",
       "        -0.48709538, -0.44158018, -0.30076282,  0.26001286,  2.49060129,\n",
       "         0.57757387,  0.13183441, -0.69819508,  0.4231384 ,  0.70374297,\n",
       "        -0.72468896,  0.87407593, -1.35584058, -1.31197559, -0.42727518,\n",
       "         0.04387301, -0.64188359,  0.04035279,  0.80814083,  0.17915801,\n",
       "        -0.03759501, -0.17117194,  0.8634247 ,  0.89400936, -1.02646268,\n",
       "        -0.40870283, -1.07663335, -0.49601617,  1.46907905,  0.31221928,\n",
       "         0.0980058 , -0.49289822,  1.29571924, -0.006363  , -1.64103253,\n",
       "        -0.63091508, -0.40336489, -0.08694741,  0.07724568,  0.28818918,\n",
       "         0.17009483, -0.14328781, -1.65715965,  0.18342092,  0.3921194 ,\n",
       "         0.82570765, -0.36720144, -0.0081198 , -1.11398982,  0.98131316,\n",
       "         0.24687409,  0.11537834,  0.43465745,  0.01842656, -0.38078795,\n",
       "        -1.10792002, -0.70870189, -0.28212305,  1.21852913,  0.13215654,\n",
       "        -0.34597261,  0.53839565, -0.8038232 , -0.23637131,  0.04289925,\n",
       "        -0.11421433, -0.28043723, -0.35098505,  0.49950273, -0.31389448,\n",
       "        -0.45267238, -0.94548017,  0.12059862,  0.4022092 , -0.61468845,\n",
       "        -0.28757482, -0.06927296,  0.44870707, -0.31278249, -0.29196054,\n",
       "        -0.15553777, -0.58790234, -0.9154033 , -0.35863236, -0.51684161,\n",
       "         0.23636067,  0.44583096,  0.07461684, -0.73864723, -1.00505408,\n",
       "        -0.32942216,  0.91806192,  0.05611067, -1.07653876,  0.40989995,\n",
       "         0.04830964, -0.6717381 , -0.78826393,  0.01268008, -1.04466409,\n",
       "         0.0141891 , -0.08439568, -0.96915798,  0.8552601 ,  0.19521728,\n",
       "         1.25209495, -0.89886036, -0.92322236, -0.09957591,  0.03728772,\n",
       "         0.12403451,  0.05581157, -0.35262057, -0.05239396, -0.61349633,\n",
       "         0.13615889,  0.14182044, -1.37718247,  0.55912654,  1.18577459,\n",
       "        -0.66536354,  0.86343858,  0.10115383,  0.06819073, -0.29931214,\n",
       "        -0.28808436, -0.04858648,  0.21316174, -0.56873013,  0.37891367,\n",
       "        -0.46649382, -0.53590603,  0.74880585,  0.16666302, -1.09901583,\n",
       "        -0.64212602, -1.1696606 , -0.97666449,  1.31686152, -0.08681573,\n",
       "        -0.33967852, -0.48264606,  0.40638856, -0.98792162, -0.84917763]),\n",
       " 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_train = np.random.normal(0, 1, (250, 100))\n",
    "cf_pred = np.random.normal(0, 1, (250, 100))\n",
    "y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 250)\n",
    "\n",
    "vec_k = np.arange(1, 21)\n",
    "kfolds = 5\n",
    "ran_st = 42\n",
    "n_jobs = 5\n",
    "\n",
    "avg_best_cv(y_train, cf_train, cf_pred, vec_k, kfolds, ran_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.44393830e-01,  3.04777805e-01, -7.30719048e-02, -3.26082405e-01,\n",
       "         3.81329813e-01,  1.30972455e-01, -2.61571644e-01, -1.42097656e-01,\n",
       "        -8.44905315e-01,  4.02866129e-01,  4.21783096e-01, -9.73881031e-02,\n",
       "         2.59118699e-01, -1.63032294e-01, -2.22390572e-01,  3.24450696e-01,\n",
       "        -1.34905237e-01,  3.92479816e-01,  1.79175690e-01,  8.38567763e-01,\n",
       "        -9.73594292e-01,  1.21511869e-01, -7.82427872e-02,  3.06259158e-01,\n",
       "        -2.26138480e-01, -6.16735368e-01, -6.22661004e-02,  3.42196567e-01,\n",
       "        -7.31029073e-01,  1.65345286e-01, -4.05535443e-01,  6.76578567e-02,\n",
       "        -4.08318865e-02,  5.04584649e-02, -5.44232492e-02,  7.05748911e-01,\n",
       "         1.59861499e-01, -1.21258702e-01, -3.63573411e-01,  4.15538409e-01,\n",
       "        -1.23298028e-01, -3.09491964e-01, -7.09850046e-02,  5.04592889e-01,\n",
       "         2.24217723e-01,  5.39043112e-01,  3.71242331e-01, -1.52677312e-01,\n",
       "         5.58870090e-01, -9.97417194e-02, -6.69052413e-01, -2.28245855e-01,\n",
       "        -3.47432944e-01, -6.03074160e-03, -1.04152976e-01, -2.59312782e-03,\n",
       "         2.34690215e-01,  1.82377318e-01,  5.53341990e-02,  1.36229152e+00,\n",
       "        -1.10871762e-01,  5.19793730e-01, -1.28340394e-01,  1.98512041e-01,\n",
       "         7.26981554e-02, -7.05753031e-02,  4.44878029e-01, -4.73269982e-02,\n",
       "         3.47422143e-01, -4.18401525e-01, -2.43890957e-01,  1.20686253e-01,\n",
       "         1.70161174e-01,  1.21354134e-01,  3.46349879e-01,  6.84127263e-01,\n",
       "        -1.09717525e-01,  6.31737046e-01, -3.46298774e-01,  3.74904507e-01,\n",
       "        -1.35435165e-02,  4.53833789e-01, -6.34694852e-01, -3.67069823e-01,\n",
       "        -4.35515899e-01, -6.47592445e-01, -2.59303363e-01, -8.45019144e-01,\n",
       "        -4.35989261e-01, -2.22885574e-01, -3.24140955e-01,  9.66674693e-01,\n",
       "        -2.88999129e-02,  3.51060200e-01,  1.01961542e+00,  5.99179306e-03,\n",
       "         4.80784385e-01,  3.91526858e-01, -1.31079799e-01,  2.39633623e-01,\n",
       "        -2.48309983e-01,  3.15011834e-01,  1.23157639e-01,  5.48280361e-01,\n",
       "        -9.18634376e-02, -3.47437402e-03, -5.68149908e-01,  1.44167717e-01,\n",
       "         6.82470276e-01, -5.16664727e-01,  2.61650983e-01, -6.38154891e-01,\n",
       "        -3.78249276e-01, -2.50010920e-01, -5.81386563e-01,  8.54842454e-01,\n",
       "        -8.03739349e-01, -5.39973170e-01,  8.21178694e-02,  1.64364258e-01,\n",
       "        -1.39364528e-01,  4.88946232e-01, -9.19688068e-02, -4.36728377e-01,\n",
       "        -3.12548027e-01,  8.88086537e-01,  5.24567203e-01, -6.10202221e-01,\n",
       "         1.87042335e-01, -2.87069903e-01, -4.82029553e-01, -1.01765008e-01,\n",
       "         8.97303301e-01,  5.94055708e-01,  5.03522879e-01,  4.24304589e-01,\n",
       "         7.05767672e-02, -4.42223192e-02, -1.96493003e-02, -2.85491742e-01,\n",
       "        -1.29343468e-02, -7.00509397e-02, -3.10420269e-01,  6.15266255e-01,\n",
       "         1.11750293e-01, -4.98104740e-02,  4.72462127e-01, -3.40749636e-01,\n",
       "        -1.52966410e-02,  9.14578391e-01,  4.18941179e-01,  1.91515996e-01,\n",
       "        -2.34718062e-01, -3.19018184e-02, -8.40866656e-01,  7.76560586e-03,\n",
       "        -2.27653609e-01,  2.94217324e-01,  1.44987282e-01, -3.41743865e-01,\n",
       "         4.76197006e-01,  1.76847008e-01,  9.22789511e-01, -3.32175848e-01,\n",
       "         5.30547063e-01, -3.69346627e-01,  3.23311771e-01, -7.07900964e-01,\n",
       "        -1.00613875e-01,  2.87183442e-01,  4.66541721e-01,  3.56337338e-01,\n",
       "         2.08567151e-01,  3.09329360e-01, -2.80314958e-01, -2.91801106e-01,\n",
       "         2.37185849e-01,  1.87049036e-01,  7.30966157e-01, -2.62952256e-01,\n",
       "        -3.37027093e-02,  2.50220191e-01,  5.56103242e-01, -1.84363805e-01,\n",
       "         8.55048199e-04,  4.06386216e-01, -3.27671291e-01,  1.24260400e-01,\n",
       "         4.66655337e-01, -1.71053870e-01,  4.23217872e-03, -4.14348855e-02,\n",
       "         2.49271074e-01, -6.49912911e-01,  4.24557549e-01,  5.04270550e-01,\n",
       "         2.30210196e-01, -1.78069862e-01, -4.70393961e-01,  4.54742729e-01,\n",
       "        -2.36900033e-01, -6.36671057e-01, -9.19517958e-02, -7.38872494e-01,\n",
       "        -4.77137797e-01,  1.01297717e-03,  2.32456787e-01,  3.31769733e-01,\n",
       "        -9.15365958e-01, -7.23157784e-01, -9.02554041e-01, -4.63308960e-01,\n",
       "        -4.57416704e-01, -7.07261530e-02,  1.14603615e-01,  2.71971784e-01,\n",
       "         6.27677703e-01, -2.40229350e-01,  3.10793316e-01,  5.02898046e-01,\n",
       "        -3.57842561e-01,  7.91551581e-02,  4.75795224e-01, -1.04813053e-01,\n",
       "         3.80711719e-01,  1.66597468e-01,  2.44668157e-01,  5.75257705e-02,\n",
       "        -1.79874886e-01,  1.13601780e+00, -6.16394291e-01, -1.45730410e-02,\n",
       "         6.75059653e-02, -4.33669026e-02,  2.20751929e-01, -3.68479473e-01,\n",
       "         1.66407370e-01, -1.43763688e-01,  6.90765601e-01,  2.17043460e-01,\n",
       "        -1.23834308e-01,  8.10367232e-02,  1.55413608e-01, -7.10603393e-01,\n",
       "        -2.08563545e-01, -6.40766260e-01, -5.72102658e-01, -1.18081051e-01,\n",
       "        -4.06209546e-01, -1.42055987e-01]),\n",
       " 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_best_cv_par(y_train, cf_train, cf_pred, vec_k, kfolds, ran_st, n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_train = np.random.normal(0, 1, (1000, 100))\n",
    "cf_pred = np.random.normal(0, 1, (1000, 100))\n",
    "y_train = np.sum(cf_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 1000)\n",
    "cf_train\n",
    "kfolds = 5\n",
    "ran_st = 42\n",
    "n_jobs = 1\n",
    "\n",
    "peLASSO_cv(y_train, cf_train, cf_pred, kfolds, ran_st, n_jobs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.random.normal(0, 1, (200, 100))\n",
    "x_pred = np.random.normal(0, 1, (200, 100))\n",
    "y_train = np.sum(x_train[:, :10], axis = 1) + np.random.normal(0, 0.1, 200)\n",
    "\n",
    "n_models = 5\n",
    "split_grid = np.arange(1, 5)\n",
    "size_grid = np.array([10, 15, 20])\n",
    "kfolds = 5\n",
    "n_jobs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds, coefs = psgd_cv(y_train, x_train, x_pred, n_models, split_grid, size_grid, kfolds, n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpy2.robjects import numpy2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects as ro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Load the PSGD R package\n",
    "    psgd = importr('PSGD')\n",
    "\n",
    "    # Access the cv.PSGD function from the PSGD package\n",
    "    cv_psgd = psgd.cv_PSGD\n",
    "\n",
    "    # Convert to R objects\n",
    "    y_train_r = numpy2ri.py2rpy(y_train)\n",
    "    x_train_r = numpy2ri.py2rpy(x_train)\n",
    "    x_pred_r = numpy2ri.py2rpy(x_pred)\n",
    "    split_grid_r = numpy2ri.py2rpy(split_grid)\n",
    "    size_grid_r = numpy2ri.py2rpy(size_grid)\n",
    "\n",
    "    # Ensure Y is a matrix\n",
    "    y_train_r = ro.r.matrix(y_train_r, nrow=y_train.shape[0], ncol = 1)\n",
    "    \n",
    "    # Convert to Vector\n",
    "    split_grid_r = ro.FloatVector(split_grid)\n",
    "    size_grid_r = ro.FloatVector(size_grid)\n",
    "    group_index = ro.IntVector(range(1, n_models + 1))\n",
    "\n",
    "    # Fast-Best-Split-Selection\n",
    "    output = cv_psgd(x = x_train_r, y = y_train_r, n_models = float(n_models),\n",
    "                     model_type = \"Linear\", include_intercept = True,\n",
    "                     split_grid = split_grid_r, size_grid = size_grid_r,\n",
    "                     max_iter = float(100), cycling_iter = float(0), n_folds = float(kfolds), n_threads = float(n_jobs))\n",
    "\n",
    "    # Extract coefficients and make predictions as per your original R code\n",
    "    psgd_coef = ro.r['coef'](output, group_index = group_index)\n",
    "    psgd_predictions = ro.r['predict'](output, newx = x_pred_r, group_index = group_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Path\n",
    "path  =  os.path.dirname(os.getcwd()) # os.path.dirname(os.getcwd()) #r'/Users/slehmann/Library/CloudStorage/Dropbox/QUBO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "def sim_data(N_obs, n_obs, n_preds, non_zero, p, rho, scenario, snr, random_state):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to simulate data for Monte Carlo Study according to Fan and Lv (2008)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set Seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Simulating nonzero betas\n",
    "    b_nonzero = (-1) ** np.random.binomial(1, p, non_zero) * ((4 * np.log(n_obs) / np.sqrt(n_obs)) + abs(np.random.standard_normal(non_zero)))\n",
    "    \n",
    "    # Simulate all Betas\n",
    "    b = np.append(b_nonzero, np.repeat(0, n_preds - non_zero))\n",
    "    \n",
    "    ## Shuffle\n",
    "    #random.shuffle(b)\n",
    "    \n",
    "    # Indice of actice Betas\n",
    "    active = np.where(b != 0)[0]\n",
    "    \n",
    "    # Simulate Covariance Matrix\n",
    "    if scenario == 1:\n",
    "        \n",
    "        # Set Covariance Matrix between Predictors\n",
    "        cov_mat = np.full((n_preds, n_preds), rho)\n",
    "        np.fill_diagonal(cov_mat, 1.0)\n",
    "        \n",
    "    if scenario == 2:\n",
    "        \n",
    "        # Set Covariance Matrix between Predictors\n",
    "        cov_mat = np.zeros((n_preds, n_preds))\n",
    "        cov_mat[:non_zero, :non_zero] = rho\n",
    "        np.fill_diagonal(cov_mat, 1.0)\n",
    "\n",
    "    # Simulate Predictor-Time-Series\n",
    "    X = np.random.multivariate_normal([0.0]*n_preds, cov_mat, N_obs)\n",
    "    pred_names = \"X\"+pd.Series(range(1, n_preds+1)).astype(str) \n",
    "    \n",
    "    # Simulate Noise\n",
    "    adj   = np.sqrt((b.transpose() @ cov_mat @ b) / snr)\n",
    "    error = np.random.standard_normal(N_obs)   \n",
    "\n",
    "    # Set Target Variable\n",
    "    y = X @ b + adj * error\n",
    "    \n",
    "    # Return\n",
    "    return(y, X, pred_names, adj * error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Pre-Process Data\n",
    "def prepro(X_train, X_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to add intercept and to standardize data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize Data\n",
    "    scaler  =  StandardScaler()   \n",
    "    X_train =  scaler.fit_transform(X_train)\n",
    "    X_pred  =  scaler.transform(X_pred)\n",
    "    \n",
    "    # ## Add Constant\n",
    "    # X_train =  sm.add_constant(X_train)\n",
    "    # X_pred  =  sm.add_constant(X_pred, has_constant = 'add')\n",
    "    \n",
    "    return X_train, X_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Candidate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Candidate Models in one Function\n",
    "def candidate_models_(y_train_,\n",
    "                      y_pred_,\n",
    "                      X_train_,\n",
    "                      X_pred_,\n",
    "                      lambda_vec_,\n",
    "                      alpha_vec_,\n",
    "                      n_iter_,\n",
    "                      dim_vec_,\n",
    "                      rep_range_):\n",
    "    \n",
    "    ### Pre-Process Data ###\n",
    "    X_train, X_pred = prepro(X_train_, X_pred_)\n",
    "        \n",
    "    ### Elastic-Net-Forecasts ###\n",
    "    if (len(lambda_vec_) == 0) or (len(alpha_vec_) == 0):\n",
    "        preds_eln = np.empty((0, X_pred.shape[0]), float) \n",
    "    else: \n",
    "        preds_eln = eln(y_train_, X_train, X_pred, lambda_vec_, alpha_vec_, n_iter_)\n",
    "    \n",
    "    ### Compressed Regressions ###\n",
    "    if (np.sum(dim_vec_) == 0) or (np.sum(rep_range_) == 0):\n",
    "        preds_cr = np.empty((0, X_pred.shape[0]), float) \n",
    "    else:\n",
    "        preds_cr = cr_reg(y_train_, X_train, X_pred, dim_vec, rep_range)\n",
    "            \n",
    "    ### Concatenate Predictions ###\n",
    "    cand_forecasts = np.concatenate([preds_eln, preds_cr])\n",
    "    \n",
    "    ### Transpose\n",
    "    cand_forecasts = cand_forecasts.transpose()\n",
    "    \n",
    "    return y_pred_, cand_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Candidate Models in Parallel\n",
    "def candidate_models_cv(y, X, kfolds, krepeats, n_lambda, alpha_range, cr_range, rep_range, n_core, ran_st):\n",
    "    \n",
    "    # Set up Repeated K-Fold\n",
    "    rkf = RepeatedKFold(n_splits=kfolds, n_repeats=krepeats, random_state=ran_st)\n",
    "    \n",
    "    ### Calculate Lambda-Path for Elastic-Net ###\n",
    "    lambdapath = lambda_path(y, X, n_lambda)\n",
    "    \n",
    "    ### Create Candidate Models ###\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        y_preds, cand_forecasts = zip(*list(pool_.map(lambda idx: candidate_models_n(y[idx[0]], y[idx[1]], X[idx[0]], X[idx[1]], k_range, cr_range, rep_range), rkf.split(X, y))))\n",
    "        pool_.close()\n",
    "        \n",
    "    ### Concatenate Predictions ###\n",
    "    cand_forecasts = np.concatenate(cand_forecasts)\n",
    "    \n",
    "    ### Concatenate Targets ###\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "        \n",
    "    # Return\n",
    "    return y_preds, cand_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using R inside Python\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.ipython.html\n",
    "rpy2.ipython.html.init_printing()\n",
    "import rpy2.robjects.packages as rpackages\n",
    "from rpy2.robjects.vectors import StrVector\n",
    "from rpy2.robjects.packages import importr\n",
    "utils = rpackages.importr('utils')\n",
    "utils.chooseCRANmirror(ind=1)\n",
    "base = rpackages.importr('base')\n",
    "\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Install R Packages\n",
    "utils.install_packages('PSGD')\n",
    "\n",
    "# Load R Packages\n",
    "PSGD = importr('PSGD')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = PSGD.cv_PSGD(x = base.matrix(cf_train, ncol = cf_train.shape[1]), y = base.matrix(targets_train), n_models = 5.0, \n",
    "                      model_type = \"Linear\", include_intercept = True,\n",
    "                      split_grid = robjects.FloatVector([1.0, 2.0, 3.0, 4.0]), size_grid = robjects.FloatVector([1.0, 2.0, 3.0, 4.0]),\n",
    "                      max_iter = 100.0, cycling_iter = 5.0, n_folds = 5.0, n_threads = 1.0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(output)[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Angle Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and predict LARS-Models\n",
    "def lars_n(y_train, cf_train, cf_pred, n):    \n",
    "        \n",
    "    # Define Model\n",
    "    model = Lars(fit_intercept = True,\n",
    "                 fit_path = False,\n",
    "                 jitter = None,\n",
    "                 n_nonzero_coefs = n,\n",
    "                 random_state = 123)\n",
    "\n",
    "    # Fit Model\n",
    "    model.fit(cf_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(cf_pred)\n",
    "    \n",
    "    return(np.concatenate(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform LARS in Parallel\n",
    "def lars(y_train, cf_train, cf_pred, n_range, n_core):\n",
    "    \n",
    "    # Parallelize over Subset Size\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        predictions = np.array(pool_.map(lambda n: lars_n(y_train, cf_train, cf_pred, n), n_range))\n",
    "        pool_.close()\n",
    "\n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stepwise Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Forward Stepwise Selection\n",
    "def fss_n(y_train, cf_train, cf_pred, n):\n",
    "    \n",
    "        # Model\n",
    "        model = LinearRegression()\n",
    "    \n",
    "        # Sequential Forward Selection\n",
    "        sfs = SequentialFeatureSelector(model,\n",
    "                                        n_features_to_select = n,\n",
    "                                        direction = 'forward')\n",
    "    \n",
    "        # Select Features\n",
    "        active_set = sfs.fit(cf_train, y_train).get_support()\n",
    "    \n",
    "        # Fit Model\n",
    "        model.fit(cf_train[:, active_set], y_train)\n",
    "    \n",
    "        # Predict\n",
    "        pred = model.predict(cf_pred[:, active_set])\n",
    "        \n",
    "        # Return\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Forward Stepwise Selection in Parallel\n",
    "def fss(y_train, cf_train, cf_pred, n_range, n_core):\n",
    "    \n",
    "    # Parallelize over Subset Size\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        predictions = np.array(pool_.map(lambda n: fss_n(y_train, cf_train, cf_pred, n), n_range))\n",
    "        pool_.close()\n",
    "\n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially-Egalitarian Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and predict peLASSO-Models\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "def peLASSO(y_train, cf_train, cf_pred, n_alpha, n_iter, cv_splits, cv_repeats):\n",
    "    \n",
    "    ### Step 1: Select to zero\n",
    "    # Define Cross-Validation Method\n",
    "    cv = RepeatedKFold(n_splits = cv_splits,\n",
    "                       n_repeats = cv_repeats,\n",
    "                       random_state = 123)\n",
    "    \n",
    "    # Define Model\n",
    "    model_lasso = LassoCV(fit_intercept = True,\n",
    "                          n_alphas = n_alpha,\n",
    "                          max_iter = n_iter,\n",
    "                          cv = cv,\n",
    "                          verbose = 0,\n",
    "                          n_jobs=1)\n",
    "    \n",
    "    # Fit Model\n",
    "    model_lasso.fit(cf_train, y_train)\n",
    "    \n",
    "    # Get & select only active candidate models\n",
    "    active_cf_train = cf_train[:, model_lasso.coef_.astype(bool)]\n",
    "    active_cf_pred  = cf_pred[: , model_lasso.coef_.astype(bool)]\n",
    "\n",
    "    ### Step 2: Shrink towards equality\n",
    "    # Check if active candidate models exist\n",
    "    if active_cf_train.shape[1] > 0:\n",
    "        \n",
    "        mean_cf = active_cf_train.mean(axis = 1)\n",
    "    \n",
    "        # Define Cross-Validation Method\n",
    "        cv = RepeatedKFold(n_splits = cv_splits,\n",
    "                           n_repeats = cv_repeats,\n",
    "                           random_state = 1)\n",
    "    \n",
    "        # Define Model\n",
    "        model_elasso = LassoCV(fit_intercept = True,\n",
    "                               n_alphas = n_alpha,\n",
    "                               max_iter = n_iter,\n",
    "                               cv = cv,\n",
    "                               verbose = 0,\n",
    "                               n_jobs=1)\n",
    "    \n",
    "        # Fit Model\n",
    "        model_elasso.fit(active_cf_train, (y_train-mean_cf))\n",
    "        \n",
    "        # Coefficients\n",
    "        coefs = model_elasso.coef_ + 1.0 / active_cf_train.shape[1]\n",
    "    \n",
    "        # Predict\n",
    "        pred = active_cf_pred @ coefs\n",
    "            \n",
    "    else:\n",
    "        print(\"peLASSO: No active candidate models\")\n",
    "        \n",
    "        # Set Prediction to mean\n",
    "        pred = [y_train.mean()] * cf_pred.shape[0]\n",
    "        \n",
    "    # Return Prediction\n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate X \n",
    "X = np.random.normal(0, 1, (250, 5))\n",
    "Y = np.sum(X, axis = 1) + np.random.normal(0, 5, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha = .5,\n",
    "              max_iter = 1000,\n",
    "              fit_intercept = False)\n",
    "model.fit(X, Y)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha = 2.0,\n",
    "              max_iter = 1000,\n",
    "              fit_intercept = False)\n",
    "model.fit(X, Y - np.mean(X, axis = 1))\n",
    "model.coef_ + 1.0 / X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average-Best Forecast Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual-based average-best forecast combination\n",
    "def avg_best(y_train, cf_train, cf_pred, n_range):\n",
    "    \n",
    "    # Set up Array\n",
    "    predictions = np.full((len(n_range), cf_pred.shape[0]), np.nan)\n",
    "    \n",
    "    # Calculate Squared Errors\n",
    "    se = ([[value] for value in y_train] - cf_train) ** 2\n",
    "\n",
    "    # Mean-Squared-Error\n",
    "    mse = np.mean(se, axis = 0)\n",
    "\n",
    "    # Get indices of the average-best N candidate models\n",
    "    ind = np.argsort(mse)\n",
    "\n",
    "    # Init Counter\n",
    "    i = 0\n",
    "    \n",
    "    # Loop over Subset Size\n",
    "    for n in n_range:\n",
    "        \n",
    "        # Predict\n",
    "        pred = np.mean(cf_pred[:, ind[:n]], axis = 1)\n",
    "        \n",
    "        # Append Prediction\n",
    "        predictions[i] = pred\n",
    "        \n",
    "        # Update Counter\n",
    "        i += 1\n",
    "        \n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Best Subset Selection of Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(y_train, cf_train, cf_pred, alpha, n_sub, bssf_timeout, method):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    cf_train  =  cf_train / n_sub\n",
    "    cf_pred   =  cf_pred  / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(cf_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(y_train.transpose() @ cf_train + alpha * n_sub)\n",
    "    Q         =  - 2 * np.diag(aux_mat) + cf_train.transpose() @ cf_train + alpha * ivec @ ivec.transpose()\n",
    "    \n",
    "    if method == \"dwave\":\n",
    "\n",
    "        # Initialize BQM\n",
    "        bqm  =  BinaryQuadraticModel('BINARY')\n",
    "        bqm  =  bqm.from_qubo(Q)\n",
    "\n",
    "        # Normalize\n",
    "        bqm.normalize()\n",
    "\n",
    "        # Preprocess (?)\n",
    "        #roof_duality(bqm)    \n",
    "\n",
    "        # Select Solver\n",
    "        solver_qpu  =  SimulatedAnnealingSampler() #LeapHybridSampler() SimulatedAnnealingSampler() EmbeddingComposite(DWaveSampler())\n",
    "        #solver_pp   =  SteepestDescentSolver()    #SteepestDescentSolver()\n",
    "\n",
    "        # Submit for Solution\n",
    "        sampleset  =  solver_qpu.sample(bqm, \n",
    "                                        num_reads = n_times,\n",
    "                                        #time_limit = 90,\n",
    "                                        label = \"Best Subset Selection of Forecasts\",\n",
    "                                        seed = 123) # f'Best Subset Selection of Forecasts{t}'\n",
    "\n",
    "        ## Postprocess Problem\n",
    "        #sampleset_pp = solver_pp.sample(bqm,\n",
    "        #                                initial_states = sampleset.lowest())\n",
    "\n",
    "        # Get Solution\n",
    "        solution    =  np.array(list(sampleset.first[0].values()))\n",
    "    \n",
    "    if method == \"qubo\":\n",
    "    \n",
    "        # Set up Model\n",
    "        model = gp.Model()\n",
    "        model.Params.TimeLimit = bssf_timeout\n",
    "        \n",
    "        # Decision Variables\n",
    "        b = model.addMVar(shape=Q.shape[0], vtype=gp.GRB.BINARY, name=\"b\")\n",
    "        \n",
    "        # Objective Function\n",
    "        model.setObjective(b @ Q @ b, gp.GRB.MINIMIZE)\n",
    "        \n",
    "        # Optimize\n",
    "        model.optimize()\n",
    "        solution = np.array(model.x)\n",
    "        \n",
    "    if method == \"qcbo\":\n",
    "        \n",
    "        # Set up Model\n",
    "        model = gp.Model()\n",
    "        model.params.timelimit = bssf_timeout\n",
    "\n",
    "        # Decision Variables\n",
    "        b = model.addMVar(shape=cf_train.shape[1], vtype=gp.GRB.BINARY, name=\"b\")\n",
    "        norm_0 = model.addVar(lb=n_sub, ub=n_sub, name=\"norm\")\n",
    "\n",
    "        # Objective Function\n",
    "        model.setObjective(b.T @ cf_train.T @ cf_train @ b\n",
    "                           - 2*y_train.T @ cf_train @ b\n",
    "                           + np.dot(y_train, y_train), gp.GRB.MINIMIZE)\n",
    "\n",
    "        # L0-Norm Constraint\n",
    "        model.addGenConstrNorm(norm_0, b, which=0, name=\"budget\")\n",
    "\n",
    "        # Optimize\n",
    "        model.optimize()\n",
    "        solution = np.array(model.x)[:-1]\n",
    "    \n",
    "    # Test Solution\n",
    "    if np.sum(solution) != n_sub:\n",
    "        print(f\"Warning: Number of selected features does not match --- {np.sum(solution)} instead of {n_sub}!\")\n",
    "    \n",
    "    # Prediction \n",
    "    pred = solution @ cf_pred.transpose()\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_st = 0\n",
    "train = 90\n",
    "n_obs = 100\n",
    "bernoulli_p = 0.2\n",
    "n_preds = 10\n",
    "kfolds = 5\n",
    "krepeats = 1\n",
    "k_range_ssf = [1, 2]\n",
    "k_range_cr = [1, 2]\n",
    "rep_range_cr = range(5)\n",
    "n_core = 1\n",
    "\n",
    "### Simulate Data ###\n",
    "y, X, pred_names, error = sim_data(n_obs, train, n_preds, 2, bernoulli_p, 0.2, 1, 0.3, 0)\n",
    "\n",
    "### Split in Train and Test Data ###\n",
    "y_train, y_pred, X_train, X_pred = y[:train].copy(), y[train:].copy(), X[:train].copy(), X[train:].copy()\n",
    "### Create Train Candidate Models ###\n",
    "#targets_train, cf_train = candidate_models(y_train, X_train, kfolds, krepeats, k_range_ssf, k_range_cr, rep_range_cr, n_core, ran_st)\n",
    "    \n",
    "### Create Test Candidate Models ###\n",
    "#targets_test, cf_test = candidate_models_n(y_train, y_pred, X_train, X_pred, k_range_ssf, k_range_cr, rep_range_cr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MC-Parameter ######\n",
    "# Number of MC-Runs\n",
    "n_mc   =  10\n",
    "n_core =  6\n",
    "\n",
    "# Set Parameter\n",
    "n_obs       =  1050\n",
    "train       =  50\n",
    "n_preds     =  10\n",
    "b_range     =  [2, 5, 8]\n",
    "bernoulli_p =  0.2\n",
    "corr_range  =  [0.2, 0.5, 0.8]\n",
    "scenario_range = [1, 2]\n",
    "snr_range   =  [0.1, 0.3, 0.5]  # --> PVE = SNR / (1 + SNR)\n",
    "\n",
    "### Cross Validation ###\n",
    "kfolds   = 5\n",
    "krepeats = 1\n",
    "\n",
    "###### Parameter Subset Forecasts ######\n",
    "# Subset Lengths\n",
    "k_range_ssf = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  \n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "if np.sum(k_range_ssf) == 0:\n",
    "    n_sub  =  0\n",
    "else:\n",
    "    n_sub  =  int(sum([min(n_models(n_preds, k), n_max) for k in k_range_ssf]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "k_range_cr  =  [0] #[1, 2, 3, 4] \n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range_cr  =  [0] #range(0, 60) \n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "if (np.sum(k_range_cr) == 0) or (len(rep_range_cr) == 0):\n",
    "    n_cr = 0\n",
    "else: \n",
    "    n_cr  =  len(k_range_cr) * len(rep_range_cr)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) \n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter LARS ######\n",
    "k_range_lars = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "###### Parameter Forward Stepwise Selection ######\n",
    "k_range_fss = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  ### -> klein machen\n",
    "\n",
    "###### Parameter peLasso ######\n",
    "n_alpha        = 200\n",
    "n_iter_peL     = 1000\n",
    "cv_splits_peL  = 5\n",
    "cv_repeats_peL = 1\n",
    "\n",
    "###### Parameter Average-Best ######\n",
    "k_range_avg_best = [1, 5, 10, 25, 50, 100, 250, 500, 750, 1023] \n",
    "\n",
    "###### Parameter BSSF ######\n",
    "bssf_alpha   =  2000\n",
    "bssf_timeout =  1.0\n",
    "k_range_bssf =  [1, 5, 10, 25, 50, 100, 250, 500, 750, 1023] \n",
    "n_bssf       =  len(k_range_bssf)\n",
    "bssf_method  =  \"qubo\"\n",
    "\n",
    "######## Objects ########\n",
    "# Set up Matrices for Results\n",
    "cf_train = np.full((train, (n_sub + n_cr)), np.nan)  \n",
    "cf_test  = np.full((n_obs - train, (n_sub + n_cr)), np.nan)  \n",
    "\n",
    "benchmark        = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc), np.nan) ####\n",
    "se_benchmark     = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "\n",
    "cf_weights       = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, (n_sub + n_cr)), np.nan)\n",
    "bssf_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, n_obs - train), np.nan)\n",
    "se_bssf_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, n_obs - train), np.nan)\n",
    "\n",
    "csr_forecast     = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_ssf), n_obs - train), np.nan)\n",
    "se_csr_forecast  = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_ssf), n_obs - train), np.nan)\n",
    "\n",
    "lars_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_lars), n_obs - train), np.nan)\n",
    "se_lars_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_lars), n_obs - train), np.nan)\n",
    "\n",
    "fss_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_fss), n_obs - train), np.nan)\n",
    "se_fss_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_fss), n_obs - train), np.nan)\n",
    "\n",
    "pelasso_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "se_pelasso_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "\n",
    "avg_best_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_avg_best), n_obs - train), np.nan)\n",
    "se_avg_best_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_avg_best), n_obs - train), np.nan)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Monte-Carlo Runs\n",
    "program_starts = time.time()\n",
    "for r, ran_st in enumerate(tqdm(range(n_mc))):\n",
    "    \n",
    "    # Loop over Covariance-Scenario\n",
    "    for c, sce in enumerate(scenario_range):\n",
    "        \n",
    "        # Loop over Signal-to-Noise-Ratio\n",
    "        for s, snr in enumerate(snr_range):\n",
    "    \n",
    "            # Loop over Covariance-Sets\n",
    "            for p, rho in enumerate(corr_range):\n",
    "        \n",
    "                # Loop over Coefficient-Sets\n",
    "                for b, beta in enumerate(b_range):\n",
    "    \n",
    "                    ### Simulate Data ###\n",
    "                    y, X, pred_names, error = sim_data(n_obs, train, n_preds, beta, bernoulli_p, rho, sce, snr, ran_st)\n",
    "                    \n",
    "                    ### Split in Train and Test Data ###\n",
    "                    y_train, y_pred, X_train, X_pred = y[:train].copy(), y[train:].copy(), X[:train].copy(), X[train:].copy()\n",
    "\n",
    "                    ### Create Train Candidate Models ###\n",
    "                    targets_train, cf_train = candidate_models(y_train, X_train, kfolds, krepeats, k_range_ssf, k_range_cr, rep_range_cr, n_core, ran_st)\n",
    "                        \n",
    "                    ### Create Test Candidate Models ###\n",
    "                    targets_test, cf_test = candidate_models_n(y_train, y_pred, X_train, X_pred, k_range_ssf, k_range_cr, rep_range_cr) \n",
    "\n",
    "                    ### Benchmark: PHM ###\n",
    "                    benchmark[c][s][p][b][r] =  targets_train.mean()\n",
    "                    se_benchmark[c][s][p][b][r] = (targets_test - targets_train.mean()) ** 2\n",
    "\n",
    "                    ### Benchmark: Complete Subset Regression ###\n",
    "                    idx_ss = np.cumsum([0] + [len(list(combinations(range(n_preds), k))) for k in k_range_ssf])\n",
    "                    csr_forecast[c][s][p][b][r] = [np.mean(cf_test[:, :n_sub][:, idx_ss[i]:idx_ss[i+1]], axis = 1) for i in range(len(idx_ss)-1)]\n",
    "                    se_csr_forecast[c][s][p][b][r] = (targets_test - csr_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: LARS ###\n",
    "                    lars_forecast[c][s][p][b][r] = lars(targets_train, cf_train, cf_test, k_range_lars, n_core)\n",
    "                    se_lars_forecast[c][s][p][b][r] = (targets_test - lars_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: Forward Stepwise Selection ###\n",
    "                    fss_forecast[c][s][p][b][r] = fss(targets_train, cf_train, cf_test, k_range_fss, n_core)\n",
    "                    se_fss_forecast[c][s][p][b][r] = (targets_test - fss_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: peLASSO ###\n",
    "                    pelasso_forecast[c][s][p][b][r] = peLASSO(targets_train, cf_train, cf_test, n_alpha, n_iter_peL, cv_splits_peL, cv_repeats_peL)\n",
    "                    se_pelasso_forecast[c][s][p][b][r] = (targets_test - pelasso_forecast[c][s][p][b][r]) ** 2                    \n",
    "\n",
    "                    ### Benchmark: Average-Best ###\n",
    "                    avg_best_forecast[c][s][p][b][r] = avg_best(targets_train, cf_train, cf_test, k_range_avg_best)\n",
    "                    se_avg_best_forecast[c][s][p][b][r] = (targets_test - avg_best_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Best Selection of Forecast ###\n",
    "                    bssf_forecast[c][s][p][b][r], cf_weights[c][s][p][b][r] = zip(*list(map(lambda k_bssf: bssf(targets_train, cf_train, cf_test, bssf_alpha, k_bssf, bssf_timeout, bssf_method), k_range_bssf)))\n",
    "                    se_bssf_forecast[c][s][p][b][r] = (targets_test - bssf_forecast[c][s][p][b][r]) ** 2\n",
    "    \n",
    "    ### Save \n",
    "    # ...\n",
    "    \n",
    "# Time                \n",
    "program_ends = time.time()\n",
    "         \n",
    "# Loop over all combinations\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        for s in range(len(snr_range)):\n",
    "            for c in range(len(scenario_range)):\n",
    "                \n",
    "\n",
    "                # Calculate Forecast Combination Method Performances  \n",
    "                mse_bssf = np.sum(np.sum(se_bssf_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_csr  = np.sum(np.sum( se_csr_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_lars = np.sum(np.sum(se_lars_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_fss       =  np.sum(np.sum(se_fss_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_pelasso   =  np.sum(np.sum(se_pelasso_forecast[c][s][p][b], axis = 1), axis = 0)\n",
    "                mse_avg_best  =  np.sum(np.sum(se_avg_best_forecast[c][s][p][b], axis = 2), axis = 0)                \n",
    "\n",
    "                # Calculate Benchmark Performance  \n",
    "                mse_phm  =  np.sum(np.sum(se_benchmark[c][s][p][b], axis = 1), axis = 0)\n",
    "\n",
    "                # Create Rows\n",
    "                new_row_bbsf = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"BSSF\",       \"OOS-R2\": np.array2string(100 * (1 - mse_bssf / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_csr  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"CSR\",        \"OOS-R2\": np.array2string(100 * (1 - mse_csr  / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_lars = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"LARS\",       \"OOS-R2\": np.array2string(100 * (1 - mse_lars / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_fss  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"FSS\",        \"OOS-R2\": np.array2string(100 * (1 - mse_fss  / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_peL  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"peLASSO\",    \"OOS-R2\": np.array2string(100 * (1 - mse_pelasso / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_avgB = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"Avg_Best_N\", \"OOS-R2\": np.array2string(100 * (1 - mse_avg_best / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "\n",
    "                # Add Rows\n",
    "                new_rows = pd.DataFrame.from_dict([new_row_bbsf,\n",
    "                                                   new_row_csr,\n",
    "                                                   new_row_lars,\n",
    "                                                   new_row_fss,\n",
    "                                                   new_row_peL,\n",
    "                                                   new_row_avgB\n",
    "                                                   ])\n",
    "                \n",
    "                ## Add to CSV\n",
    "                #output_path = path + \"/Results/my_csv.csv\"\n",
    "                #new_rows.to_csv(output_path, mode='a', header=not os.path.exists(output_path))\n",
    "                \n",
    "                # Print Results\n",
    "                print(f\"BSSF:       Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_bssf / mse_phm), 2)) + \"%\")\n",
    "                print(f\"CSR:        Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_csr  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"LARS:       Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_lars / mse_phm), 2)) + \"%\")\n",
    "                print(f\"FSS:        Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_fss  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"peLASSO:    Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_pelasso  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"Avg-Best N: Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_avg_best / mse_phm), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ### Simulate Data ###\n",
    "                    y, X, pred_names, error = sim_data(n_obs, train, n_preds, 2, bernoulli_p, 0.2, 1, 0.3, 0)\n",
    "                    \n",
    "                    ### Split in Train and Test Data ###\n",
    "                    y_train, y_pred, X_train, X_pred = y[:train].copy(), y[train:].copy(), X[:train].copy(), X[train:].copy()\n",
    "\n",
    "                    ### Create Train Candidate Models ###\n",
    "                    targets_train, cf_train = candidate_models(y_train, X_train, kfolds, krepeats, k_range_ssf, k_range_cr, rep_range_cr, n_core, 0)\n",
    "                        \n",
    "                    ### Create Test Candidate Models ###\n",
    "                    targets_test, cf_test = candidate_models_n(y_train, y_pred, X_train, X_pred, k_range_ssf, k_range_cr, rep_range_cr) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up DataFrame\n",
    "dataframe_plot = pd.DataFrame()\n",
    "\n",
    "# Create DataFrame\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        # Subset Weights\n",
    "        chosen_cm = cf_weights[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "        # Aggregated\n",
    "        chosen_cm = np.sum(chosen_cm, axis = 0)\n",
    "\n",
    "        # Replace Zero with NaN\n",
    "        chosen_cm[chosen_cm == 0] = np.nan\n",
    "        chosen_cm_agg = chosen_cm.copy()\n",
    "\n",
    "        # Set to One\n",
    "        chosen_cm[chosen_cm > 1 ] = 1\n",
    "\n",
    "        # Adapt Column-Values\n",
    "        chosen_cm = chosen_cm * np.arange(1, chosen_cm.shape[1]+1)\n",
    "\n",
    "        # Create DataFrame\n",
    "        chosen_cm = pd.DataFrame(chosen_cm, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "        chosen_cm_agg = pd.DataFrame(chosen_cm_agg, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "\n",
    "        # Index\n",
    "        chosen_cm = chosen_cm.reset_index(names=\"INDEX\")\n",
    "        chosen_cm_agg = chosen_cm_agg.reset_index(names=\"INDEX\")\n",
    "\n",
    "        # Melt Data\n",
    "        chosen_cm = chosen_cm.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Value')\n",
    "        chosen_cm_agg = chosen_cm_agg.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Weight')\n",
    "\n",
    "        # Concatenate\n",
    "        chosen_cm = pd.concat([chosen_cm, chosen_cm_agg])\n",
    "        \n",
    "        # Transform Candidate Model Names\n",
    "        chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.split('_').str[0]\n",
    "        \n",
    "        # Add Column for Correlation\n",
    "        chosen_cm['Correlation'] = corr_range[p] \n",
    "        \n",
    "        # Add Column for Betas\n",
    "        chosen_cm['Betas'] = str(b_range[b])\n",
    "        \n",
    "        # Append\n",
    "        dataframe_plot = pd.concat([dataframe_plot, chosen_cm])\n",
    "        \n",
    "# Plot Theme\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Plot Data\n",
    "g = sns.relplot(\n",
    "    data = dataframe_plot,\n",
    "    y = 'Value', x = 'INDEX',\n",
    "    hue = \"Candidate_Model\", size = \"Weight\", \n",
    "    col = \"Betas\", row = \"Correlation\",\n",
    "    sizes = (3, 75), height = 5.0, aspect = 1.5,\n",
    "    alpha = 0.75, palette = \"muted\",\n",
    "    #legend = True\n",
    "    )\n",
    "\n",
    "# Legend\n",
    "#g._legend.remove()\n",
    "#h, l = g.ax.get_legend_handles_labels()\n",
    "#g.ax.legend(h[0:13], l[0:13], bbox_to_anchor=(1.0, 0.75), loc=2, fontsize=10, frameon=False)\n",
    "\n",
    "# Title & Axis\n",
    "g.set(xlabel='Combination Size',\n",
    "      ylabel='Candidate Models')\n",
    "      #title=f\"Selected Candidate Models: \\n Correlation {corr_range[p]} & Betas {b_range[b]}\")\n",
    "\n",
    "# Margins\n",
    "#g.ax.margins(x = 0.05, y = 0)\n",
    "\n",
    "# Set number of ticks for y-axis\n",
    "idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "#g.ax.set_yticks(idx)\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "#g.ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "## iterate over axes of FacetGrid\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_yticks(idx)\n",
    "    ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "# Tick-Label Size\n",
    "g.set_yticklabels(size = 8)\n",
    "g.set_xticklabels(size = 10)\n",
    "\n",
    "# Add Horizontal Lines\n",
    "#for i in idx:\n",
    "#    g.ax.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "#for label in g.ax.get_xticklabels():\n",
    "#    label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        # Subset Weights\n",
    "        chosen_cm = cf_weights[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "        # Aggregated\n",
    "        chosen_cm = np.sum(chosen_cm, axis = 0)\n",
    "\n",
    "        # Replace Zero with NaN\n",
    "        chosen_cm[chosen_cm == 0] = np.nan\n",
    "        chosen_cm_agg = chosen_cm.copy()\n",
    "\n",
    "        # Set to One\n",
    "        chosen_cm[chosen_cm > 1 ] = 1\n",
    "\n",
    "        # Adapt Column-Values\n",
    "        chosen_cm = chosen_cm * np.arange(1, chosen_cm.shape[1]+1)\n",
    "\n",
    "        # Create DataFrame\n",
    "        chosen_cm = pd.DataFrame(chosen_cm, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "        chosen_cm_agg = pd.DataFrame(chosen_cm_agg, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "\n",
    "        # Index\n",
    "        chosen_cm = chosen_cm.reset_index(names=\"INDEX\")\n",
    "        chosen_cm_agg = chosen_cm_agg.reset_index(names=\"INDEX\")\n",
    "\n",
    "        # Melt Data\n",
    "        chosen_cm = chosen_cm.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Value')\n",
    "        chosen_cm_agg = chosen_cm_agg.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Weight')\n",
    "\n",
    "        # Concatenate\n",
    "        chosen_cm = pd.concat([chosen_cm, chosen_cm_agg])\n",
    "\n",
    "        # Transform Candidate Model Names\n",
    "        chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.split('_').str[0]\n",
    "        #chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.startswith(\"SSF\").replace({True: \"SSF\", False: \"CR\"})\n",
    "\n",
    "        # Plot Theme\n",
    "        sns.set_theme(style=\"ticks\")\n",
    "\n",
    "        # Draw each cell as a scatter point with varying size and color\n",
    "        g = sns.relplot(\n",
    "            data = chosen_cm,\n",
    "            y = 'Value', x = 'INDEX', hue = \"Candidate_Model\", \n",
    "            size = \"Weight\", sizes = (3, 75),\n",
    "            height = 6.5, alpha = 0.75, palette=\"muted\",\n",
    "            #legend = True\n",
    "            )\n",
    "\n",
    "        # Legend\n",
    "        g._legend.remove()\n",
    "        h, l = g.ax.get_legend_handles_labels()\n",
    "        g.ax.legend(h[0:13], l[0:13], bbox_to_anchor=(1.0, 0.75), loc=2, fontsize=10, frameon=False)\n",
    "\n",
    "        # Title & Axis\n",
    "        g.set(xlabel='Combination Size',\n",
    "              ylabel='Candidate Models',\n",
    "              title=f\"Selected Candidate Models: \\n Correlation {corr_range[p]} & Betas {b_range[b]}\")\n",
    "\n",
    "        # Margins\n",
    "        g.ax.margins(x = 0.05, y = 0)\n",
    "\n",
    "        # Set number of ticks for y-axis\n",
    "        idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "        g.ax.set_yticks(idx)\n",
    "\n",
    "        # Set ticks labels for x-axis\n",
    "        g.ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "        # Tick-Label Size\n",
    "        g.set_yticklabels(size = 8)\n",
    "        g.set_xticklabels(size = 10)\n",
    "\n",
    "        # Add Horizontal Lines\n",
    "        #for i in idx:\n",
    "        #    g.ax.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "        # Rotate x-axis labels\n",
    "        for label in g.ax.get_xticklabels():\n",
    "            label.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Plot\n",
    "fig, ax = plt.subplots(1,1) \n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "# Plot \n",
    "colors = {'SSF':'blue', 'CR':'green'}\n",
    "ax.scatter(x = chosen_cm['INDEX'], y = chosen_cm['Weight'], c=chosen_cm['Candidate_Model'].map(colors), s = 1)\n",
    "#ax.plot(chosen_cm['INDEX'], chosen_cm.iloc[:, 1:], marker = \"o\", lw = 0, ms = 4)\n",
    "\n",
    "# Add title and axis names\n",
    "plt.title('Selected Candidate Models')\n",
    "plt.ylabel('Candidate Models')\n",
    "plt.xlabel('Combination Size')\n",
    "\n",
    "# Legend\n",
    "#plt.legend(loc = \"upper right\")\n",
    "\n",
    "# Margins\n",
    "plt.margins(x=0.10, y=0)\n",
    "\n",
    "# Set number of ticks for x-axis\n",
    "idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "ax.set_yticks(idx)\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "ax.set_yticklabels([[[*ssf_names, *cr_names]][0][i] for i in idx], rotation='horizontal', fontsize=6)\n",
    "\n",
    "# Add horizontal lines\n",
    "for i in idx:\n",
    "    plt.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot: Welches K hat was ausgewählt\n",
    "### Benchmark: Complete Subset Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is:\" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is:\" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = benchmark[np.arange(0, n_mc * len(corr_range) * len(b_range), len(b_range))]\n",
    "b2 = benchmark[np.arange(1, n_mc * len(corr_range) * len(b_range), len(b_range))]\n",
    "\n",
    "b1_p1 = b1[np.arange(0, n_mc * len(corr_range), len(corr_range))]\n",
    "b1_p2 = b1[np.arange(1, n_mc * len(corr_range), len(corr_range))]\n",
    "b1_p3 = b1[np.arange(2, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "b2_p1 = b2[np.arange(0, n_mc * len(corr_range), len(corr_range))]\n",
    "b2_p2 = b2[np.arange(1, n_mc * len(corr_range), len(corr_range))]\n",
    "b2_p3 = b2[np.arange(2, n_mc * len(corr_range), len(corr_range))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Goyal Welch Data\n",
    "data  =  pd.read_csv(path + r'/Data/PredictorData2022.xlsx - Quarterly.csv', thousands=',')\n",
    "\n",
    "# Equity Premium\n",
    "data['equity_premium'] = data['CRSP_SPvw'] - data['Rfree']\n",
    "\n",
    "# Dividend Price Ratio \n",
    "data['dp'] = np.log(data['D12']) - np.log(data['Index'])\n",
    "\n",
    "# Dividend Yield \n",
    "data['dy'] = np.log(data['D12'])- np.log(data['Index'].shift(1))\n",
    "\n",
    "# Earnings Price Ratio \n",
    "data['ep'] = np.log(data['E12']) - np.log(data['Index'])\n",
    "\n",
    "# Dividend Payout Ratio \n",
    "data['dpayr'] = np.log(data['D12']) - np.log(data['E12'])\n",
    "\n",
    "# Book to Market Ratio\n",
    "data['bmr'] = data['b/m']\n",
    "\n",
    "# # Net Equity Expansion\n",
    "data['ntis'] = data['ntis']\n",
    "\n",
    "# Treasury Bill Rate\n",
    "data['tbl'] = data['tbl']\n",
    "\n",
    "# Long Term Rate\n",
    "data['ltr'] = data['ltr']\n",
    "\n",
    "# Term Spread \n",
    "data['tsp'] = data['lty'] - data['tbl']\n",
    "\n",
    "# Default Return Spread \n",
    "data['dfr'] = data['corpr'] - data['ltr']\n",
    "\n",
    "# Inflation\n",
    "data['infl'] = data['infl']\n",
    "\n",
    "# Investment of Capital Ratio\n",
    "data['ik']  = data['ik']\n",
    "\n",
    "# Default Yield Spread\n",
    "data['dfy'] = data['BAA'] - data['AAA']\n",
    "\n",
    "# Realized Volatility\n",
    "data['rvol'] = data['svar']\n",
    "\n",
    "# reorganize the dataframe\n",
    "data = data[['yyyyq', \"equity_premium\", \"dp\", \"dy\", \"ep\", \"dpayr\", \"bmr\", \"ntis\", \"tbl\", \"ltr\", \"tsp\", \"dfr\", \"infl\", \"ik\"]]\n",
    "\n",
    "# Convert Date\n",
    "data['yyyyq'] = data['yyyyq'].astype(str)\n",
    "data['yyyyq'] = data.apply(lambda x: x['yyyyq'][:4]+'-Q'+x['yyyyq'][4:], axis=1)\n",
    "data['yyyyq'] = pd.to_datetime(data['yyyyq'])\n",
    "\n",
    "# Resetting the index\n",
    "data.set_index('yyyyq', inplace=True)\n",
    "data.index = data.index.to_period('Q')\n",
    "\n",
    "# Lag all Predictors\n",
    "data.iloc[:,1:]  =  data.iloc[:,1:].shift(1)\n",
    "\n",
    "# Drop Na\n",
    "data = data.loc[\"1946Q1\":, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Set Seed ######\n",
    "#random.seed(123)\n",
    "\n",
    "###### Data ######\n",
    "# Set Target Variable\n",
    "y  =  data.loc[:, [\"equity_premium\"]]\n",
    "X  =  data.drop(\"equity_premium\", axis = 1)\n",
    "\n",
    "# Get Predictor Names\n",
    "pred_names = list(X.columns)\n",
    "\n",
    "# Number of AR-Terms to include\n",
    "mlags =  2\n",
    "\n",
    "# Create Lags\n",
    "X  =  create_lags(y, X, mlags)\n",
    "\n",
    "# Drop Missing Values\n",
    "y  =  y.loc[\"1947Q2\":, ] #y[mlags:]\n",
    "X  =  X.loc[\"1947Q2\":, ] #X[mlags:]\n",
    "\n",
    "# Check NA\n",
    "any(X.isna().any())\n",
    "\n",
    "###### Parameter Subset Forecasts\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  # 20000\n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "n_sub  =  int(sum([min(n_models((X.shape[1]-mlags), k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2, 3] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 100) # 10000\n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) # 300000\n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  0.5\n",
    "n_times     =  50\n",
    "bssf_range  =  [1, 2, 3] # range(1, 5)\n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "### General Parameter ######\n",
    "# Initial Training-Period\n",
    "init       =  4 * 50 #4 * 10\n",
    "\n",
    "# Total Length\n",
    "total =  len(y) \n",
    "\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts  =  np.full((total, (n_sub + n_cr)), np.nan)     #np.full((total, (n_sub + n_cr + n_dt)), np.nan)\n",
    "cf_weights      =  np.full((total, (n_sub + n_cr)), np.nan)\n",
    "benchmark       =  np.full(total, np.nan)\n",
    "bssf_forecast   =  np.full(total, np.nan)\n",
    "bssf_opt        =  np.full(total, np.nan)\n",
    "sse_bssf        =  np.zeros(n_bssf)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Time\n",
    "for t in tqdm(range(init, total)):\n",
    "        \n",
    "    # Pre-Process Data\n",
    "    y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "    \n",
    "    ### Benchmark: AR(X) Model\n",
    "    pred          =  ar_mod(y_train, lags = mlags)\n",
    "    benchmark[t]  =  pred.iloc[0]\n",
    "    \n",
    "    ### Subset Forecasts\n",
    "    # Set up List to store Subset-Forecasts\n",
    "    preds_ssf =  np.full(n_sub, np.nan)\n",
    "    idx_sub   =  0\n",
    "    \n",
    "    # Loop over Subset Size \n",
    "    for k in k_range:\n",
    "    \n",
    "        # Get all possible Subset of length k\n",
    "        col_idx   =  list(range(mlags+1, X_train.shape[1]))\n",
    "        subs_idx  =  complete_sub(col_idx, k)\n",
    "\n",
    "        # Randomly select n_upper Subsets\n",
    "        feature_set  =  subs_idx #random_select(subs_idx, n_max, random_state = 123)\n",
    "\n",
    "        # Loop over Subsets\n",
    "        for feature in feature_set:\n",
    "\n",
    "            # Compute Subset-Regression-Forecast\n",
    "            pred  =  ssf(y_train, X_train, X_pred, feature, mlags)\n",
    "            preds_ssf[idx_sub] = pred\n",
    "            idx_sub += 1\n",
    "            \n",
    "    ### Compressed Regressions\n",
    "    # Set up List to store Compressed-Regression-Forecasts\n",
    "    preds_cr = np.full(n_cr, np.nan)\n",
    "    idx_cr   = 0\n",
    "    \n",
    "    # Loop over number of Components\n",
    "    for n_comp in cr_range:\n",
    "\n",
    "        # Loop over n repetitions\n",
    "        for r in rep_range:\n",
    "        \n",
    "            # Compute Compressed-Regression-Forecasts\n",
    "            pred  =  cr_reg(y_train, X_train, X_pred, n_comp, mlags, r)\n",
    "            preds_cr[idx_cr] = pred\n",
    "            idx_cr += 1\n",
    "            \n",
    "    # ### Decision Tree Regressions\n",
    "    # # Set up Matrix to store Decision-Tree-Forecasts\n",
    "    # preds_dt   = np.full(n_dt, np.nan)\n",
    "    # \n",
    "    # # Loop over number of Components\n",
    "    # for idx_dt, r in enumerate(dt_range):\n",
    "    #     \n",
    "    #     # Compute Decision-Tree-Forecasts\n",
    "    #     pred  =  dt_reg(y_train, X_train, X_pred, r)\n",
    "    #     preds_dt[idx_dt] = pred[0]\n",
    "\n",
    "    # Append Results\n",
    "    cand_forecasts[t][:n_sub]             =  preds_ssf \n",
    "    cand_forecasts[t][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "    #cand_forecasts[t][(n_sub+n_cr):]      =  preds_dt\n",
    "\n",
    "    ### Best Selection of Forecast\n",
    "    if t > init:\n",
    "    \n",
    "        # Set up Matrix to store Forecasts\n",
    "        bssf_forecasts  =  np.full(n_bssf, np.nan)\n",
    "        bssf_weights    =  np.zeros([n_bssf, n_sub + n_cr]) \n",
    "           \n",
    "        # Get \"best\" Subset-Size until now (lowest Sum of Squared Errors)\n",
    "        s_opt  =  np.argmin(sse_bssf)\n",
    "    \n",
    "        # Loop over Subset Sizes\n",
    "        for idx_bssf, s in enumerate(bssf_range):\n",
    "    \n",
    "            # Compute Best-Subset-Selection-of-Forecasts\n",
    "            pred  =  bssf(y_train[init:], cand_forecasts[init:t], cand_forecasts[t], alpha, s, n_times)\n",
    "            bssf_forecasts[idx_bssf]  =  pred[0]\n",
    "            bssf_weights[idx_bssf]    =  pred[1]\n",
    "    \n",
    "            # Compute Sum of Squared Errors\n",
    "            sse_bssf[idx_bssf] =  sse_bssf[idx_bssf] + (y_pred.iloc[0,0] - pred[0]) ** 2\n",
    "    \n",
    "        # Select Forecast \n",
    "        bssf_forecast[t] =  bssf_forecasts[s_opt]\n",
    "        cf_weights[t]    =  bssf_weights[s_opt]\n",
    "        bssf_opt[t]      =  bssf_range[s_opt]\n",
    "        \n",
    "# Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(len(pred_names)), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "#dt_names = [f\"DT_{idx_dt}\" for idx_dt in dt_range]\n",
    "        \n",
    "# Convert Results to DataFrame\n",
    "cand_forecasts  =  pd.DataFrame(cand_forecasts, index = y.index, columns = [*ssf_names, *cr_names]) #, *dt_names])\n",
    "benchmark       =  pd.DataFrame(benchmark,      index = y.index, columns = [\"AR\"])\n",
    "bssf_forecast   =  pd.DataFrame(bssf_forecast,  index = y.index, columns = [\"BSSF\"])\n",
    "cf_weights      =  pd.DataFrame(cf_weights,     index = y.index, columns = [*ssf_names, *cr_names]) #, *dt_names])\n",
    "bssf_opt        =  pd.DataFrame(bssf_opt,     index = y.index, columns = [\"Subset_Size\"])\n",
    "\n",
    "# Cut off initial Training-Period\n",
    "sub_y              =  y.iloc[init:].copy()\n",
    "sub_cand_forecasts =  cand_forecasts.iloc[init:].copy()\n",
    "sub_benchmark      =  benchmark.iloc[init:].copy()\n",
    "sub_bssf_forecast  =  bssf_forecast.iloc[init:].copy()\n",
    "sub_cf_weights     =  cf_weights.iloc[init:].copy()\n",
    "sub_bssf_opt       =  bssf_opt.iloc[init:].copy()\n",
    "\n",
    "# OOS-Period\n",
    "oos_start  =  \"1999Q4\"\n",
    "oos_end    =  \"2022Q4\" \n",
    "oos_y             =  sub_y.loc[oos_start:oos_end].copy()\n",
    "oos_cand_forecast =  sub_cand_forecasts.loc[oos_start:oos_end].copy()\n",
    "oos_benchmark     =  sub_benchmark.loc[oos_start:oos_end].copy()\n",
    "oos_bssf_forecast =  sub_bssf_forecast.loc[oos_start:oos_end].copy()\n",
    "oos_cf_weights    =  sub_cf_weights.loc[oos_start:oos_end].copy()\n",
    "oos_bssf_opt      =  sub_bssf_opt.loc[oos_start:oos_end].copy()\n",
    "\n",
    "# Evaluation\n",
    "np.sum((oos_y.iloc[:,0] - oos_bssf_forecast.iloc[:,0]) ** 2) / np.sum((oos_y.iloc[:,0] - oos_benchmark.iloc[:,0]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Zero with NaN\n",
    "oos_cf_weights.replace({0:np.nan}, inplace=True)\n",
    "\n",
    "# Adapt Column-Values\n",
    "vec = list(range(1, oos_cf_weights.shape[1]+1))\n",
    "tmp = oos_cf_weights * vec\n",
    "\n",
    "# Dates\n",
    "tmp = tmp.reset_index(names=\"date\")\n",
    "\n",
    "# Plot \n",
    "# tmp.plot(x='date', y = tmp.columns[1:],\n",
    "#          figsize=(10, 5), legend=False,\n",
    "#          marker=\"o\", ms = 1, \n",
    "#          title=\"Selected Candidate Models\", ylabel=\"Selected Candidate Models\")\n",
    "#          #yticks = (np.arange(98), list(tmp.columns[1:])))\n",
    "\n",
    "tmp_long = pd.melt(tmp, id_vars = \"date\")\n",
    "tmp_long['variable'] = tmp_long['variable'].str.startswith(\"SSF\").replace({True: \"SSF\", False: \"CR\"})\n",
    "tmp_long.set_index(\"date\", inplace = True)\n",
    "tmp_long.groupby(\"variable\")[\"value\"].plot(legend=True, figsize = (10, 5),\n",
    "                                           marker=\"o\", ms = 2, lw = 0,\n",
    "                                           ylim = [-1, cand_forecasts.shape[1]+1],\n",
    "                                           title=\"Selected Candidate Models\", ylabel=\"Selected Candidate Models\")\n",
    "plt.show()\n",
    "\n",
    "# Subset Size\n",
    "oos_bssf_opt.plot(figsize=(10, 5), legend=False, \n",
    "                  color = \"black\", marker=\"o\", ms = 1, lw = 0,\n",
    "                  title = \"Subset Size\", xlabel=\"date\", ylabel=\"Selected Subset Size\",\n",
    "                  ylim  =  [min(bssf_range)-0.5, max(bssf_range)+0.5],\n",
    "                  yticks = np.arange(min(bssf_range), max(bssf_range)+1, step=1.0))\n",
    "plt.show()\n",
    "\n",
    "# CSSED\n",
    "cssed = np.cumsum(((oos_y.iloc[:,0] - oos_benchmark.iloc[:,0]) ** 2) - ((oos_y.iloc[:,0] - oos_bssf_forecast.iloc[:,0]) ** 2))\n",
    "cssed.plot(figsize=(10, 5),\n",
    "            xlabel = \"date\", ylabel = \"CSSED\", title = \"Cumulated Sum of Squared Error Differences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MC-Parameter ######\n",
    "# Number of MC-Runs\n",
    "n_mc  =  2\n",
    "\n",
    "# Set Parameter\n",
    "n_obs       =  100\n",
    "n_preds     =  8\n",
    "init        =  50\n",
    "mlags       =  0\n",
    "corr_range  =  [0.5, 0.95]\n",
    "b_range     =  np.array([[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "###### Parameter Subset Forecasts ######\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3, 4, 5, 6, 7, 8] \n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  \n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "n_sub  =  int(sum([min(n_models(n_preds, k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2] \n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 50) \n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) \n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  10.0\n",
    "n_times     =  1\n",
    "bssf_range  =  [1, 2, 3] \n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "######## Objects ########\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts   = np.full((len(range(init, n_obs)), (n_sub + n_cr)), np.nan)     \n",
    "benchmark        = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "cf_weights       = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf, (n_sub + n_cr)), np.nan)\n",
    "bssf_forecast    = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "csr_forecast     = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "se_benchmark     = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "se_bssf_forecast = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "se_csr_forecast  = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Monte-Carlo Runs\n",
    "i = 0\n",
    "for r in tqdm(range(n_mc)):\n",
    "    \n",
    "    # Loop over Covariance-Sets\n",
    "    for p in corr_range:\n",
    "        \n",
    "        # Loop over Coefficient-Sets\n",
    "        for b in b_range:\n",
    "    \n",
    "            ### Simulate Data ###\n",
    "            y, X, pred_names = sim(n_obs, n_preds, b, p, r)\n",
    "            \n",
    "            ### Benchmark: PHM ###\n",
    "            benchmark[i]    = y.iloc[:-1].mean().iloc[0]\n",
    "            se_benchmark[i] = (y.iloc[-1,0] - benchmark[i]) ** 2\n",
    "\n",
    "            # Loop over t / Create Candidate Models\n",
    "            for t in range(init, n_obs):\n",
    "            \n",
    "                ### Pre-Process Data ###\n",
    "                y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "\n",
    "                ### Subset Forecasts ###\n",
    "                feature_set  =  list(chain(*list(map(lambda k: complete_sub(list(range(1, X_train.shape[1])), k), k_range))))\n",
    "                preds_ssf    =  np.array(list(map(lambda feature: ssf(y_train, X_train, X_pred, feature, 0), feature_set)))\n",
    "            \n",
    "                ## Set up List to store Subset-Forecasts\n",
    "                #preds_ssf = np.full(n_sub, np.nan)\n",
    "                #idx_sub   = 0\n",
    "                #\n",
    "                ## Loop over Subset Size \n",
    "                #for k in k_range:\n",
    "                #\n",
    "                #    # Get all possible Subsets of length k\n",
    "                #    col_idx  = list(range(1, X_train.shape[1]))\n",
    "                #    subs_idx = complete_sub(col_idx, k)\n",
    "                #\n",
    "                #    # Randomly select n_upper Subsets\n",
    "                #    feature_set  =  subs_idx #random_select(subs_idx, n_max, random_state = 123)\n",
    "                #\n",
    "                #    # Loop over Subsets\n",
    "                #    for feature in feature_set:\n",
    "                #    \n",
    "                #        # Compute Subset-Regression-Forecast\n",
    "                #        pred  =  ssf(y_train, X_train, X_pred, feature, 0)\n",
    "                #        preds_ssf[idx_sub] = pred\n",
    "                #        idx_sub += 1\n",
    "\n",
    "                ### Compressed Regressions ###\n",
    "                preds_cr = np.array(list(chain(*[list(map(lambda rep: cr_reg(y_train, X_train, X_pred, n_comp, 0, rep), rep_range)) for n_comp in cr_range])))\n",
    "                \n",
    "                # # Set up List to store Compressed-Regression-Forecasts\n",
    "                # preds_cr   = np.full(n_cr, np.nan)\n",
    "                # idx_cr     = 0\n",
    "                # \n",
    "                # # Loop over number of Components\n",
    "                # for n_comp in cr_range:\n",
    "                # \n",
    "                #     # Loop over n repetitions\n",
    "                #     for rep in rep_range:\n",
    "                #     \n",
    "                #         # Compute Compressed-Regression-Forecasts\n",
    "                #         pred  =  cr_reg(y_train, X_train, X_pred, n_comp, 0, rep)\n",
    "                #         preds_cr[idx_cr] = pred\n",
    "                #         idx_cr += 1\n",
    "\n",
    "                # ### Decision Tree Regressions\n",
    "                # preds_dt = np.array(list(map(lambda r: dt_reg(y_train, X_train, X_pred, r), dt_range)))\n",
    "                \n",
    "                # # Set up Matrix to store Decision-Tree-Forecasts\n",
    "                # preds_dt   = np.full(n_dt, np.nan)\n",
    "                # \n",
    "                # # Loop over number of Components\n",
    "                # for idx_dt, r in enumerate(dt_range):\n",
    "                #     \n",
    "                #     # Compute Decision-Tree-Forecasts\n",
    "                #     pred  =  dt_reg(y_train, X_train, X_pred, r)\n",
    "                #     preds_dt[idx_dt] = pred[0]\n",
    "\n",
    "                # Append Results\n",
    "                cand_forecasts[t-init][:n_sub]             =  preds_ssf \n",
    "                cand_forecasts[t-init][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "                #cand_forecasts[t-init][(n_sub+n_cr):]     =  preds_dt\n",
    "                \n",
    "            ### Benchmark: Complete Subset Regression ###\n",
    "            tmp_ = np.cumsum([0] + [len(list(combinations(range(n_preds), k))) for k in k_range])\n",
    "            csr_forecast[i]     =  [np.mean(preds_ssf[tmp_[i]:tmp_[i+1]]) for i in range(len(tmp_)-1)]\n",
    "            se_csr_forecast[i]  =  (y_pred.iloc[0,0] - csr_forecast[i]) ** 2\n",
    "\n",
    "            ### Best Selection of Forecast ###\n",
    "            bssf_forecast[i], cf_weights[i] = zip(*list(map(lambda s: bssf(y_train[init:], cand_forecasts[:-1], cand_forecasts[-1], alpha, s, n_times), bssf_range)))\n",
    "            se_bssf_forecast[i] = (y_pred.values[0] - bssf_forecast[i]) ** 2\n",
    "            \n",
    "            # # Set up Matrix to store Forecasts\n",
    "            # kssf_forecasts  =  np.full(n_bssf, np.nan)\n",
    "            # kssf_weights    =  np.zeros([n_bssf, n_sub + n_cr]) \n",
    "\n",
    "            # # Loop over Subset Sizes\n",
    "            # for idx_bssf, s in enumerate(bssf_range):\n",
    "            #     \n",
    "            #     # Compute Best-Subset-Selection-of-Forecasts\n",
    "            #     pred = bssf(y_train[init:], cand_forecasts[:-1], cand_forecasts[-1], alpha, s, n_times)\n",
    "            #     bssf_forecast[i][idx_bssf] = pred[0]\n",
    "            #     cf_weights[i][idx_bssf]    = pred[1]\n",
    "            #     se_bssf_forecast[i][idx_bssf] = (y_pred.iloc[0,0] - pred[0]) ** 2\n",
    "             \n",
    "            # Update index   \n",
    "            i += 1\n",
    "            \n",
    "# # Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(n_preds), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "\n",
    "### Evaluation ###\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Fred-MD-Data stationary\n",
    "def transform_tcode(data):\n",
    "    \n",
    "    # Get Transformation-Code\n",
    "    tcode = data[0]\n",
    "    \n",
    "    # Get Data\n",
    "    data = data[1:]\n",
    "\n",
    "    if tcode == 1:\n",
    "        output = data\n",
    "    elif tcode == 2:\n",
    "        output = data - np.roll(data, 1)\n",
    "    elif tcode == 3:\n",
    "        output = (data - np.roll(data, 1)) - (np.roll(data, 1) - np.roll(data, 2))\n",
    "    elif tcode == 4:\n",
    "        output = np.log(data)\n",
    "    elif tcode == 5:\n",
    "        output = np.log(data) - np.roll(np.log(data), 1)\n",
    "    elif tcode == 6:\n",
    "        output = (np.log(data) - np.roll(np.log(data), 1)) - (np.roll(np.log(data), 1) - np.roll(np.log(data), 2))\n",
    "    else:\n",
    "        output = (data / np.roll(data, 1) - 1) - (np.roll(data, 1) / np.roll(data, 2) - 1)\n",
    "\n",
    "    return np.concatenate(([tcode], output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x_dataset  =  pd.read_csv(path + r'/fred_md_202306.csv')\n",
    "\n",
    "# Drop Variables with too many missing values\n",
    "x_dataset  =  x_dataset.drop([\"PERMIT\", \"PERMITNE\", \"PERMITMW\", \"PERMITS\",\n",
    "                              \"PERMITW\", \"ACOGNO\", \"ANDENOx\", \"CP3Mx\",\n",
    "                              \"COMPAPFFx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VIXCLSx\"],\n",
    "                              axis=1)\n",
    "\n",
    "# Transform remaining Columns\n",
    "x_dataset.iloc[:, 1:]  =  x_dataset.iloc[:,1:].apply(lambda x: transform_tcode(x))\n",
    "\n",
    "# Drop First Row with Transformation-Code\n",
    "x_dataset  =  x_dataset.iloc[1:,:]\n",
    "\n",
    "# Lag Data\n",
    "x_dataset.iloc[:,1:]  =  x_dataset.iloc[:,1:].shift(1)\n",
    "\n",
    "# Convert Date\n",
    "x_dataset['sasdate']  =  pd.to_datetime(x_dataset['sasdate'])\n",
    "\n",
    "# Filter Data\n",
    "x_dataset  =  x_dataset[(x_dataset['sasdate'] >= '1959-04-01') & (x_dataset['sasdate'] <= '2023-03-01')]\n",
    "\n",
    "# Resetting the index\n",
    "x_dataset.set_index('sasdate', inplace=True)\n",
    "x_dataset.index = x_dataset.index.to_period('M')\n",
    "\n",
    "# Load Data\n",
    "y_dataset  =  pd.read_csv(path + r'/fred_md_202306.csv')\n",
    "\n",
    "# Select and Rename Variables\n",
    "y_dataset  =  y_dataset.loc[:, [\"sasdate\", \"CPIAUCSL\", \"INDPRO\", \"UNRATE\"]]\n",
    "y_dataset  =  y_dataset.rename(columns={'CPIAUCSL': 'CPIAUCSL_h1', 'INDPRO': 'INDPRO_h1', 'UNRATE': 'UNRATE_h1'})\n",
    "\n",
    "# Transform Variables\n",
    "y_dataset[[\"CPIAUCSL_h1\"]]  =  y_dataset[[\"CPIAUCSL_h1\"]].apply(lambda x: 4 * 100 * (np.log(x) - np.roll(np.log(x), 1)))\n",
    "y_dataset[[\"INDPRO_h1\"]]    =  y_dataset[[\"INDPRO_h1\"]].apply(lambda x: 4 * 100 * (np.log(x) - np.roll(np.log(x), 1)))\n",
    "y_dataset[[\"UNRATE_h1\"]]    =  y_dataset[[\"UNRATE_h1\"]]\n",
    "\n",
    "# Drop First Row with Transformation-Code\n",
    "y_dataset  =  y_dataset.iloc[1:,:]\n",
    "\n",
    "# Convert Date\n",
    "y_dataset['sasdate']  =  pd.to_datetime(y_dataset['sasdate'])\n",
    "\n",
    "# Filter Data\n",
    "y_dataset  =  y_dataset[(y_dataset['sasdate'] >= '1959-04-01') & (y_dataset['sasdate'] <= '2023-03-01')]\n",
    "\n",
    "# Resetting the index\n",
    "y_dataset.set_index('sasdate', inplace=True)\n",
    "y_dataset.index = y_dataset.index.to_period('M')\n",
    "\n",
    "# Set Target Variable\n",
    "y  =  y_dataset.loc[:, [\"CPIAUCSL_h1\"]]\n",
    "X  =  x_dataset.drop(\"CPIAUCSL\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Selection of Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, X_train, X_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    X_train  =  X_train / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(X_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ X_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat[0])\n",
    "    Q         =  - 2 * diag_mat + X_train.transpose() @ X_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    # Initialize BQM\n",
    "    bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    bqm  =  bqm.from_qubo(Q)\n",
    "    \n",
    "    # Solve\n",
    "    solver     =  SimulatedAnnealingSampler()\n",
    "    #solver     =  SteepestDescentSolver()\n",
    "    #solver     =  TabuSampler()\n",
    "    #solver     =  TreeDecompositionSolver()\n",
    "\n",
    "    sampleset  =  solver.sample(bqm, num_reads = n_times)\n",
    "    solution   =  list(sampleset.first[0].values())\n",
    "    \n",
    "    # Prediction \n",
    "    pred       = solution @ X_pred\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series-Cross-Validation to Tune Parameter k\n",
    "def cv_ts(Y, X, splits, k_seq, alpha, n_times):\n",
    "\n",
    "    # Set up TS-Split\n",
    "    tscv    =  TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Set up Result-Matrix\n",
    "    cv_res  =  np.zeros((splits, len(k_seq)))\n",
    "\n",
    "    # Loop over Train-Test-Splits\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        # Split Data\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index],   Y.iloc[test_index]\n",
    "\n",
    "        # Cross-Validation k\n",
    "        for k in k_seq:\n",
    "\n",
    "            # Selected Predictors\n",
    "            solution    =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[1]\n",
    "\n",
    "            # Prediction\n",
    "            prediction  =  solution @ X_test.transpose()\n",
    "\n",
    "            # MSE\n",
    "            cv_res[i, k-1]  =  np.mean((Y_test.squeeze() - prediction) ** 2)\n",
    "\n",
    "            # Select k with smalltest average MSE\n",
    "            k  =  cv_res.mean(axis=0).argmin() + 1\n",
    "            \n",
    "    # Return \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate Forecasts\n",
    "cand_forecasts  =  results\n",
    "\n",
    "# Target Variable\n",
    "target_var  =  y_dataset.loc[:, [\"CPIAUCSL_h1\"]]\n",
    "\n",
    "# Get Dates\n",
    "dates  =  cand_forecasts.index.values\n",
    "\n",
    "# Match Time Frames\n",
    "target_var  =  target_var.loc[dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "alpha    =  1\n",
    "n_mods   =  cand_forecasts.shape[1]\n",
    "\n",
    "# Set Vector & Matrices\n",
    "X     =  cand_forecasts.copy()\n",
    "Y     =  target_var.copy()\n",
    "\n",
    "# Set Time Parameter\n",
    "init    =  int(12 * 5)\n",
    "final   =  len(Y)\n",
    "\n",
    "# Set up Empty Array\n",
    "predictions  =  np.zeros(final)\n",
    "predictions.fill(np.nan)\n",
    "\n",
    "# Loop\n",
    "for t in tqdm(range(init, final)):\n",
    "    \n",
    "    # Train Data\n",
    "    X_train  =  X.iloc[:t, ]\n",
    "    Y_train  =  Y.iloc[:t, ]\n",
    "    \n",
    "    # Prediction Data\n",
    "    X_pred   =  X.iloc[t, ]\n",
    "    \n",
    "    # Cross-Validation\n",
    "    splits   =  5\n",
    "    n_times  =  500\n",
    "    k_seq    =  [1, 2, 3, 4, 5]\n",
    "    k_opt    =  cv_ts(Y_train, X_train, splits, k_seq, alpha, n_times)\n",
    "    \n",
    "    # Prediction \n",
    "    n_times  =  500\n",
    "    predictions[t]  =  bssf(Y_train, X_train, X_pred, alpha, k_opt, n_times)[0]\n",
    "\n",
    "# Convert to Pandas Series\n",
    "predictions  =  pd.Series(predictions, name = \"qubo\", index = Y.index).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Target Variable\n",
    "target_var      =  pyreadr.read_r(path + '/Data/Results/Target_Var/target_var.RDS')[None]\n",
    "\n",
    "# Load all Candidate Forecasts\n",
    "cand_forecasts  =  pd.DataFrame()\n",
    "files           =  os.scandir(path + '/Data/Results/Candidate_Forecasts')\n",
    "\n",
    "# Loop\n",
    "for file in files:\n",
    "    if (file.path.endswith(\".RDS\")):\n",
    "        aux  =  pyreadr.read_r(file)[None]\n",
    "        cand_forecasts  =  pd.concat([cand_forecasts, aux], axis = 1)\n",
    "        \n",
    "# Drop Na\n",
    "cand_forecasts  =  cand_forecasts.dropna()\n",
    "\n",
    "# Get Dates\n",
    "dates           =  cand_forecasts.index.values\n",
    "\n",
    "# Match Time Frames\n",
    "target_var      =  target_var.loc[dates]\n",
    "\n",
    "# Dimensions\n",
    "print(cand_forecasts.shape)\n",
    "print(target_var.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, X_train, X_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    X_train  =  X_train / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(X_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ X_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat[0])\n",
    "    Q         =  - 2 * diag_mat + X_train.transpose() @ X_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    # Initialize BQM\n",
    "    bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    bqm  =  bqm.from_qubo(Q)\n",
    "    \n",
    "    # Solve\n",
    "    solver     =  SimulatedAnnealingSampler()\n",
    "    #solver     =  SteepestDescentSolver()\n",
    "    #solver     =  TabuSampler()\n",
    "    #solver     =  TreeDecompositionSolver()\n",
    "\n",
    "    sampleset  =  solver.sample(bqm, num_reads = n_times)\n",
    "    solution   =  list(sampleset.first[0].values())\n",
    "    \n",
    "    # Prediction \n",
    "    pred       = solution @ X_pred\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series-Cross-Validation to Tune Parameter k\n",
    "def cv_ts(Y, X, splits, k_seq, alpha, n_times):\n",
    "\n",
    "    # Set up TS-Split\n",
    "    tscv    =  TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Set up Result-Matrix\n",
    "    cv_res  =  np.zeros((splits, len(k_seq)))\n",
    "\n",
    "    # Loop over Train-Test-Splits\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        # Split Data\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index],   Y.iloc[test_index]\n",
    "\n",
    "        # Cross-Validation k\n",
    "        for k in k_seq:\n",
    "\n",
    "            # Selected Predictors\n",
    "            solution    =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[1]\n",
    "\n",
    "            # Prediction\n",
    "            prediction  =  solution @ X_test.transpose()\n",
    "\n",
    "            # MSE\n",
    "            cv_res[i, k-1]  =  np.mean((Y_test.squeeze() - prediction) ** 2)\n",
    "\n",
    "            # Select k with smalltest average MSE\n",
    "            k  =  cv_res.mean(axis=0).argmin() + 1\n",
    "            \n",
    "    # Return \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "alpha    =  1\n",
    "n_mods   =  cand_forecasts.shape[1]\n",
    "\n",
    "# Set Vector & Matrices\n",
    "X     =  cand_forecasts.copy()\n",
    "Y     =  target_var.copy()\n",
    "\n",
    "# Set Time Parameter\n",
    "init    =  int(12 * 5)\n",
    "final   =  len(Y)\n",
    "\n",
    "# Set up Empty Array\n",
    "predictions  =  np.zeros(final)\n",
    "predictions.fill(np.nan)\n",
    "\n",
    "# Loop\n",
    "for t in range(init, final):\n",
    "    \n",
    "    # Train Data\n",
    "    X_train  =  X.iloc[:t, ]\n",
    "    Y_train  =  Y.iloc[:t, ]\n",
    "    \n",
    "    # Prediction Data\n",
    "    X_pred   =  X.iloc[t, ]\n",
    "    \n",
    "    # Cross-Validation\n",
    "    splits   =  5\n",
    "    n_times  =  500\n",
    "    k_seq    =  [1, 2, 3, 4, 5]\n",
    "    k        =  cv_ts(Y_train, X_train, splits, k_seq, alpha, n_times)\n",
    "    \n",
    "    # Prediction \n",
    "    n_times  =  500\n",
    "    predictions[t]  =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[0]\n",
    "\n",
    "# Convert to Pandas Series\n",
    "predictions  =  pd.Series(predictions, name = \"qubo\", index = Y.index).to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Eval-Function (OOS-R2)\n",
    "def oos_r2(observation, prediction, benchmark):\n",
    "    \n",
    "    # Squared Error Model\n",
    "    se1  =  (observation - prediction) ** 2\n",
    "    \n",
    "    # Squared Error Benchmark\n",
    "    se2  =  (observation - benchmark) ** 2\n",
    "    \n",
    "    # Out-of-Sample R2\n",
    "    oos_r2  =  (1 - sum(se1) / sum(se2)) * 100\n",
    "    \n",
    "    # Return \n",
    "    return(oos_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OOS-Period\n",
    "eval_start  =  \"1974-12-01\"\n",
    "eval_end    =  \"2020-12-01\"\n",
    "\n",
    "# Keep only OOS-Period\n",
    "oos_target_var      =  target_var.loc[eval_start:eval_end].squeeze()\n",
    "oos_benchmark       =  cand_forecasts[eval_start:eval_end][\"pred_hist_mean\"]\n",
    "oos_bssf            =  predictions.loc[eval_start:eval_end].squeeze()\n",
    "oos_cand_forecasts  =  cand_forecasts[eval_start:eval_end]\n",
    "\n",
    "# Evaluate Best Subset Selection of Forecasts\n",
    "oos_r2(oos_target_var, oos_bssf, oos_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Candidate Models\n",
    "eval_cand_mods  =  oos_cand_forecasts.apply(lambda x: oos_r2(oos_target_var, x, oos_benchmark), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cand_mods.sort_values(ascending = False).to_frame(\"OOS-R2\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OOS-Period\n",
    "eval_start  =  \"1974-12-01\"\n",
    "eval_end    =  \"2020-12-01\"\n",
    "\n",
    "# Keep only OOS-Period\n",
    "oos_target_var      =  target_var.loc[eval_start:eval_end].squeeze()\n",
    "oos_benchmark       =  cand_forecasts[eval_start:eval_end][\"pred_hist_mean\"]\n",
    "oos_cand_forecasts  =  cand_forecasts[eval_start:eval_end]\n",
    "\n",
    "# Evaluate Candidate Models\n",
    "eval_cand_mods  =  oos_cand_forecasts.apply(lambda x: oos_r2(oos_target_var, x, oos_benchmark), axis = 0)\n",
    "\n",
    "# Show Results\n",
    "print(eval_cand_mods.filter(like = \"XGB\"))\n",
    "print(eval_cand_mods.filter(like = \"GBM\"))\n",
    "print(eval_cand_mods.filter(like = \"pcr\"))\n",
    "print(eval_cand_mods.filter(like = \"glm\"), n =)\n",
    "#eval_cand_mods.sort_values(ascending = False).to_frame(\"OOS-R2\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BQM\n",
    "qubo  =  { (i,j) : Q.iloc[i,j] for i in range(0, 11) for j in range(0, 11) }\n",
    "bqm   =  BinaryQuadraticModel('BINARY')\n",
    "bqm   =  bqm.from_qubo(Q)\n",
    "\n",
    "# Initialize BQM\n",
    "bqm = BinaryQuadraticModel('BINARY')\n",
    "\n",
    "# Add Linear Coefficients\n",
    "for i in range(0,11):\n",
    "    lin_coef  =  Q.iloc[i,i]\n",
    "    bqm.add_linear(i, lin_coef)\n",
    "    \n",
    "# Add Quadratic Coefficients\n",
    "for i in range(0,11):\n",
    "    for j in range(0,11):\n",
    "        if i != j:\n",
    "            quad_coef  =  Q.iloc[i,j]\n",
    "            bqm.add_quadratic(i, j, quad_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
