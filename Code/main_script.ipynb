{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-10-22\n"
     ]
    }
   ],
   "source": [
    "# Load Packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import multiprocess as mp\n",
    "from multiprocess import Pool\n",
    "\n",
    "from functools import partial  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "\n",
    "from statsmodels.tsa.tsatools import lagmat\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import random_projection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from dimod import BinaryQuadraticModel\n",
    "from dwave.samplers import SimulatedAnnealingSampler\n",
    "from dwave.samplers import SteepestDescentSolver\n",
    "from dwave.preprocessing import roof_duality\n",
    "# from dwave.system import LeapHybridSampler\n",
    "# from dwave.system import DWaveSampler\n",
    "# from dwave.system import EmbeddingComposite\n",
    "# from dimod import ExactSolver\n",
    "# from dwave.samplers import TabuSampler\n",
    "# from dwave.samplers import TreeDecompositionSolver\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobi_optimods.qubo import solve_qubo\n",
    "gp.setParam('OutputFlag', 0)\n",
    "\n",
    "if os.name == 'nt':\n",
    "    import dill\n",
    "    dill.settings['recurse'] = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Path\n",
    "path  =  os.path.dirname(os.getcwd()) # os.path.dirname(os.getcwd()) #r'/Users/slehmann/Library/CloudStorage/Dropbox/QUBO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "def sim(n_obs, n_preds, b, p, r):\n",
    "    \n",
    "    \"\"\"\n",
    "    Simulate Data for Regression Problem\n",
    "    with pre-determined Covariance Matrix between Predictors\n",
    "    and pre-determined Coefficients\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set Seed\n",
    "    np.random.seed(r)\n",
    "    \n",
    "    # Set Covarirance Matrix between Predictors\n",
    "    cov_mat = np.full((n_preds, n_preds), p)\n",
    "    np.fill_diagonal(cov_mat, 1.0)\n",
    "\n",
    "    # Simulate Predictor Time Series\n",
    "    X = np.random.multivariate_normal([0.0]*n_preds, cov_mat, n_obs)\n",
    "    pred_names = \"X\"+pd.Series(range(1, n_preds+1)).astype(str) #list(X.columns)\n",
    "\n",
    "    # Set Noise\n",
    "    noise  =  1.0\n",
    "    eps    =  np.random.normal(0.0, noise, n_obs)\n",
    "\n",
    "    # Set Coefficients\n",
    "    b_scl = b * noise / np.sqrt(n_obs)   \n",
    "\n",
    "    # Set Target Variable\n",
    "    y = X @ b_scl + eps\n",
    "    \n",
    "    # Return\n",
    "    return(y, X, pred_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Lags\n",
    "def create_lags(y, X, mlags):\n",
    "    \n",
    "    \"\"\"\n",
    "    Add mlags lags of y to X\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add all lags from zero to maxlag\n",
    "    X = np.concatenate((lagmat(y, maxlag = mlags, use_pandas = True), X), axis = 1)\n",
    "    \n",
    "    # Return\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Pre-Process Data\n",
    "def prepro(y, X, t):\n",
    "    \n",
    "    \"\"\"\n",
    "    Split Data in Train and Predict Data and Standardize Data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train Data\n",
    "    y_train  =  y[:t]\n",
    "    X_train  =  X[:t]\n",
    "\n",
    "    # Predict Data\n",
    "    y_pred  =  y[t]\n",
    "    X_pred  =  X[[t]]\n",
    "    \n",
    "    # Standardize Data\n",
    "    scaler  =  StandardScaler()   \n",
    "    X_train =  scaler.fit_transform(X_train)\n",
    "    X_pred  =  scaler.transform(X_pred)\n",
    "    \n",
    "    ## Add Constant\n",
    "    X_train =  sm.add_constant(X_train)\n",
    "    X_pred  =  sm.add_constant(X_pred, has_constant = 'add')\n",
    "    \n",
    "    return y_train, X_train, y_pred, X_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Complete) Subset Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return array of all subsets of length k\n",
    "def complete_sub(arr, k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Elements are treated as unique based on their position, not on their value.\n",
    "    So if the input elements are unique, there will be no repeated values in each combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all subsets of size k\n",
    "    subset = list(combinations(arr, k)) \n",
    "    \n",
    "    # Return \n",
    "    return subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate number of models\n",
    "def n_models(K, k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate number of models\n",
    "    \"\"\"\n",
    "    \n",
    "    return math.factorial(K) / (math.factorial(k) * math.factorial(K-k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly select n_max items from array\n",
    "def random_select(arr, n_max, random_state):\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random state\n",
    "    random.seed(random_state)\n",
    "    \n",
    "    # Set upper Boundary\n",
    "    upper_bound  =  len(arr) if len(arr) < n_max else n_max\n",
    "    \n",
    "    # Randomly select items without repetition\n",
    "    rand_arr  =  random.sample(arr, k = upper_bound)\n",
    "    \n",
    "    # Return \n",
    "    return rand_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to produce Subset Regression Forecasts\n",
    "def ssf(y_train, X_train, X_pred, feature, mlags):\n",
    "    \n",
    "    # Subset Feature Space (incl. constant)\n",
    "    X_train_subset = X_train[:, list(range(0, mlags+1)) + list(feature)]\n",
    "    X_pred_subset  = X_pred[:, list(range(0, mlags+1)) + list(feature)]\n",
    "    \n",
    "    # Fit Model\n",
    "    model =  sm.OLS(y_train, X_train_subset) # LinearRegression() \n",
    "    regr  =  model.fit() # model.fit(X_train_subset, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    pred = regr.predict(X_pred_subset)\n",
    "    \n",
    "    return(pred[0], regr.params[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressed Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compressed Regression (Gaussian random projection)\n",
    "def cr_reg(y_train, X_train, X_pred, n_comp, mlags, ran_st):\n",
    "    \n",
    "    # Set up Random-Projection-Matrix\n",
    "    projector = random_projection.GaussianRandomProjection(n_components = n_comp, random_state = ran_st)\n",
    "    \n",
    "    # Transform\n",
    "    X_train_proj =  projector.fit_transform(X_train[:, (mlags+1):])\n",
    "    X_pred_proj  =  projector.fit_transform(X_pred[:,  (mlags+1):])\n",
    "\n",
    "    # Add Constant + Lags\n",
    "    rp_train =  np.concatenate([X_train[:, :(mlags+1)], X_train_proj], axis = 1)\n",
    "    rp_pred  =  np.concatenate([X_pred[:,  :(mlags+1)], X_pred_proj],  axis = 1)\n",
    "\n",
    "    # Fit Model\n",
    "    model  =  sm.OLS(y_train, rp_train) #LinearRegression()\n",
    "    regr   =  model.fit() # model.fit(rp_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    pred = regr.predict(rp_pred)\n",
    "    \n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression\n",
    "def dt_reg(y_train, X_train, X_pred, ran_st):\n",
    "    \n",
    "    # Set up Regressor Object \n",
    "    model = DecisionTreeRegressor(criterion = \"squared_error\",\n",
    "                                  max_depth = 20,\n",
    "                                  splitter  = \"random\",\n",
    "                                  random_state = ran_st)\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(X_pred)\n",
    "\n",
    "    # Return Prediction\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Candidate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Candidate Models in one Function\n",
    "def candidate_models(y_, X_, t_, k_range_, cr_range_, rep_range_):\n",
    "    \n",
    "    ### Pre-Process Data ###\n",
    "    y_train, X_train, y_pred, X_pred = prepro(y_, X_, t_)\n",
    "    \n",
    "    ### Subset Forecasts ###\n",
    "    if np.sum(k_range_) == 0:\n",
    "        preds_ssf = np.array([]) \n",
    "    else: \n",
    "        feature_set       = list(chain(*list(map(lambda k: complete_sub(list(range(1, X_train.shape[1])), k), k_range_))))\n",
    "        preds_ssf, coeffs = zip(*list(map(lambda feature: ssf(y_train, X_train, X_pred, feature, 0), feature_set)))\n",
    "    \n",
    "    ### Compressed Regressions ###\n",
    "    if (np.sum(cr_range_) == 0) or (np.sum(rep_range_) == 0):\n",
    "        preds_cr = np.array([]) \n",
    "    else:\n",
    "        preds_cr = np.array(list(map(lambda z: cr_reg(y_train, X_train, X_pred, z[0], 0, z[1]), product(cr_range_, rep_range_))))\n",
    "            \n",
    "    ### Concatenate Predictions ###\n",
    "    cand_forecasts = np.concatenate([preds_ssf, preds_cr])\n",
    "    \n",
    "    return cand_forecasts\n",
    "\n",
    "#f = partial(candidate_models, y_ = y, X_ = X, k_range_ = k_range, cr_range_ = cr_range, rep_range_ = rep_range)\n",
    "#with mp.Pool(5) as pool:\n",
    "#    cand_forecasts = np.array([result for result in pool.map(lambda t: f(t_ = t), range(init, n_obs))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AR-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive Model\n",
    "def ar_mod(y_train, lags):\n",
    "\n",
    "    # Fit AR-Model\n",
    "    model = AutoReg(y_train, lags=lags).fit()\n",
    "    \n",
    "    # Prediction\n",
    "    pred = model.forecast(steps=1)\n",
    "\n",
    "    # Return Prediction\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Selection of Forecasts (D-Wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, cf_train, cf_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    cf_train  =  cf_train / n_sub\n",
    "    cf_pred   =  cf_pred  / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(cf_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ cf_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat) \n",
    "    Q         =  - 2 * diag_mat + cf_train.transpose() @ cf_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    ## Initialize BQM\n",
    "    #bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    #bqm  =  bqm.from_qubo(Q)\n",
    "    #\n",
    "    ## Normalize\n",
    "    #bqm.normalize()\n",
    "    #\n",
    "    ## Preprocess (?)\n",
    "    ##roof_duality(bqm)    \n",
    "    #\n",
    "    ## Select Solver\n",
    "    #solver_qpu  =  SimulatedAnnealingSampler() #LeapHybridSampler() SimulatedAnnealingSampler() EmbeddingComposite(DWaveSampler())\n",
    "    ##solver_pp   =  SteepestDescentSolver()    #SteepestDescentSolver()\n",
    "    #\n",
    "    ## Submit for Solution\n",
    "    #sampleset  =  solver_qpu.sample(bqm, \n",
    "    #                                num_reads = n_times,\n",
    "    #                                #time_limit = 90,\n",
    "    #                                label = \"Best Subset Selection of Forecasts\",\n",
    "    #                                seed = 123) # f'Best Subset Selection of Forecasts{t}'\n",
    "    #\n",
    "    ### Postprocess Problem\n",
    "    ##sampleset_pp = solver_pp.sample(bqm,\n",
    "    ##                                initial_states = sampleset.lowest())\n",
    "    #\n",
    "    ## Get Solution\n",
    "    #solution    =  np.array(list(sampleset.first[0].values()))\n",
    "    \n",
    "    # Solve with Gurobi\n",
    "    #result    =  solve_qubo(Q)\n",
    "    #solution  =  result.solution\n",
    "    model = gp.Model()\n",
    "    model.Params.TimeLimit = 1*1\n",
    "    b = model.addMVar(shape=Q.shape[0], vtype=gp.GRB.BINARY, name=\"b\")\n",
    "    model.setObjective(b @ Q @ b, gp.GRB.MINIMIZE)\n",
    "    model.optimize()\n",
    "    solution = np.array(model.x)\n",
    "    \n",
    "    # Test\n",
    "    if np.sum(solution) != n_sub:\n",
    "        print(f\"Warning: Number of selected features does not match --- {np.sum(solution)} instead of {n_sub}!\")\n",
    "    \n",
    "    # Prediction \n",
    "    pred = solution @ cf_pred.transpose()\n",
    "    \n",
    "    # Return \n",
    "    return(pred[0], solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:44<00:00, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSSF: Avg. OOS-R2 for Corr. 0.95 and Betas [1. 1. 1. 1. 0. 0. 0. 0.] is: [-9.51 -8.12 -7.73 -6.28 -6.21 -6.72 -6.22 -4.91 -5.25 -5.66 -5.81]%\n",
      "CSR:  Avg. OOS-R2 for Corr. 0.95 and Betas [1. 1. 1. 1. 0. 0. 0. 0.] is: [ 0.   -9.57 -8.71 -7.95 -7.32 -6.81 -6.41 -6.1  -5.88]%\n",
      "BSSF: Avg. OOS-R2 for Corr. 0.95 and Betas [1. 1. 1. 1. 1. 1. 1. 1.] is: [-18.3   -8.95 -11.17 -10.36  -8.5  -10.31 -10.02  -9.19  -9.43  -8.35\n",
      "  -9.39]%\n",
      "CSR:  Avg. OOS-R2 for Corr. 0.95 and Betas [1. 1. 1. 1. 1. 1. 1. 1.] is: [  0.   -12.32 -11.7  -11.01 -10.39  -9.85  -9.4   -9.01  -8.71]%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###### MC-Parameter ######\n",
    "# Number of MC-Runs\n",
    "n_mc  =  500\n",
    "\n",
    "# Set Parameter\n",
    "n_obs       =  100\n",
    "n_preds     =  8\n",
    "init        =  50\n",
    "mlags       =  0\n",
    "corr_range  =  [0.0, 0.5, 0.95]\n",
    "b_range     =  np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                         [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                         [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "###### Parameter Subset Forecasts ######\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3, 4, 5, 6, 7, 8] \n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  \n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "if np.sum(k_range) == 0:\n",
    "    n_sub  =  0\n",
    "else:\n",
    "    n_sub  =  int(sum([min(n_models(n_preds, k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2, 3, 4] \n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 60) \n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "if (np.sum(cr_range) == 0) or (len(rep_range) == 0):\n",
    "    n_cr = 0\n",
    "else: \n",
    "    n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) \n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  10.0\n",
    "n_times     =  250\n",
    "bssf_range  =  [1, 10, 25, 100, 495] \n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "######## Objects ########\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts   = np.full((len(range(init, n_obs)), (n_sub + n_cr)), np.nan)     \n",
    "benchmark        = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "cf_weights       = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf, (n_sub + n_cr)), np.nan)\n",
    "bssf_forecast    = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "csr_forecast     = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "se_benchmark     = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "se_bssf_forecast = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "se_csr_forecast  = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Monte-Carlo Runs\n",
    "i = 0\n",
    "for r in tqdm(range(n_mc)):\n",
    "    \n",
    "    # Loop over Covariance-Sets\n",
    "    for p in corr_range:\n",
    "        \n",
    "        # Loop over Coefficient-Sets\n",
    "        for b in b_range:\n",
    "    \n",
    "            ### Simulate Data ###\n",
    "            y, X, pred_names = sim(n_obs, n_preds, b, p, r)\n",
    "            \n",
    "            ### Create Candidate Models ###\n",
    "            if __name__ == '__main__':\n",
    "                pool_ = Pool(6)\n",
    "                cand_forecasts = np.array(pool_.map(lambda t: candidate_models(y, X, t, k_range, cr_range, rep_range), range(init, n_obs)))\n",
    "                pool_.close()\n",
    "                \n",
    "                #f = partial(candidate_models, y_ = y, X_ = X, k_range_ = k_range, cr_range_ = cr_range, rep_range_ = rep_range)\n",
    "                #with mp.Pool(5) as pool:\n",
    "                #    cand_forecasts = np.array([result for result in pool.map(lambda t: f(t_ = t), range(init, n_obs))])\n",
    "\n",
    "            ## Loop over t / Create Candidate Models\n",
    "            #for t in range(init, n_obs):\n",
    "            #\n",
    "            #    ### Pre-Process Data ###\n",
    "            #    y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "            #\n",
    "            #    ### Subset Forecasts ###\n",
    "            #    feature_set  =  list(chain(*list(map(lambda k: complete_sub(list(range(1, X_train.shape[1])), k), k_range))))\n",
    "            #    preds_ssf, coeffs  =  zip(*list(map(lambda feature: ssf(y_train, X_train, X_pred, feature, 0), feature_set)))\n",
    "            #\n",
    "            #    ### Compressed Regressions ###\n",
    "            #    preds_cr  = np.array(list(map(lambda z: cr_reg(y_train, X_train, X_pred, z[0], 0, z[1]), product(cr_range, rep_range))))\n",
    "            #\n",
    "            #    # ### Decision Tree Regressions ###\n",
    "            #    # preds_dt = np.array(list(map(lambda r: dt_reg(y_train, X_train, X_pred, r), dt_range)))\n",
    "            #\n",
    "            #    # Append Results\n",
    "            #    cand_forecasts[t-init][:n_sub]             =  preds_ssf \n",
    "            #    cand_forecasts[t-init][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "            #    #cand_forecasts[t-init][(n_sub+n_cr):]     =  preds_dt\n",
    "            \n",
    "            ### Benchmark: PHM ###\n",
    "            benchmark[i]    = y[:-1].mean()\n",
    "            se_benchmark[i] = (y[-1] - benchmark[i]) ** 2\n",
    "                \n",
    "            ### Benchmark: Complete Subset Regression ###\n",
    "            tmp_ = np.cumsum([0] + [len(list(combinations(range(n_preds), k))) for k in k_range])\n",
    "            csr_forecast[i]     =  [np.mean(cand_forecasts[-1, :n_sub][tmp_[i]:tmp_[i+1]]) for i in range(len(tmp_)-1)]\n",
    "            se_csr_forecast[i]  =  (y[-1] - csr_forecast[i]) ** 2\n",
    "\n",
    "            ### Best Selection of Forecast ###\n",
    "            bssf_forecast[i], cf_weights[i] = zip(*list(map(lambda s: bssf(y[init:-1], cand_forecasts[:-1], cand_forecasts[[-1]], alpha, s, n_times), bssf_range)))\n",
    "            se_bssf_forecast[i] = (y[-1] - bssf_forecast[i]) ** 2\n",
    "             \n",
    "            # Update index   \n",
    "            i += 1\n",
    "            \n",
    "# # Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(n_preds), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "\n",
    "### Evaluation ###\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up DataFrame\n",
    "dataframe_plot = pd.DataFrame()\n",
    "\n",
    "# Create DataFrame\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        # Subset Weights\n",
    "        chosen_cm = cf_weights[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "        # Aggregated\n",
    "        chosen_cm = np.sum(chosen_cm, axis = 0)\n",
    "\n",
    "        # Replace Zero with NaN\n",
    "        chosen_cm[chosen_cm == 0] = np.nan\n",
    "        chosen_cm_agg = chosen_cm.copy()\n",
    "\n",
    "        # Set to One\n",
    "        chosen_cm[chosen_cm > 1 ] = 1\n",
    "\n",
    "        # Adapt Column-Values\n",
    "        chosen_cm = chosen_cm * np.arange(1, chosen_cm.shape[1]+1)\n",
    "\n",
    "        # Create DataFrame\n",
    "        chosen_cm = pd.DataFrame(chosen_cm, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "        chosen_cm_agg = pd.DataFrame(chosen_cm_agg, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "\n",
    "        # Index\n",
    "        chosen_cm = chosen_cm.reset_index(names=\"INDEX\")\n",
    "        chosen_cm_agg = chosen_cm_agg.reset_index(names=\"INDEX\")\n",
    "\n",
    "        # Melt Data\n",
    "        chosen_cm = chosen_cm.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Value')\n",
    "        chosen_cm_agg = chosen_cm_agg.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Weight')\n",
    "\n",
    "        # Concatenate\n",
    "        chosen_cm = pd.concat([chosen_cm, chosen_cm_agg])\n",
    "        \n",
    "        # Transform Candidate Model Names\n",
    "        chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.split('_').str[0]\n",
    "        \n",
    "        # Add Column for Correlation\n",
    "        chosen_cm['Correlation'] = corr_range[p] \n",
    "        \n",
    "        # Add Column for Betas\n",
    "        chosen_cm['Betas'] = str(b_range[b])\n",
    "        \n",
    "        # Append\n",
    "        dataframe_plot = pd.concat([dataframe_plot, chosen_cm])\n",
    "        \n",
    "# Plot Theme\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Plot Data\n",
    "g = sns.relplot(\n",
    "    data = dataframe_plot,\n",
    "    y = 'Value', x = 'INDEX',\n",
    "    hue = \"Candidate_Model\", size = \"Weight\", \n",
    "    col = \"Betas\", row = \"Correlation\",\n",
    "    sizes = (3, 75), height = 5.0, aspect = 1.5,\n",
    "    alpha = 0.75, palette = \"muted\",\n",
    "    #legend = True\n",
    "    )\n",
    "\n",
    "# Legend\n",
    "#g._legend.remove()\n",
    "#h, l = g.ax.get_legend_handles_labels()\n",
    "#g.ax.legend(h[0:13], l[0:13], bbox_to_anchor=(1.0, 0.75), loc=2, fontsize=10, frameon=False)\n",
    "\n",
    "# Title & Axis\n",
    "g.set(xlabel='Combination Size',\n",
    "      ylabel='Candidate Models')\n",
    "      #title=f\"Selected Candidate Models: \\n Correlation {corr_range[p]} & Betas {b_range[b]}\")\n",
    "\n",
    "# Margins\n",
    "#g.ax.margins(x = 0.05, y = 0)\n",
    "\n",
    "# Set number of ticks for y-axis\n",
    "idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "#g.ax.set_yticks(idx)\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "#g.ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "## iterate over axes of FacetGrid\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_yticks(idx)\n",
    "    ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "# Tick-Label Size\n",
    "g.set_yticklabels(size = 8)\n",
    "g.set_xticklabels(size = 10)\n",
    "\n",
    "# Add Horizontal Lines\n",
    "#for i in idx:\n",
    "#    g.ax.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "#for label in g.ax.get_xticklabels():\n",
    "#    label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        # Subset Weights\n",
    "        chosen_cm = cf_weights[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "        # Aggregated\n",
    "        chosen_cm = np.sum(chosen_cm, axis = 0)\n",
    "\n",
    "        # Replace Zero with NaN\n",
    "        chosen_cm[chosen_cm == 0] = np.nan\n",
    "        chosen_cm_agg = chosen_cm.copy()\n",
    "\n",
    "        # Set to One\n",
    "        chosen_cm[chosen_cm > 1 ] = 1\n",
    "\n",
    "        # Adapt Column-Values\n",
    "        chosen_cm = chosen_cm * np.arange(1, chosen_cm.shape[1]+1)\n",
    "\n",
    "        # Create DataFrame\n",
    "        chosen_cm = pd.DataFrame(chosen_cm, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "        chosen_cm_agg = pd.DataFrame(chosen_cm_agg, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "\n",
    "        # Index\n",
    "        chosen_cm = chosen_cm.reset_index(names=\"INDEX\")\n",
    "        chosen_cm_agg = chosen_cm_agg.reset_index(names=\"INDEX\")\n",
    "\n",
    "        # Melt Data\n",
    "        chosen_cm = chosen_cm.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Value')\n",
    "        chosen_cm_agg = chosen_cm_agg.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Weight')\n",
    "\n",
    "        # Concatenate\n",
    "        chosen_cm = pd.concat([chosen_cm, chosen_cm_agg])\n",
    "\n",
    "        # Transform Candidate Model Names\n",
    "        chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.split('_').str[0]\n",
    "        #chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.startswith(\"SSF\").replace({True: \"SSF\", False: \"CR\"})\n",
    "\n",
    "        # Plot Theme\n",
    "        sns.set_theme(style=\"ticks\")\n",
    "\n",
    "        # Draw each cell as a scatter point with varying size and color\n",
    "        g = sns.relplot(\n",
    "            data = chosen_cm,\n",
    "            y = 'Value', x = 'INDEX', hue = \"Candidate_Model\", \n",
    "            size = \"Weight\", sizes = (3, 75),\n",
    "            height = 6.5, alpha = 0.75, palette=\"muted\",\n",
    "            #legend = True\n",
    "            )\n",
    "\n",
    "        # Legend\n",
    "        g._legend.remove()\n",
    "        h, l = g.ax.get_legend_handles_labels()\n",
    "        g.ax.legend(h[0:13], l[0:13], bbox_to_anchor=(1.0, 0.75), loc=2, fontsize=10, frameon=False)\n",
    "\n",
    "        # Title & Axis\n",
    "        g.set(xlabel='Combination Size',\n",
    "              ylabel='Candidate Models',\n",
    "              title=f\"Selected Candidate Models: \\n Correlation {corr_range[p]} & Betas {b_range[b]}\")\n",
    "\n",
    "        # Margins\n",
    "        g.ax.margins(x = 0.05, y = 0)\n",
    "\n",
    "        # Set number of ticks for y-axis\n",
    "        idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "        g.ax.set_yticks(idx)\n",
    "\n",
    "        # Set ticks labels for x-axis\n",
    "        g.ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "        # Tick-Label Size\n",
    "        g.set_yticklabels(size = 8)\n",
    "        g.set_xticklabels(size = 10)\n",
    "\n",
    "        # Add Horizontal Lines\n",
    "        #for i in idx:\n",
    "        #    g.ax.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "        # Rotate x-axis labels\n",
    "        for label in g.ax.get_xticklabels():\n",
    "            label.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Plot\n",
    "fig, ax = plt.subplots(1,1) \n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "# Plot \n",
    "colors = {'SSF':'blue', 'CR':'green'}\n",
    "ax.scatter(x = chosen_cm['INDEX'], y = chosen_cm['Weight'], c=chosen_cm['Candidate_Model'].map(colors), s = 1)\n",
    "#ax.plot(chosen_cm['INDEX'], chosen_cm.iloc[:, 1:], marker = \"o\", lw = 0, ms = 4)\n",
    "\n",
    "# Add title and axis names\n",
    "plt.title('Selected Candidate Models')\n",
    "plt.ylabel('Candidate Models')\n",
    "plt.xlabel('Combination Size')\n",
    "\n",
    "# Legend\n",
    "#plt.legend(loc = \"upper right\")\n",
    "\n",
    "# Margins\n",
    "plt.margins(x=0.10, y=0)\n",
    "\n",
    "# Set number of ticks for x-axis\n",
    "idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "ax.set_yticks(idx)\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "ax.set_yticklabels([[[*ssf_names, *cr_names]][0][i] for i in idx], rotation='horizontal', fontsize=6)\n",
    "\n",
    "# Add horizontal lines\n",
    "for i in idx:\n",
    "    plt.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot: Welches K hat was ausgewählt\n",
    "### Benchmark: Complete Subset Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is:\" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is:\" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = benchmark[np.arange(0, n_mc * len(corr_range) * len(b_range), len(b_range))]\n",
    "b2 = benchmark[np.arange(1, n_mc * len(corr_range) * len(b_range), len(b_range))]\n",
    "\n",
    "b1_p1 = b1[np.arange(0, n_mc * len(corr_range), len(corr_range))]\n",
    "b1_p2 = b1[np.arange(1, n_mc * len(corr_range), len(corr_range))]\n",
    "b1_p3 = b1[np.arange(2, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "b2_p1 = b2[np.arange(0, n_mc * len(corr_range), len(corr_range))]\n",
    "b2_p2 = b2[np.arange(1, n_mc * len(corr_range), len(corr_range))]\n",
    "b2_p3 = b2[np.arange(2, n_mc * len(corr_range), len(corr_range))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Goyal Welch Data\n",
    "data  =  pd.read_csv(path + r'/Data/PredictorData2022.xlsx - Quarterly.csv', thousands=',')\n",
    "\n",
    "# Equity Premium\n",
    "data['equity_premium'] = data['CRSP_SPvw'] - data['Rfree']\n",
    "\n",
    "# Dividend Price Ratio \n",
    "data['dp'] = np.log(data['D12']) - np.log(data['Index'])\n",
    "\n",
    "# Dividend Yield \n",
    "data['dy'] = np.log(data['D12'])- np.log(data['Index'].shift(1))\n",
    "\n",
    "# Earnings Price Ratio \n",
    "data['ep'] = np.log(data['E12']) - np.log(data['Index'])\n",
    "\n",
    "# Dividend Payout Ratio \n",
    "data['dpayr'] = np.log(data['D12']) - np.log(data['E12'])\n",
    "\n",
    "# Book to Market Ratio\n",
    "data['bmr'] = data['b/m']\n",
    "\n",
    "# # Net Equity Expansion\n",
    "data['ntis'] = data['ntis']\n",
    "\n",
    "# Treasury Bill Rate\n",
    "data['tbl'] = data['tbl']\n",
    "\n",
    "# Long Term Rate\n",
    "data['ltr'] = data['ltr']\n",
    "\n",
    "# Term Spread \n",
    "data['tsp'] = data['lty'] - data['tbl']\n",
    "\n",
    "# Default Return Spread \n",
    "data['dfr'] = data['corpr'] - data['ltr']\n",
    "\n",
    "# Inflation\n",
    "data['infl'] = data['infl']\n",
    "\n",
    "# Investment of Capital Ratio\n",
    "data['ik']  = data['ik']\n",
    "\n",
    "# Default Yield Spread\n",
    "data['dfy'] = data['BAA'] - data['AAA']\n",
    "\n",
    "# Realized Volatility\n",
    "data['rvol'] = data['svar']\n",
    "\n",
    "# reorganize the dataframe\n",
    "data = data[['yyyyq', \"equity_premium\", \"dp\", \"dy\", \"ep\", \"dpayr\", \"bmr\", \"ntis\", \"tbl\", \"ltr\", \"tsp\", \"dfr\", \"infl\", \"ik\"]]\n",
    "\n",
    "# Convert Date\n",
    "data['yyyyq'] = data['yyyyq'].astype(str)\n",
    "data['yyyyq'] = data.apply(lambda x: x['yyyyq'][:4]+'-Q'+x['yyyyq'][4:], axis=1)\n",
    "data['yyyyq'] = pd.to_datetime(data['yyyyq'])\n",
    "\n",
    "# Resetting the index\n",
    "data.set_index('yyyyq', inplace=True)\n",
    "data.index = data.index.to_period('Q')\n",
    "\n",
    "# Lag all Predictors\n",
    "data.iloc[:,1:]  =  data.iloc[:,1:].shift(1)\n",
    "\n",
    "# Drop Na\n",
    "data = data.loc[\"1946Q1\":, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Set Seed ######\n",
    "#random.seed(123)\n",
    "\n",
    "###### Data ######\n",
    "# Set Target Variable\n",
    "y  =  data.loc[:, [\"equity_premium\"]]\n",
    "X  =  data.drop(\"equity_premium\", axis = 1)\n",
    "\n",
    "# Get Predictor Names\n",
    "pred_names = list(X.columns)\n",
    "\n",
    "# Number of AR-Terms to include\n",
    "mlags =  2\n",
    "\n",
    "# Create Lags\n",
    "X  =  create_lags(y, X, mlags)\n",
    "\n",
    "# Drop Missing Values\n",
    "y  =  y.loc[\"1947Q2\":, ] #y[mlags:]\n",
    "X  =  X.loc[\"1947Q2\":, ] #X[mlags:]\n",
    "\n",
    "# Check NA\n",
    "any(X.isna().any())\n",
    "\n",
    "###### Parameter Subset Forecasts\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  # 20000\n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "n_sub  =  int(sum([min(n_models((X.shape[1]-mlags), k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2, 3] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 100) # 10000\n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) # 300000\n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  0.5\n",
    "n_times     =  50\n",
    "bssf_range  =  [1, 2, 3] # range(1, 5)\n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "### General Parameter ######\n",
    "# Initial Training-Period\n",
    "init       =  4 * 50 #4 * 10\n",
    "\n",
    "# Total Length\n",
    "total =  len(y) \n",
    "\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts  =  np.full((total, (n_sub + n_cr)), np.nan)     #np.full((total, (n_sub + n_cr + n_dt)), np.nan)\n",
    "cf_weights      =  np.full((total, (n_sub + n_cr)), np.nan)\n",
    "benchmark       =  np.full(total, np.nan)\n",
    "bssf_forecast   =  np.full(total, np.nan)\n",
    "bssf_opt        =  np.full(total, np.nan)\n",
    "sse_bssf        =  np.zeros(n_bssf)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Time\n",
    "for t in tqdm(range(init, total)):\n",
    "        \n",
    "    # Pre-Process Data\n",
    "    y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "    \n",
    "    ### Benchmark: AR(X) Model\n",
    "    pred          =  ar_mod(y_train, lags = mlags)\n",
    "    benchmark[t]  =  pred.iloc[0]\n",
    "    \n",
    "    ### Subset Forecasts\n",
    "    # Set up List to store Subset-Forecasts\n",
    "    preds_ssf =  np.full(n_sub, np.nan)\n",
    "    idx_sub   =  0\n",
    "    \n",
    "    # Loop over Subset Size \n",
    "    for k in k_range:\n",
    "    \n",
    "        # Get all possible Subset of length k\n",
    "        col_idx   =  list(range(mlags+1, X_train.shape[1]))\n",
    "        subs_idx  =  complete_sub(col_idx, k)\n",
    "\n",
    "        # Randomly select n_upper Subsets\n",
    "        feature_set  =  subs_idx #random_select(subs_idx, n_max, random_state = 123)\n",
    "\n",
    "        # Loop over Subsets\n",
    "        for feature in feature_set:\n",
    "\n",
    "            # Compute Subset-Regression-Forecast\n",
    "            pred  =  ssf(y_train, X_train, X_pred, feature, mlags)\n",
    "            preds_ssf[idx_sub] = pred\n",
    "            idx_sub += 1\n",
    "            \n",
    "    ### Compressed Regressions\n",
    "    # Set up List to store Compressed-Regression-Forecasts\n",
    "    preds_cr = np.full(n_cr, np.nan)\n",
    "    idx_cr   = 0\n",
    "    \n",
    "    # Loop over number of Components\n",
    "    for n_comp in cr_range:\n",
    "\n",
    "        # Loop over n repetitions\n",
    "        for r in rep_range:\n",
    "        \n",
    "            # Compute Compressed-Regression-Forecasts\n",
    "            pred  =  cr_reg(y_train, X_train, X_pred, n_comp, mlags, r)\n",
    "            preds_cr[idx_cr] = pred\n",
    "            idx_cr += 1\n",
    "            \n",
    "    # ### Decision Tree Regressions\n",
    "    # # Set up Matrix to store Decision-Tree-Forecasts\n",
    "    # preds_dt   = np.full(n_dt, np.nan)\n",
    "    # \n",
    "    # # Loop over number of Components\n",
    "    # for idx_dt, r in enumerate(dt_range):\n",
    "    #     \n",
    "    #     # Compute Decision-Tree-Forecasts\n",
    "    #     pred  =  dt_reg(y_train, X_train, X_pred, r)\n",
    "    #     preds_dt[idx_dt] = pred[0]\n",
    "\n",
    "    # Append Results\n",
    "    cand_forecasts[t][:n_sub]             =  preds_ssf \n",
    "    cand_forecasts[t][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "    #cand_forecasts[t][(n_sub+n_cr):]      =  preds_dt\n",
    "\n",
    "    ### Best Selection of Forecast\n",
    "    if t > init:\n",
    "    \n",
    "        # Set up Matrix to store Forecasts\n",
    "        bssf_forecasts  =  np.full(n_bssf, np.nan)\n",
    "        bssf_weights    =  np.zeros([n_bssf, n_sub + n_cr]) \n",
    "           \n",
    "        # Get \"best\" Subset-Size until now (lowest Sum of Squared Errors)\n",
    "        s_opt  =  np.argmin(sse_bssf)\n",
    "    \n",
    "        # Loop over Subset Sizes\n",
    "        for idx_bssf, s in enumerate(bssf_range):\n",
    "    \n",
    "            # Compute Best-Subset-Selection-of-Forecasts\n",
    "            pred  =  bssf(y_train[init:], cand_forecasts[init:t], cand_forecasts[t], alpha, s, n_times)\n",
    "            bssf_forecasts[idx_bssf]  =  pred[0]\n",
    "            bssf_weights[idx_bssf]    =  pred[1]\n",
    "    \n",
    "            # Compute Sum of Squared Errors\n",
    "            sse_bssf[idx_bssf] =  sse_bssf[idx_bssf] + (y_pred.iloc[0,0] - pred[0]) ** 2\n",
    "    \n",
    "        # Select Forecast \n",
    "        bssf_forecast[t] =  bssf_forecasts[s_opt]\n",
    "        cf_weights[t]    =  bssf_weights[s_opt]\n",
    "        bssf_opt[t]      =  bssf_range[s_opt]\n",
    "        \n",
    "# Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(len(pred_names)), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "#dt_names = [f\"DT_{idx_dt}\" for idx_dt in dt_range]\n",
    "        \n",
    "# Convert Results to DataFrame\n",
    "cand_forecasts  =  pd.DataFrame(cand_forecasts, index = y.index, columns = [*ssf_names, *cr_names]) #, *dt_names])\n",
    "benchmark       =  pd.DataFrame(benchmark,      index = y.index, columns = [\"AR\"])\n",
    "bssf_forecast   =  pd.DataFrame(bssf_forecast,  index = y.index, columns = [\"BSSF\"])\n",
    "cf_weights      =  pd.DataFrame(cf_weights,     index = y.index, columns = [*ssf_names, *cr_names]) #, *dt_names])\n",
    "bssf_opt        =  pd.DataFrame(bssf_opt,     index = y.index, columns = [\"Subset_Size\"])\n",
    "\n",
    "# Cut off initial Training-Period\n",
    "sub_y              =  y.iloc[init:].copy()\n",
    "sub_cand_forecasts =  cand_forecasts.iloc[init:].copy()\n",
    "sub_benchmark      =  benchmark.iloc[init:].copy()\n",
    "sub_bssf_forecast  =  bssf_forecast.iloc[init:].copy()\n",
    "sub_cf_weights     =  cf_weights.iloc[init:].copy()\n",
    "sub_bssf_opt       =  bssf_opt.iloc[init:].copy()\n",
    "\n",
    "# OOS-Period\n",
    "oos_start  =  \"1999Q4\"\n",
    "oos_end    =  \"2022Q4\" \n",
    "oos_y             =  sub_y.loc[oos_start:oos_end].copy()\n",
    "oos_cand_forecast =  sub_cand_forecasts.loc[oos_start:oos_end].copy()\n",
    "oos_benchmark     =  sub_benchmark.loc[oos_start:oos_end].copy()\n",
    "oos_bssf_forecast =  sub_bssf_forecast.loc[oos_start:oos_end].copy()\n",
    "oos_cf_weights    =  sub_cf_weights.loc[oos_start:oos_end].copy()\n",
    "oos_bssf_opt      =  sub_bssf_opt.loc[oos_start:oos_end].copy()\n",
    "\n",
    "# Evaluation\n",
    "np.sum((oos_y.iloc[:,0] - oos_bssf_forecast.iloc[:,0]) ** 2) / np.sum((oos_y.iloc[:,0] - oos_benchmark.iloc[:,0]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Zero with NaN\n",
    "oos_cf_weights.replace({0:np.nan}, inplace=True)\n",
    "\n",
    "# Adapt Column-Values\n",
    "vec = list(range(1, oos_cf_weights.shape[1]+1))\n",
    "tmp = oos_cf_weights * vec\n",
    "\n",
    "# Dates\n",
    "tmp = tmp.reset_index(names=\"date\")\n",
    "\n",
    "# Plot \n",
    "# tmp.plot(x='date', y = tmp.columns[1:],\n",
    "#          figsize=(10, 5), legend=False,\n",
    "#          marker=\"o\", ms = 1, \n",
    "#          title=\"Selected Candidate Models\", ylabel=\"Selected Candidate Models\")\n",
    "#          #yticks = (np.arange(98), list(tmp.columns[1:])))\n",
    "\n",
    "tmp_long = pd.melt(tmp, id_vars = \"date\")\n",
    "tmp_long['variable'] = tmp_long['variable'].str.startswith(\"SSF\").replace({True: \"SSF\", False: \"CR\"})\n",
    "tmp_long.set_index(\"date\", inplace = True)\n",
    "tmp_long.groupby(\"variable\")[\"value\"].plot(legend=True, figsize = (10, 5),\n",
    "                                           marker=\"o\", ms = 2, lw = 0,\n",
    "                                           ylim = [-1, cand_forecasts.shape[1]+1],\n",
    "                                           title=\"Selected Candidate Models\", ylabel=\"Selected Candidate Models\")\n",
    "plt.show()\n",
    "\n",
    "# Subset Size\n",
    "oos_bssf_opt.plot(figsize=(10, 5), legend=False, \n",
    "                  color = \"black\", marker=\"o\", ms = 1, lw = 0,\n",
    "                  title = \"Subset Size\", xlabel=\"date\", ylabel=\"Selected Subset Size\",\n",
    "                  ylim  =  [min(bssf_range)-0.5, max(bssf_range)+0.5],\n",
    "                  yticks = np.arange(min(bssf_range), max(bssf_range)+1, step=1.0))\n",
    "plt.show()\n",
    "\n",
    "# CSSED\n",
    "cssed = np.cumsum(((oos_y.iloc[:,0] - oos_benchmark.iloc[:,0]) ** 2) - ((oos_y.iloc[:,0] - oos_bssf_forecast.iloc[:,0]) ** 2))\n",
    "cssed.plot(figsize=(10, 5),\n",
    "            xlabel = \"date\", ylabel = \"CSSED\", title = \"Cumulated Sum of Squared Error Differences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MC-Parameter ######\n",
    "# Number of MC-Runs\n",
    "n_mc  =  2\n",
    "\n",
    "# Set Parameter\n",
    "n_obs       =  100\n",
    "n_preds     =  8\n",
    "init        =  50\n",
    "mlags       =  0\n",
    "corr_range  =  [0.5, 0.95]\n",
    "b_range     =  np.array([[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "###### Parameter Subset Forecasts ######\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3, 4, 5, 6, 7, 8] \n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  \n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "n_sub  =  int(sum([min(n_models(n_preds, k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2] \n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 50) \n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) \n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  10.0\n",
    "n_times     =  1\n",
    "bssf_range  =  [1, 2, 3] \n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "######## Objects ########\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts   = np.full((len(range(init, n_obs)), (n_sub + n_cr)), np.nan)     \n",
    "benchmark        = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "cf_weights       = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf, (n_sub + n_cr)), np.nan)\n",
    "bssf_forecast    = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "csr_forecast     = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "se_benchmark     = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "se_bssf_forecast = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "se_csr_forecast  = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Monte-Carlo Runs\n",
    "i = 0\n",
    "for r in tqdm(range(n_mc)):\n",
    "    \n",
    "    # Loop over Covariance-Sets\n",
    "    for p in corr_range:\n",
    "        \n",
    "        # Loop over Coefficient-Sets\n",
    "        for b in b_range:\n",
    "    \n",
    "            ### Simulate Data ###\n",
    "            y, X, pred_names = sim(n_obs, n_preds, b, p, r)\n",
    "            \n",
    "            ### Benchmark: PHM ###\n",
    "            benchmark[i]    = y.iloc[:-1].mean().iloc[0]\n",
    "            se_benchmark[i] = (y.iloc[-1,0] - benchmark[i]) ** 2\n",
    "\n",
    "            # Loop over t / Create Candidate Models\n",
    "            for t in range(init, n_obs):\n",
    "            \n",
    "                ### Pre-Process Data ###\n",
    "                y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "\n",
    "                ### Subset Forecasts ###\n",
    "                feature_set  =  list(chain(*list(map(lambda k: complete_sub(list(range(1, X_train.shape[1])), k), k_range))))\n",
    "                preds_ssf    =  np.array(list(map(lambda feature: ssf(y_train, X_train, X_pred, feature, 0), feature_set)))\n",
    "            \n",
    "                ## Set up List to store Subset-Forecasts\n",
    "                #preds_ssf = np.full(n_sub, np.nan)\n",
    "                #idx_sub   = 0\n",
    "                #\n",
    "                ## Loop over Subset Size \n",
    "                #for k in k_range:\n",
    "                #\n",
    "                #    # Get all possible Subsets of length k\n",
    "                #    col_idx  = list(range(1, X_train.shape[1]))\n",
    "                #    subs_idx = complete_sub(col_idx, k)\n",
    "                #\n",
    "                #    # Randomly select n_upper Subsets\n",
    "                #    feature_set  =  subs_idx #random_select(subs_idx, n_max, random_state = 123)\n",
    "                #\n",
    "                #    # Loop over Subsets\n",
    "                #    for feature in feature_set:\n",
    "                #    \n",
    "                #        # Compute Subset-Regression-Forecast\n",
    "                #        pred  =  ssf(y_train, X_train, X_pred, feature, 0)\n",
    "                #        preds_ssf[idx_sub] = pred\n",
    "                #        idx_sub += 1\n",
    "\n",
    "                ### Compressed Regressions ###\n",
    "                preds_cr = np.array(list(chain(*[list(map(lambda rep: cr_reg(y_train, X_train, X_pred, n_comp, 0, rep), rep_range)) for n_comp in cr_range])))\n",
    "                \n",
    "                # # Set up List to store Compressed-Regression-Forecasts\n",
    "                # preds_cr   = np.full(n_cr, np.nan)\n",
    "                # idx_cr     = 0\n",
    "                # \n",
    "                # # Loop over number of Components\n",
    "                # for n_comp in cr_range:\n",
    "                # \n",
    "                #     # Loop over n repetitions\n",
    "                #     for rep in rep_range:\n",
    "                #     \n",
    "                #         # Compute Compressed-Regression-Forecasts\n",
    "                #         pred  =  cr_reg(y_train, X_train, X_pred, n_comp, 0, rep)\n",
    "                #         preds_cr[idx_cr] = pred\n",
    "                #         idx_cr += 1\n",
    "\n",
    "                # ### Decision Tree Regressions\n",
    "                # preds_dt = np.array(list(map(lambda r: dt_reg(y_train, X_train, X_pred, r), dt_range)))\n",
    "                \n",
    "                # # Set up Matrix to store Decision-Tree-Forecasts\n",
    "                # preds_dt   = np.full(n_dt, np.nan)\n",
    "                # \n",
    "                # # Loop over number of Components\n",
    "                # for idx_dt, r in enumerate(dt_range):\n",
    "                #     \n",
    "                #     # Compute Decision-Tree-Forecasts\n",
    "                #     pred  =  dt_reg(y_train, X_train, X_pred, r)\n",
    "                #     preds_dt[idx_dt] = pred[0]\n",
    "\n",
    "                # Append Results\n",
    "                cand_forecasts[t-init][:n_sub]             =  preds_ssf \n",
    "                cand_forecasts[t-init][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "                #cand_forecasts[t-init][(n_sub+n_cr):]     =  preds_dt\n",
    "                \n",
    "            ### Benchmark: Complete Subset Regression ###\n",
    "            tmp_ = np.cumsum([0] + [len(list(combinations(range(n_preds), k))) for k in k_range])\n",
    "            csr_forecast[i]     =  [np.mean(preds_ssf[tmp_[i]:tmp_[i+1]]) for i in range(len(tmp_)-1)]\n",
    "            se_csr_forecast[i]  =  (y_pred.iloc[0,0] - csr_forecast[i]) ** 2\n",
    "\n",
    "            ### Best Selection of Forecast ###\n",
    "            bssf_forecast[i], cf_weights[i] = zip(*list(map(lambda s: bssf(y_train[init:], cand_forecasts[:-1], cand_forecasts[-1], alpha, s, n_times), bssf_range)))\n",
    "            se_bssf_forecast[i] = (y_pred.values[0] - bssf_forecast[i]) ** 2\n",
    "            \n",
    "            # # Set up Matrix to store Forecasts\n",
    "            # kssf_forecasts  =  np.full(n_bssf, np.nan)\n",
    "            # kssf_weights    =  np.zeros([n_bssf, n_sub + n_cr]) \n",
    "\n",
    "            # # Loop over Subset Sizes\n",
    "            # for idx_bssf, s in enumerate(bssf_range):\n",
    "            #     \n",
    "            #     # Compute Best-Subset-Selection-of-Forecasts\n",
    "            #     pred = bssf(y_train[init:], cand_forecasts[:-1], cand_forecasts[-1], alpha, s, n_times)\n",
    "            #     bssf_forecast[i][idx_bssf] = pred[0]\n",
    "            #     cf_weights[i][idx_bssf]    = pred[1]\n",
    "            #     se_bssf_forecast[i][idx_bssf] = (y_pred.iloc[0,0] - pred[0]) ** 2\n",
    "             \n",
    "            # Update index   \n",
    "            i += 1\n",
    "            \n",
    "# # Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(n_preds), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "\n",
    "### Evaluation ###\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Fred-MD-Data stationary\n",
    "def transform_tcode(data):\n",
    "    \n",
    "    # Get Transformation-Code\n",
    "    tcode = data[0]\n",
    "    \n",
    "    # Get Data\n",
    "    data = data[1:]\n",
    "\n",
    "    if tcode == 1:\n",
    "        output = data\n",
    "    elif tcode == 2:\n",
    "        output = data - np.roll(data, 1)\n",
    "    elif tcode == 3:\n",
    "        output = (data - np.roll(data, 1)) - (np.roll(data, 1) - np.roll(data, 2))\n",
    "    elif tcode == 4:\n",
    "        output = np.log(data)\n",
    "    elif tcode == 5:\n",
    "        output = np.log(data) - np.roll(np.log(data), 1)\n",
    "    elif tcode == 6:\n",
    "        output = (np.log(data) - np.roll(np.log(data), 1)) - (np.roll(np.log(data), 1) - np.roll(np.log(data), 2))\n",
    "    else:\n",
    "        output = (data / np.roll(data, 1) - 1) - (np.roll(data, 1) / np.roll(data, 2) - 1)\n",
    "\n",
    "    return np.concatenate(([tcode], output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x_dataset  =  pd.read_csv(path + r'/fred_md_202306.csv')\n",
    "\n",
    "# Drop Variables with too many missing values\n",
    "x_dataset  =  x_dataset.drop([\"PERMIT\", \"PERMITNE\", \"PERMITMW\", \"PERMITS\",\n",
    "                              \"PERMITW\", \"ACOGNO\", \"ANDENOx\", \"CP3Mx\",\n",
    "                              \"COMPAPFFx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VIXCLSx\"],\n",
    "                              axis=1)\n",
    "\n",
    "# Transform remaining Columns\n",
    "x_dataset.iloc[:, 1:]  =  x_dataset.iloc[:,1:].apply(lambda x: transform_tcode(x))\n",
    "\n",
    "# Drop First Row with Transformation-Code\n",
    "x_dataset  =  x_dataset.iloc[1:,:]\n",
    "\n",
    "# Lag Data\n",
    "x_dataset.iloc[:,1:]  =  x_dataset.iloc[:,1:].shift(1)\n",
    "\n",
    "# Convert Date\n",
    "x_dataset['sasdate']  =  pd.to_datetime(x_dataset['sasdate'])\n",
    "\n",
    "# Filter Data\n",
    "x_dataset  =  x_dataset[(x_dataset['sasdate'] >= '1959-04-01') & (x_dataset['sasdate'] <= '2023-03-01')]\n",
    "\n",
    "# Resetting the index\n",
    "x_dataset.set_index('sasdate', inplace=True)\n",
    "x_dataset.index = x_dataset.index.to_period('M')\n",
    "\n",
    "# Load Data\n",
    "y_dataset  =  pd.read_csv(path + r'/fred_md_202306.csv')\n",
    "\n",
    "# Select and Rename Variables\n",
    "y_dataset  =  y_dataset.loc[:, [\"sasdate\", \"CPIAUCSL\", \"INDPRO\", \"UNRATE\"]]\n",
    "y_dataset  =  y_dataset.rename(columns={'CPIAUCSL': 'CPIAUCSL_h1', 'INDPRO': 'INDPRO_h1', 'UNRATE': 'UNRATE_h1'})\n",
    "\n",
    "# Transform Variables\n",
    "y_dataset[[\"CPIAUCSL_h1\"]]  =  y_dataset[[\"CPIAUCSL_h1\"]].apply(lambda x: 4 * 100 * (np.log(x) - np.roll(np.log(x), 1)))\n",
    "y_dataset[[\"INDPRO_h1\"]]    =  y_dataset[[\"INDPRO_h1\"]].apply(lambda x: 4 * 100 * (np.log(x) - np.roll(np.log(x), 1)))\n",
    "y_dataset[[\"UNRATE_h1\"]]    =  y_dataset[[\"UNRATE_h1\"]]\n",
    "\n",
    "# Drop First Row with Transformation-Code\n",
    "y_dataset  =  y_dataset.iloc[1:,:]\n",
    "\n",
    "# Convert Date\n",
    "y_dataset['sasdate']  =  pd.to_datetime(y_dataset['sasdate'])\n",
    "\n",
    "# Filter Data\n",
    "y_dataset  =  y_dataset[(y_dataset['sasdate'] >= '1959-04-01') & (y_dataset['sasdate'] <= '2023-03-01')]\n",
    "\n",
    "# Resetting the index\n",
    "y_dataset.set_index('sasdate', inplace=True)\n",
    "y_dataset.index = y_dataset.index.to_period('M')\n",
    "\n",
    "# Set Target Variable\n",
    "y  =  y_dataset.loc[:, [\"CPIAUCSL_h1\"]]\n",
    "X  =  x_dataset.drop(\"CPIAUCSL\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Selection of Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, X_train, X_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    X_train  =  X_train / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(X_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ X_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat[0])\n",
    "    Q         =  - 2 * diag_mat + X_train.transpose() @ X_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    # Initialize BQM\n",
    "    bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    bqm  =  bqm.from_qubo(Q)\n",
    "    \n",
    "    # Solve\n",
    "    solver     =  SimulatedAnnealingSampler()\n",
    "    #solver     =  SteepestDescentSolver()\n",
    "    #solver     =  TabuSampler()\n",
    "    #solver     =  TreeDecompositionSolver()\n",
    "\n",
    "    sampleset  =  solver.sample(bqm, num_reads = n_times)\n",
    "    solution   =  list(sampleset.first[0].values())\n",
    "    \n",
    "    # Prediction \n",
    "    pred       = solution @ X_pred\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series-Cross-Validation to Tune Parameter k\n",
    "def cv_ts(Y, X, splits, k_seq, alpha, n_times):\n",
    "\n",
    "    # Set up TS-Split\n",
    "    tscv    =  TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Set up Result-Matrix\n",
    "    cv_res  =  np.zeros((splits, len(k_seq)))\n",
    "\n",
    "    # Loop over Train-Test-Splits\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        # Split Data\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index],   Y.iloc[test_index]\n",
    "\n",
    "        # Cross-Validation k\n",
    "        for k in k_seq:\n",
    "\n",
    "            # Selected Predictors\n",
    "            solution    =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[1]\n",
    "\n",
    "            # Prediction\n",
    "            prediction  =  solution @ X_test.transpose()\n",
    "\n",
    "            # MSE\n",
    "            cv_res[i, k-1]  =  np.mean((Y_test.squeeze() - prediction) ** 2)\n",
    "\n",
    "            # Select k with smalltest average MSE\n",
    "            k  =  cv_res.mean(axis=0).argmin() + 1\n",
    "            \n",
    "    # Return \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate Forecasts\n",
    "cand_forecasts  =  results\n",
    "\n",
    "# Target Variable\n",
    "target_var  =  y_dataset.loc[:, [\"CPIAUCSL_h1\"]]\n",
    "\n",
    "# Get Dates\n",
    "dates  =  cand_forecasts.index.values\n",
    "\n",
    "# Match Time Frames\n",
    "target_var  =  target_var.loc[dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "alpha    =  1\n",
    "n_mods   =  cand_forecasts.shape[1]\n",
    "\n",
    "# Set Vector & Matrices\n",
    "X     =  cand_forecasts.copy()\n",
    "Y     =  target_var.copy()\n",
    "\n",
    "# Set Time Parameter\n",
    "init    =  int(12 * 5)\n",
    "final   =  len(Y)\n",
    "\n",
    "# Set up Empty Array\n",
    "predictions  =  np.zeros(final)\n",
    "predictions.fill(np.nan)\n",
    "\n",
    "# Loop\n",
    "for t in tqdm(range(init, final)):\n",
    "    \n",
    "    # Train Data\n",
    "    X_train  =  X.iloc[:t, ]\n",
    "    Y_train  =  Y.iloc[:t, ]\n",
    "    \n",
    "    # Prediction Data\n",
    "    X_pred   =  X.iloc[t, ]\n",
    "    \n",
    "    # Cross-Validation\n",
    "    splits   =  5\n",
    "    n_times  =  500\n",
    "    k_seq    =  [1, 2, 3, 4, 5]\n",
    "    k_opt    =  cv_ts(Y_train, X_train, splits, k_seq, alpha, n_times)\n",
    "    \n",
    "    # Prediction \n",
    "    n_times  =  500\n",
    "    predictions[t]  =  bssf(Y_train, X_train, X_pred, alpha, k_opt, n_times)[0]\n",
    "\n",
    "# Convert to Pandas Series\n",
    "predictions  =  pd.Series(predictions, name = \"qubo\", index = Y.index).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Target Variable\n",
    "target_var      =  pyreadr.read_r(path + '/Data/Results/Target_Var/target_var.RDS')[None]\n",
    "\n",
    "# Load all Candidate Forecasts\n",
    "cand_forecasts  =  pd.DataFrame()\n",
    "files           =  os.scandir(path + '/Data/Results/Candidate_Forecasts')\n",
    "\n",
    "# Loop\n",
    "for file in files:\n",
    "    if (file.path.endswith(\".RDS\")):\n",
    "        aux  =  pyreadr.read_r(file)[None]\n",
    "        cand_forecasts  =  pd.concat([cand_forecasts, aux], axis = 1)\n",
    "        \n",
    "# Drop Na\n",
    "cand_forecasts  =  cand_forecasts.dropna()\n",
    "\n",
    "# Get Dates\n",
    "dates           =  cand_forecasts.index.values\n",
    "\n",
    "# Match Time Frames\n",
    "target_var      =  target_var.loc[dates]\n",
    "\n",
    "# Dimensions\n",
    "print(cand_forecasts.shape)\n",
    "print(target_var.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, X_train, X_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    X_train  =  X_train / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(X_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ X_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat[0])\n",
    "    Q         =  - 2 * diag_mat + X_train.transpose() @ X_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    # Initialize BQM\n",
    "    bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    bqm  =  bqm.from_qubo(Q)\n",
    "    \n",
    "    # Solve\n",
    "    solver     =  SimulatedAnnealingSampler()\n",
    "    #solver     =  SteepestDescentSolver()\n",
    "    #solver     =  TabuSampler()\n",
    "    #solver     =  TreeDecompositionSolver()\n",
    "\n",
    "    sampleset  =  solver.sample(bqm, num_reads = n_times)\n",
    "    solution   =  list(sampleset.first[0].values())\n",
    "    \n",
    "    # Prediction \n",
    "    pred       = solution @ X_pred\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series-Cross-Validation to Tune Parameter k\n",
    "def cv_ts(Y, X, splits, k_seq, alpha, n_times):\n",
    "\n",
    "    # Set up TS-Split\n",
    "    tscv    =  TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Set up Result-Matrix\n",
    "    cv_res  =  np.zeros((splits, len(k_seq)))\n",
    "\n",
    "    # Loop over Train-Test-Splits\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        # Split Data\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index],   Y.iloc[test_index]\n",
    "\n",
    "        # Cross-Validation k\n",
    "        for k in k_seq:\n",
    "\n",
    "            # Selected Predictors\n",
    "            solution    =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[1]\n",
    "\n",
    "            # Prediction\n",
    "            prediction  =  solution @ X_test.transpose()\n",
    "\n",
    "            # MSE\n",
    "            cv_res[i, k-1]  =  np.mean((Y_test.squeeze() - prediction) ** 2)\n",
    "\n",
    "            # Select k with smalltest average MSE\n",
    "            k  =  cv_res.mean(axis=0).argmin() + 1\n",
    "            \n",
    "    # Return \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "alpha    =  1\n",
    "n_mods   =  cand_forecasts.shape[1]\n",
    "\n",
    "# Set Vector & Matrices\n",
    "X     =  cand_forecasts.copy()\n",
    "Y     =  target_var.copy()\n",
    "\n",
    "# Set Time Parameter\n",
    "init    =  int(12 * 5)\n",
    "final   =  len(Y)\n",
    "\n",
    "# Set up Empty Array\n",
    "predictions  =  np.zeros(final)\n",
    "predictions.fill(np.nan)\n",
    "\n",
    "# Loop\n",
    "for t in range(init, final):\n",
    "    \n",
    "    # Train Data\n",
    "    X_train  =  X.iloc[:t, ]\n",
    "    Y_train  =  Y.iloc[:t, ]\n",
    "    \n",
    "    # Prediction Data\n",
    "    X_pred   =  X.iloc[t, ]\n",
    "    \n",
    "    # Cross-Validation\n",
    "    splits   =  5\n",
    "    n_times  =  500\n",
    "    k_seq    =  [1, 2, 3, 4, 5]\n",
    "    k        =  cv_ts(Y_train, X_train, splits, k_seq, alpha, n_times)\n",
    "    \n",
    "    # Prediction \n",
    "    n_times  =  500\n",
    "    predictions[t]  =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[0]\n",
    "\n",
    "# Convert to Pandas Series\n",
    "predictions  =  pd.Series(predictions, name = \"qubo\", index = Y.index).to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Eval-Function (OOS-R2)\n",
    "def oos_r2(observation, prediction, benchmark):\n",
    "    \n",
    "    # Squared Error Model\n",
    "    se1  =  (observation - prediction) ** 2\n",
    "    \n",
    "    # Squared Error Benchmark\n",
    "    se2  =  (observation - benchmark) ** 2\n",
    "    \n",
    "    # Out-of-Sample R2\n",
    "    oos_r2  =  (1 - sum(se1) / sum(se2)) * 100\n",
    "    \n",
    "    # Return \n",
    "    return(oos_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OOS-Period\n",
    "eval_start  =  \"1974-12-01\"\n",
    "eval_end    =  \"2020-12-01\"\n",
    "\n",
    "# Keep only OOS-Period\n",
    "oos_target_var      =  target_var.loc[eval_start:eval_end].squeeze()\n",
    "oos_benchmark       =  cand_forecasts[eval_start:eval_end][\"pred_hist_mean\"]\n",
    "oos_bssf            =  predictions.loc[eval_start:eval_end].squeeze()\n",
    "oos_cand_forecasts  =  cand_forecasts[eval_start:eval_end]\n",
    "\n",
    "# Evaluate Best Subset Selection of Forecasts\n",
    "oos_r2(oos_target_var, oos_bssf, oos_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Candidate Models\n",
    "eval_cand_mods  =  oos_cand_forecasts.apply(lambda x: oos_r2(oos_target_var, x, oos_benchmark), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cand_mods.sort_values(ascending = False).to_frame(\"OOS-R2\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OOS-Period\n",
    "eval_start  =  \"1974-12-01\"\n",
    "eval_end    =  \"2020-12-01\"\n",
    "\n",
    "# Keep only OOS-Period\n",
    "oos_target_var      =  target_var.loc[eval_start:eval_end].squeeze()\n",
    "oos_benchmark       =  cand_forecasts[eval_start:eval_end][\"pred_hist_mean\"]\n",
    "oos_cand_forecasts  =  cand_forecasts[eval_start:eval_end]\n",
    "\n",
    "# Evaluate Candidate Models\n",
    "eval_cand_mods  =  oos_cand_forecasts.apply(lambda x: oos_r2(oos_target_var, x, oos_benchmark), axis = 0)\n",
    "\n",
    "# Show Results\n",
    "print(eval_cand_mods.filter(like = \"XGB\"))\n",
    "print(eval_cand_mods.filter(like = \"GBM\"))\n",
    "print(eval_cand_mods.filter(like = \"pcr\"))\n",
    "print(eval_cand_mods.filter(like = \"glm\"), n =)\n",
    "#eval_cand_mods.sort_values(ascending = False).to_frame(\"OOS-R2\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BQM\n",
    "qubo  =  { (i,j) : Q.iloc[i,j] for i in range(0, 11) for j in range(0, 11) }\n",
    "bqm   =  BinaryQuadraticModel('BINARY')\n",
    "bqm   =  bqm.from_qubo(Q)\n",
    "\n",
    "# Initialize BQM\n",
    "bqm = BinaryQuadraticModel('BINARY')\n",
    "\n",
    "# Add Linear Coefficients\n",
    "for i in range(0,11):\n",
    "    lin_coef  =  Q.iloc[i,i]\n",
    "    bqm.add_linear(i, lin_coef)\n",
    "    \n",
    "# Add Quadratic Coefficients\n",
    "for i in range(0,11):\n",
    "    for j in range(0,11):\n",
    "        if i != j:\n",
    "            quad_coef  =  Q.iloc[i,j]\n",
    "            bqm.add_quadratic(i, j, quad_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
