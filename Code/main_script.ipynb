{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2025-11-24\n"
     ]
    }
   ],
   "source": [
    "# Load Packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings \n",
    "\n",
    "import time\n",
    "\n",
    "#import multiprocess as mp\n",
    "from multiprocess import Pool\n",
    "\n",
    "#from functools import partial  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import chain\n",
    "from itertools import product\n",
    "\n",
    "from statsmodels.tsa.tsatools import lagmat\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import random_projection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#from sklearn.linear_model import LarsCV\n",
    "from sklearn.linear_model import Lars\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LassoCV\n",
    "#from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from dimod import BinaryQuadraticModel\n",
    "from dwave.samplers import SimulatedAnnealingSampler\n",
    "#from dwave.samplers import SteepestDescentSolver\n",
    "#from dwave.preprocessing import roof_duality\n",
    "# from dwave.system import LeapHybridSampler\n",
    "# from dwave.system import DWaveSampler\n",
    "# from dwave.system import EmbeddingComposite\n",
    "# from dimod import ExactSolver\n",
    "# from dwave.samplers import TabuSampler\n",
    "# from dwave.samplers import TreeDecompositionSolver\n",
    "\n",
    "import gurobipy as gp\n",
    "from gurobi_optimods.qubo import solve_qubo\n",
    "gp.setParam('OutputFlag', 0)\n",
    "\n",
    "if os.name == 'nt':\n",
    "    import dill\n",
    "    dill.settings['recurse'] = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Path\n",
    "path  =  os.path.dirname(os.getcwd()) # os.path.dirname(os.getcwd()) #r'/Users/slehmann/Library/CloudStorage/Dropbox/QUBO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Data\n",
    "def sim_data(N_obs, n_obs, n_preds, non_zero, p, rho, scenario, snr, random_state):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to simulate data for Monte Carlo Study according to Fan and Lv (2008)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set Seed\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Simulating nonzero betas\n",
    "    b_nonzero = (-1) ** np.random.binomial(1, p, non_zero) * ((4 * np.log(n_obs) / np.sqrt(n_obs)) + abs(np.random.standard_normal(non_zero)))\n",
    "    \n",
    "    # Simulate all Betas\n",
    "    b = np.append(b_nonzero, np.repeat(0, n_preds - non_zero))\n",
    "    \n",
    "    ## Shuffle\n",
    "    #random.shuffle(b)\n",
    "    \n",
    "    # Indice of actice Betas\n",
    "    active = np.where(b != 0)[0]\n",
    "    \n",
    "    # Simulate Covariance Matrix\n",
    "    if scenario == 1:\n",
    "        \n",
    "        # Set Covariance Matrix between Predictors\n",
    "        cov_mat = np.full((n_preds, n_preds), rho)\n",
    "        np.fill_diagonal(cov_mat, 1.0)\n",
    "        \n",
    "    if scenario == 2:\n",
    "        \n",
    "        # Set Covariance Matrix between Predictors\n",
    "        cov_mat = np.zeros((n_preds, n_preds))\n",
    "        cov_mat[:non_zero, :non_zero] = rho\n",
    "        np.fill_diagonal(cov_mat, 1.0)\n",
    "\n",
    "    # Simulate Predictor-Time-Series\n",
    "    X = np.random.multivariate_normal([0.0]*n_preds, cov_mat, N_obs)\n",
    "    pred_names = \"X\"+pd.Series(range(1, n_preds+1)).astype(str) \n",
    "    \n",
    "    # Simulate Noise\n",
    "    adj   =  np.sqrt((b.transpose() @ cov_mat @ b) / snr)\n",
    "    error =  np.random.standard_normal(N_obs)   \n",
    "\n",
    "    # Set Target Variable\n",
    "    y = X @ b + adj * error\n",
    "    \n",
    "    # Return\n",
    "    return(y, X, pred_names, adj * error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Pre-Process Data\n",
    "def prepro(X_train, X_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to add intercept and to standardize data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize Data\n",
    "    scaler  =  StandardScaler()   \n",
    "    X_train =  scaler.fit_transform(X_train)\n",
    "    X_pred  =  scaler.transform(X_pred)\n",
    "    \n",
    "    ## Add Constant\n",
    "    X_train =  sm.add_constant(X_train)\n",
    "    X_pred  =  sm.add_constant(X_pred, has_constant = 'add')\n",
    "    \n",
    "    return X_train, X_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Candiate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Complete) Subset Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return array of all subsets of length k\n",
    "def complete_sub(arr, k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Elements are treated as unique based on their position, not on their value.\n",
    "    So if the input elements are unique, there will be no repeated values in each combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all subsets of size k\n",
    "    subset = list(combinations(arr, k)) \n",
    "    \n",
    "    # Return \n",
    "    return subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate number of models\n",
    "def n_models(K, k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate the number of models\n",
    "    \"\"\"\n",
    "    \n",
    "    return math.factorial(K) / (math.factorial(k) * math.factorial(K-k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly select n_max items from array\n",
    "def random_select(arr, n_max, ran_st):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to randomly select n_max items from array\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set random state\n",
    "    random.seed(ran_st)\n",
    "    \n",
    "    # Set upper Boundary\n",
    "    upper_bound  =  len(arr) if len(arr) < n_max else n_max\n",
    "    \n",
    "    # Randomly select items without repetition\n",
    "    rand_arr  =  random.sample(arr, k = upper_bound)\n",
    "    \n",
    "    # Return \n",
    "    return rand_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to produce Subset Regression Forecasts\n",
    "def ssf(y_train, X_train, X_pred, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to fit and predict subset regression models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Subset Feature Space (incl. constant)\n",
    "    X_train_subset = X_train[:, list(range(0, 1)) + list(feature)]\n",
    "    X_pred_subset  = X_pred[:, list(range(0, 1)) + list(feature)]\n",
    "    \n",
    "    # Fit Model\n",
    "    model =  sm.OLS(y_train, X_train_subset) \n",
    "    regr  =  model.fit() \n",
    "    \n",
    "    # Predict\n",
    "    pred = regr.predict(X_pred_subset)\n",
    "    \n",
    "    return(pred, regr.params[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compressed Regression (Gaussian random projection)\n",
    "def cr_reg(y_train, X_train, X_pred, n_comp, ran_st):\n",
    "    \n",
    "    # Set up Random-Projection-Matrix\n",
    "    projector = random_projection.GaussianRandomProjection(n_components = n_comp, random_state = ran_st)\n",
    "    \n",
    "    # Transform\n",
    "    X_train_proj =  projector.fit_transform(X_train[:, 1:])\n",
    "    X_pred_proj  =  projector.fit_transform(X_pred[:,  1:])\n",
    "\n",
    "    # Add Constant to Projected Data\n",
    "    rp_train =  np.concatenate([X_train[:, :1], X_train_proj], axis = 1)\n",
    "    rp_pred  =  np.concatenate([X_pred[:,  :1], X_pred_proj],  axis = 1)\n",
    "\n",
    "    # Fit Model\n",
    "    model  =  sm.OLS(y_train, rp_train) \n",
    "    regr   =  model.fit() \n",
    "    \n",
    "    # Predict\n",
    "    pred = regr.predict(rp_pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression\n",
    "def dt_reg(y_train, X_train, X_pred, ran_st):\n",
    "    \n",
    "    # Set up Regressor Object \n",
    "    model = DecisionTreeRegressor(criterion = \"squared_error\",\n",
    "                                  max_depth = 20,\n",
    "                                  splitter  = \"random\",\n",
    "                                  random_state = ran_st)\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(X_pred)\n",
    "\n",
    "    # Return Prediction\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Candidate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Candidate Models in one Function\n",
    "def candidate_models_n(y_train_, y_pred_, X_train_, X_pred_, k_range_, cr_range_, rep_range_):\n",
    "    \n",
    "    ### Pre-Process Data ###\n",
    "    X_train, X_pred = prepro(X_train_, X_pred_)\n",
    "    \n",
    "    ### Subset Forecasts ###\n",
    "    if np.sum(k_range_) == 0:\n",
    "        preds_ssf = np.empty((0, X_pred.shape[0]), float) \n",
    "    else: \n",
    "        feature_set       = list(chain(*list(map(lambda k: complete_sub(list(range(1, X_train.shape[1])), k), k_range_))))\n",
    "        preds_ssf, coeffs = zip(*list(map(lambda feature: ssf(y_train_, X_train, X_pred, feature), feature_set)))\n",
    "    \n",
    "    ### Compressed Regressions ###\n",
    "    if (np.sum(cr_range_) == 0) or (np.sum(rep_range_) == 0):\n",
    "        preds_cr = np.empty((0, X_pred.shape[0]), float) \n",
    "    else:\n",
    "        preds_cr = np.array(list(map(lambda z: cr_reg(y_train_, X_train, X_pred, z[0], z[1]), product(cr_range_, rep_range_))))\n",
    "            \n",
    "    ### Concatenate Predictions ###\n",
    "    cand_forecasts = np.concatenate([preds_ssf, preds_cr])\n",
    "    \n",
    "    ### Transpose\n",
    "    cand_forecasts = cand_forecasts.transpose()\n",
    "    \n",
    "    return y_pred_, cand_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Candidate Models in Parallel\n",
    "def candidate_models(y, X, kfolds, krepeats, k_range, cr_range, rep_range, n_core, ran_st):\n",
    "    \n",
    "    # Set up Repeated K-Fold\n",
    "    rkf = RepeatedKFold(n_splits=kfolds, n_repeats=krepeats, random_state=ran_st)\n",
    "    \n",
    "    ### Create Candidate Models ###\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        y_preds, cand_forecasts = zip(*list(pool_.map(lambda idx: candidate_models_n(y[idx[0]], y[idx[1]], X[idx[0]], X[idx[1]], k_range, cr_range, rep_range), rkf.split(X, y))))\n",
    "        pool_.close()\n",
    "        \n",
    "    ### Concatenate Predictions ###\n",
    "    cand_forecasts = np.concatenate(cand_forecasts)\n",
    "    \n",
    "    ### Concatenate Targets ###\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "        \n",
    "    # Return\n",
    "    return y_preds, cand_forecasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Benchmark Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Angle Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and predict LARS-Models\n",
    "def lars_n(y_train, cf_train, cf_pred, n):    \n",
    "        \n",
    "    # Define Model\n",
    "    model = Lars(fit_intercept = True,\n",
    "                 fit_path = False,\n",
    "                 jitter = None,\n",
    "                 n_nonzero_coefs = n,\n",
    "                 random_state = 123)\n",
    "\n",
    "    # Fit Model\n",
    "    model.fit(cf_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(cf_pred)\n",
    "    \n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform LARS in Parallel\n",
    "def lars(y_train, cf_train, cf_pred, n_range, n_core):\n",
    "    \n",
    "    # Parallelize over Subset Size\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        predictions = np.array(pool_.map(lambda n: lars_n(y_train, cf_train, cf_pred, n), n_range))\n",
    "        pool_.close()\n",
    "\n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stepwise Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Forward Stepwise Selection\n",
    "def fss_n(y_train, cf_train, cf_pred, n):\n",
    "    \n",
    "        # Model\n",
    "        model = LinearRegression()\n",
    "    \n",
    "        # Sequential Forward Selection\n",
    "        sfs = SequentialFeatureSelector(model,\n",
    "                                        n_features_to_select = n,\n",
    "                                        direction = 'forward')\n",
    "    \n",
    "        # Select Features\n",
    "        active_set = sfs.fit(cf_train, y_train).get_support()\n",
    "    \n",
    "        # Fit Model\n",
    "        model.fit(cf_train[:, active_set], y_train)\n",
    "    \n",
    "        # Predict\n",
    "        pred = model.predict(cf_pred[:, active_set])\n",
    "        \n",
    "        # Return\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Forward Stepwise Selection in Parallel\n",
    "def fss(y_train, cf_train, cf_pred, n_range, n_core):\n",
    "    \n",
    "    # Parallelize over Subset Size\n",
    "    if __name__ == '__main__':\n",
    "        pool_ = Pool(n_core)\n",
    "        predictions = np.array(pool_.map(lambda n: fss_n(y_train, cf_train, cf_pred, n), n_range))\n",
    "        pool_.close()\n",
    "\n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially-Egalitarian Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fit and predict peLASSO-Models\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "def peLASSO(y_train, cf_train, cf_pred, n_alpha, n_iter, cv_splits, cv_repeats):\n",
    "    \n",
    "    ### Step 1: Select to zero\n",
    "    # Define Cross-Validation Method\n",
    "    cv = RepeatedKFold(n_splits = cv_splits,\n",
    "                       n_repeats = cv_repeats,\n",
    "                       random_state = 123)\n",
    "    \n",
    "    # Define Model\n",
    "    model_lasso = LassoCV(fit_intercept = True,\n",
    "                          n_alphas = n_alpha,\n",
    "                          max_iter = n_iter,\n",
    "                          cv = cv,\n",
    "                          verbose = 0,\n",
    "                          n_jobs=1)\n",
    "    \n",
    "    # Fit Model\n",
    "    model_lasso.fit(cf_train, y_train)\n",
    "    \n",
    "    # Get & select only active candidate models\n",
    "    active_cf_train = cf_train[:, model_lasso.coef_.astype(bool)]\n",
    "    active_cf_pred  = cf_pred[: , model_lasso.coef_.astype(bool)]\n",
    "\n",
    "    ### Step 2: Shrink towards equality\n",
    "    # Check if active candidate models exist\n",
    "    if active_cf_train.shape[1] > 0:\n",
    "        \n",
    "        mean_cf = active_cf_train.mean(axis = 1)\n",
    "    \n",
    "        # Define Cross-Validation Method\n",
    "        cv = RepeatedKFold(n_splits = cv_splits,\n",
    "                           n_repeats = cv_repeats,\n",
    "                           random_state = 1)\n",
    "    \n",
    "        # Define Model\n",
    "        model_elasso = LassoCV(fit_intercept = True,\n",
    "                               n_alphas = n_alpha,\n",
    "                               max_iter = n_iter,\n",
    "                               cv = cv,\n",
    "                               verbose = 0,\n",
    "                               n_jobs=1)\n",
    "    \n",
    "        # Fit Model\n",
    "        model_elasso.fit(active_cf_train, (y_train-mean_cf))\n",
    "        \n",
    "        # Coefficients\n",
    "        coefs = model_elasso.coef_ + 1.0 / active_cf_train.shape[1]\n",
    "    \n",
    "        # Predict\n",
    "        pred = active_cf_pred @ coefs\n",
    "            \n",
    "    else:\n",
    "        print(\"peLASSO: No active candidate models\")\n",
    "        \n",
    "        # Set Prediction to mean\n",
    "        pred = [y_train.mean()] * cf_pred.shape[0]\n",
    "        \n",
    "    # Return Prediction\n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate X \n",
    "X = np.random.normal(0, 1, (250, 5))\n",
    "Y = np.sum(X, axis = 1) + np.random.normal(0, 5, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha = .5,\n",
    "              max_iter = 1000,\n",
    "              fit_intercept = False)\n",
    "model.fit(X, Y)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha = 2.0,\n",
    "              max_iter = 1000,\n",
    "              fit_intercept = False)\n",
    "model.fit(X, Y - np.mean(X, axis = 1))\n",
    "model.coef_ + 1.0 / X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average-Best Forecast Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual-based average-best forecast combination\n",
    "def avg_best(y_train, cf_train, cf_pred, n_range):\n",
    "    \n",
    "    # Set up Array\n",
    "    predictions = np.full((len(n_range), cf_pred.shape[0]), np.nan)\n",
    "    \n",
    "    # Calculate Squared Errors\n",
    "    se = ([[value] for value in y_train] - cf_train) ** 2\n",
    "\n",
    "    # Mean-Squared-Error\n",
    "    mse = np.mean(se, axis = 0)\n",
    "\n",
    "    # Get indices of the average-best N candidate models\n",
    "    ind = np.argsort(mse)\n",
    "\n",
    "    # Init Counter\n",
    "    i = 0\n",
    "    \n",
    "    # Loop over Subset Size\n",
    "    for n in n_range:\n",
    "        \n",
    "        # Predict\n",
    "        pred = np.mean(cf_pred[:, ind[:n]], axis = 1)\n",
    "        \n",
    "        # Append Prediction\n",
    "        predictions[i] = pred\n",
    "        \n",
    "        # Update Counter\n",
    "        i += 1\n",
    "        \n",
    "    # Return\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Best Subset Selection of Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(y_train, cf_train, cf_pred, alpha, n_sub, bssf_timeout, method):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    cf_train  =  cf_train / n_sub\n",
    "    cf_pred   =  cf_pred  / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(cf_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(y_train.transpose() @ cf_train + alpha * n_sub)\n",
    "    Q         =  - 2 * np.diag(aux_mat) + cf_train.transpose() @ cf_train + alpha * ivec @ ivec.transpose()\n",
    "    \n",
    "    if method == \"dwave\":\n",
    "\n",
    "        # Initialize BQM\n",
    "        bqm  =  BinaryQuadraticModel('BINARY')\n",
    "        bqm  =  bqm.from_qubo(Q)\n",
    "\n",
    "        # Normalize\n",
    "        bqm.normalize()\n",
    "\n",
    "        # Preprocess (?)\n",
    "        #roof_duality(bqm)    \n",
    "\n",
    "        # Select Solver\n",
    "        solver_qpu  =  SimulatedAnnealingSampler() #LeapHybridSampler() SimulatedAnnealingSampler() EmbeddingComposite(DWaveSampler())\n",
    "        #solver_pp   =  SteepestDescentSolver()    #SteepestDescentSolver()\n",
    "\n",
    "        # Submit for Solution\n",
    "        sampleset  =  solver_qpu.sample(bqm, \n",
    "                                        num_reads = n_times,\n",
    "                                        #time_limit = 90,\n",
    "                                        label = \"Best Subset Selection of Forecasts\",\n",
    "                                        seed = 123) # f'Best Subset Selection of Forecasts{t}'\n",
    "\n",
    "        ## Postprocess Problem\n",
    "        #sampleset_pp = solver_pp.sample(bqm,\n",
    "        #                                initial_states = sampleset.lowest())\n",
    "\n",
    "        # Get Solution\n",
    "        solution    =  np.array(list(sampleset.first[0].values()))\n",
    "    \n",
    "    if method == \"qubo\":\n",
    "    \n",
    "        # Set up Model\n",
    "        model = gp.Model()\n",
    "        model.Params.TimeLimit = bssf_timeout\n",
    "        \n",
    "        # Decision Variables\n",
    "        b = model.addMVar(shape=Q.shape[0], vtype=gp.GRB.BINARY, name=\"b\")\n",
    "        \n",
    "        # Objective Function\n",
    "        model.setObjective(b @ Q @ b, gp.GRB.MINIMIZE)\n",
    "        \n",
    "        # Optimize\n",
    "        model.optimize()\n",
    "        solution = np.array(model.x)\n",
    "        \n",
    "    if method == \"qcbo\":\n",
    "        \n",
    "        # Set up Model\n",
    "        model = gp.Model()\n",
    "        model.params.timelimit = bssf_timeout\n",
    "\n",
    "        # Decision Variables\n",
    "        b = model.addMVar(shape=cf_train.shape[1], vtype=gp.GRB.BINARY, name=\"b\")\n",
    "        norm_0 = model.addVar(lb=n_sub, ub=n_sub, name=\"norm\")\n",
    "\n",
    "        # Objective Function\n",
    "        model.setObjective(b.T @ cf_train.T @ cf_train @ b\n",
    "                           - 2*y_train.T @ cf_train @ b\n",
    "                           + np.dot(y_train, y_train), gp.GRB.MINIMIZE)\n",
    "\n",
    "        # L0-Norm Constraint\n",
    "        model.addGenConstrNorm(norm_0, b, which=0, name=\"budget\")\n",
    "\n",
    "        # Optimize\n",
    "        model.optimize()\n",
    "        solution = np.array(model.x)[:-1]\n",
    "    \n",
    "    # Test Solution\n",
    "    if np.sum(solution) != n_sub:\n",
    "        print(f\"Warning: Number of selected features does not match --- {np.sum(solution)} instead of {n_sub}!\")\n",
    "    \n",
    "    # Prediction \n",
    "    pred = solution @ cf_pred.transpose()\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran_st = 0\n",
    "### Simulate Data ###\n",
    "y, X, pred_names, error = sim_data(n_obs, train, n_preds, 2, bernoulli_p, 0.2, 1, 0.3, 0)\n",
    "\n",
    "### Split in Train and Test Data ###\n",
    "y_train, y_pred, X_train, X_pred = y[:train].copy(), y[train:].copy(), X[:train].copy(), X[train:].copy()\n",
    "### Create Train Candidate Models ###\n",
    "targets_train, cf_train = candidate_models(y_train, X_train, kfolds, krepeats, k_range_ssf, k_range_cr, rep_range_cr, n_core, ran_st)\n",
    "    \n",
    "### Create Test Candidate Models ###\n",
    "targets_test, cf_test = candidate_models_n(y_train, y_pred, X_train, X_pred, k_range_ssf, k_range_cr, rep_range_cr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MC-Parameter ######\n",
    "# Number of MC-Runs\n",
    "n_mc   =  10\n",
    "n_core =  6\n",
    "\n",
    "# Set Parameter\n",
    "n_obs       =  1100\n",
    "train       =  100\n",
    "n_preds     =  10\n",
    "b_range     =  [2, 5, 8]\n",
    "bernoulli_p =  0.2\n",
    "corr_range  =  [0.2, 0.5, 0.8]\n",
    "scenario_range = [1, 2]\n",
    "snr_range   =  [0.1, 0.3, 0.5]  # --> PVE = SNR / (1 + SNR)\n",
    "\n",
    "### Cross Validation ###\n",
    "kfolds   = 5\n",
    "krepeats = 1\n",
    "\n",
    "###### Parameter Subset Forecasts ######\n",
    "# Subset Lengths\n",
    "k_range_ssf = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  \n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "if np.sum(k_range_ssf) == 0:\n",
    "    n_sub  =  0\n",
    "else:\n",
    "    n_sub  =  int(sum([min(n_models(n_preds, k), n_max) for k in k_range_ssf]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "k_range_cr  =  [0] #[1, 2, 3, 4] \n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range_cr  =  [0] #range(0, 60) \n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "if (np.sum(k_range_cr) == 0) or (len(rep_range_cr) == 0):\n",
    "    n_cr = 0\n",
    "else: \n",
    "    n_cr  =  len(k_range_cr) * len(rep_range_cr)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) \n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter LARS ######\n",
    "k_range_lars = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "###### Parameter Forward Stepwise Selection ######\n",
    "k_range_fss = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  ### -> klein machen\n",
    "\n",
    "###### Parameter peLasso ######\n",
    "n_alpha        = 200\n",
    "n_iter_peL     = 1000\n",
    "cv_splits_peL  = 5\n",
    "cv_repeats_peL = 1\n",
    "\n",
    "###### Parameter Average-Best ######\n",
    "k_range_avg_best = [1, 5, 10, 25, 50, 100, 250, 500, 750, 1023] \n",
    "\n",
    "###### Parameter BSSF ######\n",
    "bssf_alpha   =  2000\n",
    "bssf_timeout =  1.0\n",
    "k_range_bssf =  [1, 5, 10, 25, 50, 100, 250, 500, 750, 1023] \n",
    "n_bssf       =  len(k_range_bssf)\n",
    "bssf_method  =  \"qubo\"\n",
    "\n",
    "######## Objects ########\n",
    "# Set up Matrices for Results\n",
    "cf_train = np.full((train, (n_sub + n_cr)), np.nan)  \n",
    "cf_test  = np.full((n_obs - train, (n_sub + n_cr)), np.nan)  \n",
    "\n",
    "benchmark        = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc), np.nan) ####\n",
    "se_benchmark     = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "\n",
    "cf_weights       = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, (n_sub + n_cr)), np.nan)\n",
    "bssf_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, n_obs - train), np.nan)\n",
    "se_bssf_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_bssf, n_obs - train), np.nan)\n",
    "\n",
    "csr_forecast     = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_ssf), n_obs - train), np.nan)\n",
    "se_csr_forecast  = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_ssf), n_obs - train), np.nan)\n",
    "\n",
    "lars_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_lars), n_obs - train), np.nan)\n",
    "se_lars_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_lars), n_obs - train), np.nan)\n",
    "\n",
    "fss_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_fss), n_obs - train), np.nan)\n",
    "se_fss_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_fss), n_obs - train), np.nan)\n",
    "\n",
    "pelasso_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "se_pelasso_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, n_obs - train), np.nan)\n",
    "\n",
    "avg_best_forecast    = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_avg_best), n_obs - train), np.nan)\n",
    "se_avg_best_forecast = np.full((len(scenario_range), len(snr_range), len(corr_range), len(b_range), n_mc, len(k_range_avg_best), n_obs - train), np.nan)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Monte-Carlo Runs\n",
    "program_starts = time.time()\n",
    "for r, ran_st in enumerate(tqdm(range(n_mc))):\n",
    "    \n",
    "    # Loop over Covariance-Scenario\n",
    "    for c, sce in enumerate(scenario_range):\n",
    "        \n",
    "        # Loop over Signal-to-Noise-Ratio\n",
    "        for s, snr in enumerate(snr_range):\n",
    "    \n",
    "            # Loop over Covariance-Sets\n",
    "            for p, rho in enumerate(corr_range):\n",
    "        \n",
    "                # Loop over Coefficient-Sets\n",
    "                for b, beta in enumerate(b_range):\n",
    "    \n",
    "                    ### Simulate Data ###\n",
    "                    y, X, pred_names = sim_data(n_obs, train, n_preds, beta, bernoulli_p, rho, sce, snr, ran_st)\n",
    "                    \n",
    "                    ### Split in Train and Test Data ###\n",
    "                    y_train, y_pred, X_train, X_pred = y[:train].copy(), y[train:].copy(), X[:train].copy(), X[train:].copy()\n",
    "\n",
    "                    ### Create Train Candidate Models ###\n",
    "                    targets_train, cf_train = candidate_models(y_train, X_train, kfolds, krepeats, k_range_ssf, k_range_cr, rep_range_cr, n_core, ran_st)\n",
    "                        \n",
    "                    ### Create Test Candidate Models ###\n",
    "                    targets_test, cf_test = candidate_models_n(y_train, y_pred, X_train, X_pred, k_range_ssf, k_range_cr, rep_range_cr) \n",
    "\n",
    "                    ### Benchmark: PHM ###\n",
    "                    benchmark[c][s][p][b][r] =  targets_train.mean()\n",
    "                    se_benchmark[c][s][p][b][r] = (targets_test - targets_train.mean()) ** 2\n",
    "\n",
    "                    ### Benchmark: Complete Subset Regression ###\n",
    "                    idx_ss = np.cumsum([0] + [len(list(combinations(range(n_preds), k))) for k in k_range_ssf])\n",
    "                    csr_forecast[c][s][p][b][r] = [np.mean(cf_test[:, :n_sub][:, idx_ss[i]:idx_ss[i+1]], axis = 1) for i in range(len(idx_ss)-1)]\n",
    "                    se_csr_forecast[c][s][p][b][r] = (targets_test - csr_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: LARS ###\n",
    "                    lars_forecast[c][s][p][b][r] = lars(targets_train, cf_train, cf_test, k_range_lars)\n",
    "                    se_lars_forecast[c][s][p][b][r] = (targets_test - lars_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: Forward Stepwise Selection ###\n",
    "                    fss_forecast[c][s][p][b][r] = fss(targets_train, cf_train, cf_test, k_range_fss, n_core)\n",
    "                    se_fss_forecast[c][s][p][b][r] = (targets_test - fss_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Benchmark: peLASSO ###\n",
    "                    pelasso_forecast[c][s][p][b][r] = peLASSO(targets_train, cf_train, cf_test, n_alpha, n_iter_peL, cv_splits_peL, cv_repeats_peL)\n",
    "                    se_pelasso_forecast[c][s][p][b][r] = (targets_test - pelasso_forecast[c][s][p][b][r]) ** 2                    \n",
    "\n",
    "                    ### Benchmark: Average-Best ###\n",
    "                    avg_best_forecast[c][s][p][b][r] = avg_best(targets_train, cf_train, cf_test, k_range_avg_best)\n",
    "                    se_avg_best_forecast[c][s][p][b][r] = (targets_test - avg_best_forecast[c][s][p][b][r]) ** 2\n",
    "\n",
    "                    ### Best Selection of Forecast ###\n",
    "                    bssf_forecast[c][s][p][b][r], cf_weights[c][s][p][b][r] = zip(*list(map(lambda k_bssf: bssf(targets_train, cf_train, cf_test, bssf_alpha, k_bssf, bssf_timeout, bssf_method), k_range_bssf)))\n",
    "                    se_bssf_forecast[c][s][p][b][r] = (targets_test - bssf_forecast[c][s][p][b][r]) ** 2\n",
    "    \n",
    "    ### Save \n",
    "    # ...\n",
    "    \n",
    "# Time                \n",
    "program_ends = time.time()\n",
    "         \n",
    "# Loop over all combinations\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        for s in range(len(snr_range)):\n",
    "            for c in range(len(scenario_range)):\n",
    "                \n",
    "\n",
    "                # Calculate Forecast Combination Method Performances  \n",
    "                mse_bssf = np.sum(np.sum(se_bssf_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_csr  = np.sum(np.sum( se_csr_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_lars = np.sum(np.sum(se_lars_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_fss       =  np.sum(np.sum(se_fss_forecast[c][s][p][b], axis = 2), axis = 0)\n",
    "                mse_pelasso   =  np.sum(np.sum(se_pelasso_forecast[c][s][p][b], axis = 1), axis = 0)\n",
    "                mse_avg_best  =  np.sum(np.sum(se_avg_best_forecast[c][s][p][b], axis = 2), axis = 0)                \n",
    "\n",
    "                # Calculate Benchmark Performance  \n",
    "                mse_phm  =  np.sum(np.sum(se_benchmark[c][s][p][b], axis = 1), axis = 0)\n",
    "\n",
    "                # Create Rows\n",
    "                new_row_bbsf = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"BSSF\",       \"OOS-R2\": np.array2string(100 * (1 - mse_bssf / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_csr  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"CSR\",        \"OOS-R2\": np.array2string(100 * (1 - mse_csr  / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_lars = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"LARS\",       \"OOS-R2\": np.array2string(100 * (1 - mse_lars / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_fss  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"FSS\",        \"OOS-R2\": np.array2string(100 * (1 - mse_fss  / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_peL  = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"peLASSO\",    \"OOS-R2\": np.array2string(100 * (1 - mse_pelasso / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "                new_row_avgB = { \"n_mc\": n_mc, \"n_obs\": n_obs, \"n_preds\": n_preds, \"train\": train, \"rho\": corr_range[p], \"scenario\": scenario_range[c], \"bernoulli_p\": bernoulli_p, \"snr\": snr_range[s], \"b\": b_range[b], \"k_range_ssf\": k_range_ssf, \"k_range_cr\": k_range_cr, \"rep_range_cr\": rep_range_cr, \"k_range_lars\": k_range_lars, \"k_range_fss\": k_range_fss, \"n_alpha\": n_alpha, \"n_iter_peL\": n_iter_peL, \"cv_splits_peL\": cv_splits_peL, \"cv_repeats_peL\": cv_repeats_peL, \"k_range_avg_best\": k_range_avg_best, \"bssf_alpha\": bssf_alpha, \"bssf_timeout\": bssf_timeout, \"bssf_range\": k_range_bssf, \"error\": 0, \"time\" : np.floor((program_ends - program_starts) / 60), \"type\": \"Avg_Best_N\", \"OOS-R2\": np.array2string(100 * (1 - mse_avg_best / mse_phm), formatter={'float_kind':'{0:.2f}'.format}).replace('[', '').replace(']', '')}\n",
    "\n",
    "                # Add Rows\n",
    "                new_rows = pd.DataFrame.from_dict([new_row_bbsf,\n",
    "                                                   new_row_csr,\n",
    "                                                   new_row_lars,\n",
    "                                                   new_row_fss,\n",
    "                                                   new_row_peL,\n",
    "                                                   new_row_avgB\n",
    "                                                   ])\n",
    "                \n",
    "                ## Add to CSV\n",
    "                #output_path = path + \"/Results/my_csv.csv\"\n",
    "                #new_rows.to_csv(output_path, mode='a', header=not os.path.exists(output_path))\n",
    "                \n",
    "                # Print Results\n",
    "                print(f\"BSSF:       Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_bssf / mse_phm), 2)) + \"%\")\n",
    "                print(f\"CSR:        Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_csr  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"LARS:       Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_lars / mse_phm), 2)) + \"%\")\n",
    "                print(f\"FSS:        Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_fss  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"peLASSO:    Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_pelasso  / mse_phm), 2)) + \"%\")\n",
    "                print(f\"Avg-Best N: Avg. OOS-R2 for Scenario {c}, SNR {snr_range[s]}, Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - mse_avg_best / mse_phm), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up DataFrame\n",
    "dataframe_plot = pd.DataFrame()\n",
    "\n",
    "# Create DataFrame\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        # Subset Weights\n",
    "        chosen_cm = cf_weights[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "        # Aggregated\n",
    "        chosen_cm = np.sum(chosen_cm, axis = 0)\n",
    "\n",
    "        # Replace Zero with NaN\n",
    "        chosen_cm[chosen_cm == 0] = np.nan\n",
    "        chosen_cm_agg = chosen_cm.copy()\n",
    "\n",
    "        # Set to One\n",
    "        chosen_cm[chosen_cm > 1 ] = 1\n",
    "\n",
    "        # Adapt Column-Values\n",
    "        chosen_cm = chosen_cm * np.arange(1, chosen_cm.shape[1]+1)\n",
    "\n",
    "        # Create DataFrame\n",
    "        chosen_cm = pd.DataFrame(chosen_cm, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "        chosen_cm_agg = pd.DataFrame(chosen_cm_agg, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "\n",
    "        # Index\n",
    "        chosen_cm = chosen_cm.reset_index(names=\"INDEX\")\n",
    "        chosen_cm_agg = chosen_cm_agg.reset_index(names=\"INDEX\")\n",
    "\n",
    "        # Melt Data\n",
    "        chosen_cm = chosen_cm.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Value')\n",
    "        chosen_cm_agg = chosen_cm_agg.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Weight')\n",
    "\n",
    "        # Concatenate\n",
    "        chosen_cm = pd.concat([chosen_cm, chosen_cm_agg])\n",
    "        \n",
    "        # Transform Candidate Model Names\n",
    "        chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.split('_').str[0]\n",
    "        \n",
    "        # Add Column for Correlation\n",
    "        chosen_cm['Correlation'] = corr_range[p] \n",
    "        \n",
    "        # Add Column for Betas\n",
    "        chosen_cm['Betas'] = str(b_range[b])\n",
    "        \n",
    "        # Append\n",
    "        dataframe_plot = pd.concat([dataframe_plot, chosen_cm])\n",
    "        \n",
    "# Plot Theme\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Plot Data\n",
    "g = sns.relplot(\n",
    "    data = dataframe_plot,\n",
    "    y = 'Value', x = 'INDEX',\n",
    "    hue = \"Candidate_Model\", size = \"Weight\", \n",
    "    col = \"Betas\", row = \"Correlation\",\n",
    "    sizes = (3, 75), height = 5.0, aspect = 1.5,\n",
    "    alpha = 0.75, palette = \"muted\",\n",
    "    #legend = True\n",
    "    )\n",
    "\n",
    "# Legend\n",
    "#g._legend.remove()\n",
    "#h, l = g.ax.get_legend_handles_labels()\n",
    "#g.ax.legend(h[0:13], l[0:13], bbox_to_anchor=(1.0, 0.75), loc=2, fontsize=10, frameon=False)\n",
    "\n",
    "# Title & Axis\n",
    "g.set(xlabel='Combination Size',\n",
    "      ylabel='Candidate Models')\n",
    "      #title=f\"Selected Candidate Models: \\n Correlation {corr_range[p]} & Betas {b_range[b]}\")\n",
    "\n",
    "# Margins\n",
    "#g.ax.margins(x = 0.05, y = 0)\n",
    "\n",
    "# Set number of ticks for y-axis\n",
    "idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "#g.ax.set_yticks(idx)\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "#g.ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "## iterate over axes of FacetGrid\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_yticks(idx)\n",
    "    ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "# Tick-Label Size\n",
    "g.set_yticklabels(size = 8)\n",
    "g.set_xticklabels(size = 10)\n",
    "\n",
    "# Add Horizontal Lines\n",
    "#for i in idx:\n",
    "#    g.ax.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "#for label in g.ax.get_xticklabels():\n",
    "#    label.set_rotation(90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        # Subset Weights\n",
    "        chosen_cm = cf_weights[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "        # Aggregated\n",
    "        chosen_cm = np.sum(chosen_cm, axis = 0)\n",
    "\n",
    "        # Replace Zero with NaN\n",
    "        chosen_cm[chosen_cm == 0] = np.nan\n",
    "        chosen_cm_agg = chosen_cm.copy()\n",
    "\n",
    "        # Set to One\n",
    "        chosen_cm[chosen_cm > 1 ] = 1\n",
    "\n",
    "        # Adapt Column-Values\n",
    "        chosen_cm = chosen_cm * np.arange(1, chosen_cm.shape[1]+1)\n",
    "\n",
    "        # Create DataFrame\n",
    "        chosen_cm = pd.DataFrame(chosen_cm, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "        chosen_cm_agg = pd.DataFrame(chosen_cm_agg, index = [f\"K={k}\" for k in bssf_range], columns = [*ssf_names, *cr_names])\n",
    "\n",
    "        # Index\n",
    "        chosen_cm = chosen_cm.reset_index(names=\"INDEX\")\n",
    "        chosen_cm_agg = chosen_cm_agg.reset_index(names=\"INDEX\")\n",
    "\n",
    "        # Melt Data\n",
    "        chosen_cm = chosen_cm.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Value')\n",
    "        chosen_cm_agg = chosen_cm_agg.melt(id_vars = 'INDEX', var_name = 'Candidate_Model', value_name = 'Weight')\n",
    "\n",
    "        # Concatenate\n",
    "        chosen_cm = pd.concat([chosen_cm, chosen_cm_agg])\n",
    "\n",
    "        # Transform Candidate Model Names\n",
    "        chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.split('_').str[0]\n",
    "        #chosen_cm['Candidate_Model'] = chosen_cm['Candidate_Model'].str.startswith(\"SSF\").replace({True: \"SSF\", False: \"CR\"})\n",
    "\n",
    "        # Plot Theme\n",
    "        sns.set_theme(style=\"ticks\")\n",
    "\n",
    "        # Draw each cell as a scatter point with varying size and color\n",
    "        g = sns.relplot(\n",
    "            data = chosen_cm,\n",
    "            y = 'Value', x = 'INDEX', hue = \"Candidate_Model\", \n",
    "            size = \"Weight\", sizes = (3, 75),\n",
    "            height = 6.5, alpha = 0.75, palette=\"muted\",\n",
    "            #legend = True\n",
    "            )\n",
    "\n",
    "        # Legend\n",
    "        g._legend.remove()\n",
    "        h, l = g.ax.get_legend_handles_labels()\n",
    "        g.ax.legend(h[0:13], l[0:13], bbox_to_anchor=(1.0, 0.75), loc=2, fontsize=10, frameon=False)\n",
    "\n",
    "        # Title & Axis\n",
    "        g.set(xlabel='Combination Size',\n",
    "              ylabel='Candidate Models',\n",
    "              title=f\"Selected Candidate Models: \\n Correlation {corr_range[p]} & Betas {b_range[b]}\")\n",
    "\n",
    "        # Margins\n",
    "        g.ax.margins(x = 0.05, y = 0)\n",
    "\n",
    "        # Set number of ticks for y-axis\n",
    "        idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "        g.ax.set_yticks(idx)\n",
    "\n",
    "        # Set ticks labels for x-axis\n",
    "        g.ax.set_yticklabels([string.split(\"_\")[0] for string in [[[*ssf_names, *cr_names]][0][i] for i in idx]], rotation='horizontal')\n",
    "\n",
    "        # Tick-Label Size\n",
    "        g.set_yticklabels(size = 8)\n",
    "        g.set_xticklabels(size = 10)\n",
    "\n",
    "        # Add Horizontal Lines\n",
    "        #for i in idx:\n",
    "        #    g.ax.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "        # Rotate x-axis labels\n",
    "        for label in g.ax.get_xticklabels():\n",
    "            label.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Plot\n",
    "fig, ax = plt.subplots(1,1) \n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "# Plot \n",
    "colors = {'SSF':'blue', 'CR':'green'}\n",
    "ax.scatter(x = chosen_cm['INDEX'], y = chosen_cm['Weight'], c=chosen_cm['Candidate_Model'].map(colors), s = 1)\n",
    "#ax.plot(chosen_cm['INDEX'], chosen_cm.iloc[:, 1:], marker = \"o\", lw = 0, ms = 4)\n",
    "\n",
    "# Add title and axis names\n",
    "plt.title('Selected Candidate Models')\n",
    "plt.ylabel('Candidate Models')\n",
    "plt.xlabel('Combination Size')\n",
    "\n",
    "# Legend\n",
    "#plt.legend(loc = \"upper right\")\n",
    "\n",
    "# Margins\n",
    "plt.margins(x=0.10, y=0)\n",
    "\n",
    "# Set number of ticks for x-axis\n",
    "idx = [0, 8, 36, 92, 162, 218, 246, 255, 315, 375, 435]\n",
    "ax.set_yticks(idx)\n",
    "\n",
    "# Set ticks labels for x-axis\n",
    "ax.set_yticklabels([[[*ssf_names, *cr_names]][0][i] for i in idx], rotation='horizontal', fontsize=6)\n",
    "\n",
    "# Add horizontal lines\n",
    "for i in idx:\n",
    "    plt.axhline(y=i, color='r', linestyle='--', lw = 0.2)\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot: Welches K hat was ausgewählt\n",
    "### Benchmark: Complete Subset Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is:\" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is:\" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = benchmark[np.arange(0, n_mc * len(corr_range) * len(b_range), len(b_range))]\n",
    "b2 = benchmark[np.arange(1, n_mc * len(corr_range) * len(b_range), len(b_range))]\n",
    "\n",
    "b1_p1 = b1[np.arange(0, n_mc * len(corr_range), len(corr_range))]\n",
    "b1_p2 = b1[np.arange(1, n_mc * len(corr_range), len(corr_range))]\n",
    "b1_p3 = b1[np.arange(2, n_mc * len(corr_range), len(corr_range))]\n",
    "\n",
    "b2_p1 = b2[np.arange(0, n_mc * len(corr_range), len(corr_range))]\n",
    "b2_p2 = b2[np.arange(1, n_mc * len(corr_range), len(corr_range))]\n",
    "b2_p3 = b2[np.arange(2, n_mc * len(corr_range), len(corr_range))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Goyal Welch Data\n",
    "data  =  pd.read_csv(path + r'/Data/PredictorData2022.xlsx - Quarterly.csv', thousands=',')\n",
    "\n",
    "# Equity Premium\n",
    "data['equity_premium'] = data['CRSP_SPvw'] - data['Rfree']\n",
    "\n",
    "# Dividend Price Ratio \n",
    "data['dp'] = np.log(data['D12']) - np.log(data['Index'])\n",
    "\n",
    "# Dividend Yield \n",
    "data['dy'] = np.log(data['D12'])- np.log(data['Index'].shift(1))\n",
    "\n",
    "# Earnings Price Ratio \n",
    "data['ep'] = np.log(data['E12']) - np.log(data['Index'])\n",
    "\n",
    "# Dividend Payout Ratio \n",
    "data['dpayr'] = np.log(data['D12']) - np.log(data['E12'])\n",
    "\n",
    "# Book to Market Ratio\n",
    "data['bmr'] = data['b/m']\n",
    "\n",
    "# # Net Equity Expansion\n",
    "data['ntis'] = data['ntis']\n",
    "\n",
    "# Treasury Bill Rate\n",
    "data['tbl'] = data['tbl']\n",
    "\n",
    "# Long Term Rate\n",
    "data['ltr'] = data['ltr']\n",
    "\n",
    "# Term Spread \n",
    "data['tsp'] = data['lty'] - data['tbl']\n",
    "\n",
    "# Default Return Spread \n",
    "data['dfr'] = data['corpr'] - data['ltr']\n",
    "\n",
    "# Inflation\n",
    "data['infl'] = data['infl']\n",
    "\n",
    "# Investment of Capital Ratio\n",
    "data['ik']  = data['ik']\n",
    "\n",
    "# Default Yield Spread\n",
    "data['dfy'] = data['BAA'] - data['AAA']\n",
    "\n",
    "# Realized Volatility\n",
    "data['rvol'] = data['svar']\n",
    "\n",
    "# reorganize the dataframe\n",
    "data = data[['yyyyq', \"equity_premium\", \"dp\", \"dy\", \"ep\", \"dpayr\", \"bmr\", \"ntis\", \"tbl\", \"ltr\", \"tsp\", \"dfr\", \"infl\", \"ik\"]]\n",
    "\n",
    "# Convert Date\n",
    "data['yyyyq'] = data['yyyyq'].astype(str)\n",
    "data['yyyyq'] = data.apply(lambda x: x['yyyyq'][:4]+'-Q'+x['yyyyq'][4:], axis=1)\n",
    "data['yyyyq'] = pd.to_datetime(data['yyyyq'])\n",
    "\n",
    "# Resetting the index\n",
    "data.set_index('yyyyq', inplace=True)\n",
    "data.index = data.index.to_period('Q')\n",
    "\n",
    "# Lag all Predictors\n",
    "data.iloc[:,1:]  =  data.iloc[:,1:].shift(1)\n",
    "\n",
    "# Drop Na\n",
    "data = data.loc[\"1946Q1\":, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Set Seed ######\n",
    "#random.seed(123)\n",
    "\n",
    "###### Data ######\n",
    "# Set Target Variable\n",
    "y  =  data.loc[:, [\"equity_premium\"]]\n",
    "X  =  data.drop(\"equity_premium\", axis = 1)\n",
    "\n",
    "# Get Predictor Names\n",
    "pred_names = list(X.columns)\n",
    "\n",
    "# Number of AR-Terms to include\n",
    "mlags =  2\n",
    "\n",
    "# Create Lags\n",
    "X  =  create_lags(y, X, mlags)\n",
    "\n",
    "# Drop Missing Values\n",
    "y  =  y.loc[\"1947Q2\":, ] #y[mlags:]\n",
    "X  =  X.loc[\"1947Q2\":, ] #X[mlags:]\n",
    "\n",
    "# Check NA\n",
    "any(X.isna().any())\n",
    "\n",
    "###### Parameter Subset Forecasts\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  # 20000\n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "n_sub  =  int(sum([min(n_models((X.shape[1]-mlags), k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2, 3] # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 100) # 10000\n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) # 300000\n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  0.5\n",
    "n_times     =  50\n",
    "bssf_range  =  [1, 2, 3] # range(1, 5)\n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "### General Parameter ######\n",
    "# Initial Training-Period\n",
    "init       =  4 * 50 #4 * 10\n",
    "\n",
    "# Total Length\n",
    "total =  len(y) \n",
    "\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts  =  np.full((total, (n_sub + n_cr)), np.nan)     #np.full((total, (n_sub + n_cr + n_dt)), np.nan)\n",
    "cf_weights      =  np.full((total, (n_sub + n_cr)), np.nan)\n",
    "benchmark       =  np.full(total, np.nan)\n",
    "bssf_forecast   =  np.full(total, np.nan)\n",
    "bssf_opt        =  np.full(total, np.nan)\n",
    "sse_bssf        =  np.zeros(n_bssf)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Time\n",
    "for t in tqdm(range(init, total)):\n",
    "        \n",
    "    # Pre-Process Data\n",
    "    y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "    \n",
    "    ### Benchmark: AR(X) Model\n",
    "    pred          =  ar_mod(y_train, lags = mlags)\n",
    "    benchmark[t]  =  pred.iloc[0]\n",
    "    \n",
    "    ### Subset Forecasts\n",
    "    # Set up List to store Subset-Forecasts\n",
    "    preds_ssf =  np.full(n_sub, np.nan)\n",
    "    idx_sub   =  0\n",
    "    \n",
    "    # Loop over Subset Size \n",
    "    for k in k_range:\n",
    "    \n",
    "        # Get all possible Subset of length k\n",
    "        col_idx   =  list(range(mlags+1, X_train.shape[1]))\n",
    "        subs_idx  =  complete_sub(col_idx, k)\n",
    "\n",
    "        # Randomly select n_upper Subsets\n",
    "        feature_set  =  subs_idx #random_select(subs_idx, n_max, random_state = 123)\n",
    "\n",
    "        # Loop over Subsets\n",
    "        for feature in feature_set:\n",
    "\n",
    "            # Compute Subset-Regression-Forecast\n",
    "            pred  =  ssf(y_train, X_train, X_pred, feature, mlags)\n",
    "            preds_ssf[idx_sub] = pred\n",
    "            idx_sub += 1\n",
    "            \n",
    "    ### Compressed Regressions\n",
    "    # Set up List to store Compressed-Regression-Forecasts\n",
    "    preds_cr = np.full(n_cr, np.nan)\n",
    "    idx_cr   = 0\n",
    "    \n",
    "    # Loop over number of Components\n",
    "    for n_comp in cr_range:\n",
    "\n",
    "        # Loop over n repetitions\n",
    "        for r in rep_range:\n",
    "        \n",
    "            # Compute Compressed-Regression-Forecasts\n",
    "            pred  =  cr_reg(y_train, X_train, X_pred, n_comp, mlags, r)\n",
    "            preds_cr[idx_cr] = pred\n",
    "            idx_cr += 1\n",
    "            \n",
    "    # ### Decision Tree Regressions\n",
    "    # # Set up Matrix to store Decision-Tree-Forecasts\n",
    "    # preds_dt   = np.full(n_dt, np.nan)\n",
    "    # \n",
    "    # # Loop over number of Components\n",
    "    # for idx_dt, r in enumerate(dt_range):\n",
    "    #     \n",
    "    #     # Compute Decision-Tree-Forecasts\n",
    "    #     pred  =  dt_reg(y_train, X_train, X_pred, r)\n",
    "    #     preds_dt[idx_dt] = pred[0]\n",
    "\n",
    "    # Append Results\n",
    "    cand_forecasts[t][:n_sub]             =  preds_ssf \n",
    "    cand_forecasts[t][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "    #cand_forecasts[t][(n_sub+n_cr):]      =  preds_dt\n",
    "\n",
    "    ### Best Selection of Forecast\n",
    "    if t > init:\n",
    "    \n",
    "        # Set up Matrix to store Forecasts\n",
    "        bssf_forecasts  =  np.full(n_bssf, np.nan)\n",
    "        bssf_weights    =  np.zeros([n_bssf, n_sub + n_cr]) \n",
    "           \n",
    "        # Get \"best\" Subset-Size until now (lowest Sum of Squared Errors)\n",
    "        s_opt  =  np.argmin(sse_bssf)\n",
    "    \n",
    "        # Loop over Subset Sizes\n",
    "        for idx_bssf, s in enumerate(bssf_range):\n",
    "    \n",
    "            # Compute Best-Subset-Selection-of-Forecasts\n",
    "            pred  =  bssf(y_train[init:], cand_forecasts[init:t], cand_forecasts[t], alpha, s, n_times)\n",
    "            bssf_forecasts[idx_bssf]  =  pred[0]\n",
    "            bssf_weights[idx_bssf]    =  pred[1]\n",
    "    \n",
    "            # Compute Sum of Squared Errors\n",
    "            sse_bssf[idx_bssf] =  sse_bssf[idx_bssf] + (y_pred.iloc[0,0] - pred[0]) ** 2\n",
    "    \n",
    "        # Select Forecast \n",
    "        bssf_forecast[t] =  bssf_forecasts[s_opt]\n",
    "        cf_weights[t]    =  bssf_weights[s_opt]\n",
    "        bssf_opt[t]      =  bssf_range[s_opt]\n",
    "        \n",
    "# Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(len(pred_names)), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "#dt_names = [f\"DT_{idx_dt}\" for idx_dt in dt_range]\n",
    "        \n",
    "# Convert Results to DataFrame\n",
    "cand_forecasts  =  pd.DataFrame(cand_forecasts, index = y.index, columns = [*ssf_names, *cr_names]) #, *dt_names])\n",
    "benchmark       =  pd.DataFrame(benchmark,      index = y.index, columns = [\"AR\"])\n",
    "bssf_forecast   =  pd.DataFrame(bssf_forecast,  index = y.index, columns = [\"BSSF\"])\n",
    "cf_weights      =  pd.DataFrame(cf_weights,     index = y.index, columns = [*ssf_names, *cr_names]) #, *dt_names])\n",
    "bssf_opt        =  pd.DataFrame(bssf_opt,     index = y.index, columns = [\"Subset_Size\"])\n",
    "\n",
    "# Cut off initial Training-Period\n",
    "sub_y              =  y.iloc[init:].copy()\n",
    "sub_cand_forecasts =  cand_forecasts.iloc[init:].copy()\n",
    "sub_benchmark      =  benchmark.iloc[init:].copy()\n",
    "sub_bssf_forecast  =  bssf_forecast.iloc[init:].copy()\n",
    "sub_cf_weights     =  cf_weights.iloc[init:].copy()\n",
    "sub_bssf_opt       =  bssf_opt.iloc[init:].copy()\n",
    "\n",
    "# OOS-Period\n",
    "oos_start  =  \"1999Q4\"\n",
    "oos_end    =  \"2022Q4\" \n",
    "oos_y             =  sub_y.loc[oos_start:oos_end].copy()\n",
    "oos_cand_forecast =  sub_cand_forecasts.loc[oos_start:oos_end].copy()\n",
    "oos_benchmark     =  sub_benchmark.loc[oos_start:oos_end].copy()\n",
    "oos_bssf_forecast =  sub_bssf_forecast.loc[oos_start:oos_end].copy()\n",
    "oos_cf_weights    =  sub_cf_weights.loc[oos_start:oos_end].copy()\n",
    "oos_bssf_opt      =  sub_bssf_opt.loc[oos_start:oos_end].copy()\n",
    "\n",
    "# Evaluation\n",
    "np.sum((oos_y.iloc[:,0] - oos_bssf_forecast.iloc[:,0]) ** 2) / np.sum((oos_y.iloc[:,0] - oos_benchmark.iloc[:,0]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Zero with NaN\n",
    "oos_cf_weights.replace({0:np.nan}, inplace=True)\n",
    "\n",
    "# Adapt Column-Values\n",
    "vec = list(range(1, oos_cf_weights.shape[1]+1))\n",
    "tmp = oos_cf_weights * vec\n",
    "\n",
    "# Dates\n",
    "tmp = tmp.reset_index(names=\"date\")\n",
    "\n",
    "# Plot \n",
    "# tmp.plot(x='date', y = tmp.columns[1:],\n",
    "#          figsize=(10, 5), legend=False,\n",
    "#          marker=\"o\", ms = 1, \n",
    "#          title=\"Selected Candidate Models\", ylabel=\"Selected Candidate Models\")\n",
    "#          #yticks = (np.arange(98), list(tmp.columns[1:])))\n",
    "\n",
    "tmp_long = pd.melt(tmp, id_vars = \"date\")\n",
    "tmp_long['variable'] = tmp_long['variable'].str.startswith(\"SSF\").replace({True: \"SSF\", False: \"CR\"})\n",
    "tmp_long.set_index(\"date\", inplace = True)\n",
    "tmp_long.groupby(\"variable\")[\"value\"].plot(legend=True, figsize = (10, 5),\n",
    "                                           marker=\"o\", ms = 2, lw = 0,\n",
    "                                           ylim = [-1, cand_forecasts.shape[1]+1],\n",
    "                                           title=\"Selected Candidate Models\", ylabel=\"Selected Candidate Models\")\n",
    "plt.show()\n",
    "\n",
    "# Subset Size\n",
    "oos_bssf_opt.plot(figsize=(10, 5), legend=False, \n",
    "                  color = \"black\", marker=\"o\", ms = 1, lw = 0,\n",
    "                  title = \"Subset Size\", xlabel=\"date\", ylabel=\"Selected Subset Size\",\n",
    "                  ylim  =  [min(bssf_range)-0.5, max(bssf_range)+0.5],\n",
    "                  yticks = np.arange(min(bssf_range), max(bssf_range)+1, step=1.0))\n",
    "plt.show()\n",
    "\n",
    "# CSSED\n",
    "cssed = np.cumsum(((oos_y.iloc[:,0] - oos_benchmark.iloc[:,0]) ** 2) - ((oos_y.iloc[:,0] - oos_bssf_forecast.iloc[:,0]) ** 2))\n",
    "cssed.plot(figsize=(10, 5),\n",
    "            xlabel = \"date\", ylabel = \"CSSED\", title = \"Cumulated Sum of Squared Error Differences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### MC-Parameter ######\n",
    "# Number of MC-Runs\n",
    "n_mc  =  2\n",
    "\n",
    "# Set Parameter\n",
    "n_obs       =  100\n",
    "n_preds     =  8\n",
    "init        =  50\n",
    "mlags       =  0\n",
    "corr_range  =  [0.5, 0.95]\n",
    "b_range     =  np.array([[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "###### Parameter Subset Forecasts ######\n",
    "# Subset Lengths\n",
    "k_range = [1, 2, 3, 4, 5, 6, 7, 8] \n",
    "\n",
    "# Upper Bound for Subset-Sizes\n",
    "n_max = 10000  \n",
    "\n",
    "# Number of Subset-Forecasts\n",
    "n_sub  =  int(sum([min(n_models(n_preds, k), n_max) for k in k_range]))\n",
    "\n",
    "###### Parameter Compressed Regressions ######\n",
    "# Number of Components for Compressed Regression\n",
    "cr_range  =  [1, 2] \n",
    "\n",
    "# Number of runs for each random projection\n",
    "rep_range  =  range(0, 50) \n",
    "\n",
    "# Number of Compressed-Regression-Forecasts\n",
    "n_cr  =  len(cr_range) * len(rep_range)\n",
    "\n",
    "# ###### Parameter Decision Tree ######\n",
    "# dt_range  =  range(0, 5) \n",
    "# \n",
    "# # Number of Decision Trees\n",
    "# n_dt  =  len(dt_range)\n",
    "\n",
    "###### Parameter BSSF ######\n",
    "alpha       =  10.0\n",
    "n_times     =  1\n",
    "bssf_range  =  [1, 2, 3] \n",
    "n_bssf      =  len(bssf_range)\n",
    "\n",
    "######## Objects ########\n",
    "# Set up Matrices for Results\n",
    "cand_forecasts   = np.full((len(range(init, n_obs)), (n_sub + n_cr)), np.nan)     \n",
    "benchmark        = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "cf_weights       = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf, (n_sub + n_cr)), np.nan)\n",
    "bssf_forecast    = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "csr_forecast     = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "se_benchmark     = np.full( len(corr_range) * len(b_range) * n_mc, np.nan)\n",
    "se_bssf_forecast = np.full((len(corr_range) * len(b_range) * n_mc, n_bssf), np.nan)\n",
    "se_csr_forecast  = np.full((len(corr_range) * len(b_range) * n_mc, len(k_range)), np.nan)\n",
    "\n",
    "###### Start ######\n",
    "# Loop over Monte-Carlo Runs\n",
    "i = 0\n",
    "for r in tqdm(range(n_mc)):\n",
    "    \n",
    "    # Loop over Covariance-Sets\n",
    "    for p in corr_range:\n",
    "        \n",
    "        # Loop over Coefficient-Sets\n",
    "        for b in b_range:\n",
    "    \n",
    "            ### Simulate Data ###\n",
    "            y, X, pred_names = sim(n_obs, n_preds, b, p, r)\n",
    "            \n",
    "            ### Benchmark: PHM ###\n",
    "            benchmark[i]    = y.iloc[:-1].mean().iloc[0]\n",
    "            se_benchmark[i] = (y.iloc[-1,0] - benchmark[i]) ** 2\n",
    "\n",
    "            # Loop over t / Create Candidate Models\n",
    "            for t in range(init, n_obs):\n",
    "            \n",
    "                ### Pre-Process Data ###\n",
    "                y_train, X_train, y_pred, X_pred = prepro(y, X, t)\n",
    "\n",
    "                ### Subset Forecasts ###\n",
    "                feature_set  =  list(chain(*list(map(lambda k: complete_sub(list(range(1, X_train.shape[1])), k), k_range))))\n",
    "                preds_ssf    =  np.array(list(map(lambda feature: ssf(y_train, X_train, X_pred, feature, 0), feature_set)))\n",
    "            \n",
    "                ## Set up List to store Subset-Forecasts\n",
    "                #preds_ssf = np.full(n_sub, np.nan)\n",
    "                #idx_sub   = 0\n",
    "                #\n",
    "                ## Loop over Subset Size \n",
    "                #for k in k_range:\n",
    "                #\n",
    "                #    # Get all possible Subsets of length k\n",
    "                #    col_idx  = list(range(1, X_train.shape[1]))\n",
    "                #    subs_idx = complete_sub(col_idx, k)\n",
    "                #\n",
    "                #    # Randomly select n_upper Subsets\n",
    "                #    feature_set  =  subs_idx #random_select(subs_idx, n_max, random_state = 123)\n",
    "                #\n",
    "                #    # Loop over Subsets\n",
    "                #    for feature in feature_set:\n",
    "                #    \n",
    "                #        # Compute Subset-Regression-Forecast\n",
    "                #        pred  =  ssf(y_train, X_train, X_pred, feature, 0)\n",
    "                #        preds_ssf[idx_sub] = pred\n",
    "                #        idx_sub += 1\n",
    "\n",
    "                ### Compressed Regressions ###\n",
    "                preds_cr = np.array(list(chain(*[list(map(lambda rep: cr_reg(y_train, X_train, X_pred, n_comp, 0, rep), rep_range)) for n_comp in cr_range])))\n",
    "                \n",
    "                # # Set up List to store Compressed-Regression-Forecasts\n",
    "                # preds_cr   = np.full(n_cr, np.nan)\n",
    "                # idx_cr     = 0\n",
    "                # \n",
    "                # # Loop over number of Components\n",
    "                # for n_comp in cr_range:\n",
    "                # \n",
    "                #     # Loop over n repetitions\n",
    "                #     for rep in rep_range:\n",
    "                #     \n",
    "                #         # Compute Compressed-Regression-Forecasts\n",
    "                #         pred  =  cr_reg(y_train, X_train, X_pred, n_comp, 0, rep)\n",
    "                #         preds_cr[idx_cr] = pred\n",
    "                #         idx_cr += 1\n",
    "\n",
    "                # ### Decision Tree Regressions\n",
    "                # preds_dt = np.array(list(map(lambda r: dt_reg(y_train, X_train, X_pred, r), dt_range)))\n",
    "                \n",
    "                # # Set up Matrix to store Decision-Tree-Forecasts\n",
    "                # preds_dt   = np.full(n_dt, np.nan)\n",
    "                # \n",
    "                # # Loop over number of Components\n",
    "                # for idx_dt, r in enumerate(dt_range):\n",
    "                #     \n",
    "                #     # Compute Decision-Tree-Forecasts\n",
    "                #     pred  =  dt_reg(y_train, X_train, X_pred, r)\n",
    "                #     preds_dt[idx_dt] = pred[0]\n",
    "\n",
    "                # Append Results\n",
    "                cand_forecasts[t-init][:n_sub]             =  preds_ssf \n",
    "                cand_forecasts[t-init][n_sub:(n_sub+n_cr)] =  preds_cr\n",
    "                #cand_forecasts[t-init][(n_sub+n_cr):]     =  preds_dt\n",
    "                \n",
    "            ### Benchmark: Complete Subset Regression ###\n",
    "            tmp_ = np.cumsum([0] + [len(list(combinations(range(n_preds), k))) for k in k_range])\n",
    "            csr_forecast[i]     =  [np.mean(preds_ssf[tmp_[i]:tmp_[i+1]]) for i in range(len(tmp_)-1)]\n",
    "            se_csr_forecast[i]  =  (y_pred.iloc[0,0] - csr_forecast[i]) ** 2\n",
    "\n",
    "            ### Best Selection of Forecast ###\n",
    "            bssf_forecast[i], cf_weights[i] = zip(*list(map(lambda s: bssf(y_train[init:], cand_forecasts[:-1], cand_forecasts[-1], alpha, s, n_times), bssf_range)))\n",
    "            se_bssf_forecast[i] = (y_pred.values[0] - bssf_forecast[i]) ** 2\n",
    "            \n",
    "            # # Set up Matrix to store Forecasts\n",
    "            # kssf_forecasts  =  np.full(n_bssf, np.nan)\n",
    "            # kssf_weights    =  np.zeros([n_bssf, n_sub + n_cr]) \n",
    "\n",
    "            # # Loop over Subset Sizes\n",
    "            # for idx_bssf, s in enumerate(bssf_range):\n",
    "            #     \n",
    "            #     # Compute Best-Subset-Selection-of-Forecasts\n",
    "            #     pred = bssf(y_train[init:], cand_forecasts[:-1], cand_forecasts[-1], alpha, s, n_times)\n",
    "            #     bssf_forecast[i][idx_bssf] = pred[0]\n",
    "            #     cf_weights[i][idx_bssf]    = pred[1]\n",
    "            #     se_bssf_forecast[i][idx_bssf] = (y_pred.iloc[0,0] - pred[0]) ** 2\n",
    "             \n",
    "            # Update index   \n",
    "            i += 1\n",
    "            \n",
    "# # Candidate-Model-Names\n",
    "ssf_names = [f\"SSF{k}_\" + \"_\".join(map(str, sub)) for k in k_range for sub in combinations(range(n_preds), k)]\n",
    "cr_names  = [f\"CR{n_comp}_{r}\" for n_comp in cr_range for r in rep_range]\n",
    "\n",
    "### Evaluation ###\n",
    "for b in range(len(b_range)):\n",
    "    for p in range(len(corr_range)):\n",
    "        \n",
    "        se_bssf = se_bssf_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_csr  = se_csr_forecast[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        se_phm  = se_benchmark[np.arange(b, n_mc * len(corr_range) * len(b_range), len(b_range))][np.arange(p, n_mc * len(corr_range), len(corr_range))]\n",
    "        \n",
    "        print(f\"BSSF: Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_bssf) / sum(se_phm)), 2)) + \"%\")\n",
    "        print(f\"CSR:  Avg. OOS-R2 for Corr. {corr_range[p]} and Betas {b_range[b]} is: \" + str(np.round(100 * (1 - sum(se_csr) / sum(se_phm)), 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Fred-MD-Data stationary\n",
    "def transform_tcode(data):\n",
    "    \n",
    "    # Get Transformation-Code\n",
    "    tcode = data[0]\n",
    "    \n",
    "    # Get Data\n",
    "    data = data[1:]\n",
    "\n",
    "    if tcode == 1:\n",
    "        output = data\n",
    "    elif tcode == 2:\n",
    "        output = data - np.roll(data, 1)\n",
    "    elif tcode == 3:\n",
    "        output = (data - np.roll(data, 1)) - (np.roll(data, 1) - np.roll(data, 2))\n",
    "    elif tcode == 4:\n",
    "        output = np.log(data)\n",
    "    elif tcode == 5:\n",
    "        output = np.log(data) - np.roll(np.log(data), 1)\n",
    "    elif tcode == 6:\n",
    "        output = (np.log(data) - np.roll(np.log(data), 1)) - (np.roll(np.log(data), 1) - np.roll(np.log(data), 2))\n",
    "    else:\n",
    "        output = (data / np.roll(data, 1) - 1) - (np.roll(data, 1) / np.roll(data, 2) - 1)\n",
    "\n",
    "    return np.concatenate(([tcode], output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x_dataset  =  pd.read_csv(path + r'/fred_md_202306.csv')\n",
    "\n",
    "# Drop Variables with too many missing values\n",
    "x_dataset  =  x_dataset.drop([\"PERMIT\", \"PERMITNE\", \"PERMITMW\", \"PERMITS\",\n",
    "                              \"PERMITW\", \"ACOGNO\", \"ANDENOx\", \"CP3Mx\",\n",
    "                              \"COMPAPFFx\", \"TWEXAFEGSMTHx\", \"UMCSENTx\", \"VIXCLSx\"],\n",
    "                              axis=1)\n",
    "\n",
    "# Transform remaining Columns\n",
    "x_dataset.iloc[:, 1:]  =  x_dataset.iloc[:,1:].apply(lambda x: transform_tcode(x))\n",
    "\n",
    "# Drop First Row with Transformation-Code\n",
    "x_dataset  =  x_dataset.iloc[1:,:]\n",
    "\n",
    "# Lag Data\n",
    "x_dataset.iloc[:,1:]  =  x_dataset.iloc[:,1:].shift(1)\n",
    "\n",
    "# Convert Date\n",
    "x_dataset['sasdate']  =  pd.to_datetime(x_dataset['sasdate'])\n",
    "\n",
    "# Filter Data\n",
    "x_dataset  =  x_dataset[(x_dataset['sasdate'] >= '1959-04-01') & (x_dataset['sasdate'] <= '2023-03-01')]\n",
    "\n",
    "# Resetting the index\n",
    "x_dataset.set_index('sasdate', inplace=True)\n",
    "x_dataset.index = x_dataset.index.to_period('M')\n",
    "\n",
    "# Load Data\n",
    "y_dataset  =  pd.read_csv(path + r'/fred_md_202306.csv')\n",
    "\n",
    "# Select and Rename Variables\n",
    "y_dataset  =  y_dataset.loc[:, [\"sasdate\", \"CPIAUCSL\", \"INDPRO\", \"UNRATE\"]]\n",
    "y_dataset  =  y_dataset.rename(columns={'CPIAUCSL': 'CPIAUCSL_h1', 'INDPRO': 'INDPRO_h1', 'UNRATE': 'UNRATE_h1'})\n",
    "\n",
    "# Transform Variables\n",
    "y_dataset[[\"CPIAUCSL_h1\"]]  =  y_dataset[[\"CPIAUCSL_h1\"]].apply(lambda x: 4 * 100 * (np.log(x) - np.roll(np.log(x), 1)))\n",
    "y_dataset[[\"INDPRO_h1\"]]    =  y_dataset[[\"INDPRO_h1\"]].apply(lambda x: 4 * 100 * (np.log(x) - np.roll(np.log(x), 1)))\n",
    "y_dataset[[\"UNRATE_h1\"]]    =  y_dataset[[\"UNRATE_h1\"]]\n",
    "\n",
    "# Drop First Row with Transformation-Code\n",
    "y_dataset  =  y_dataset.iloc[1:,:]\n",
    "\n",
    "# Convert Date\n",
    "y_dataset['sasdate']  =  pd.to_datetime(y_dataset['sasdate'])\n",
    "\n",
    "# Filter Data\n",
    "y_dataset  =  y_dataset[(y_dataset['sasdate'] >= '1959-04-01') & (y_dataset['sasdate'] <= '2023-03-01')]\n",
    "\n",
    "# Resetting the index\n",
    "y_dataset.set_index('sasdate', inplace=True)\n",
    "y_dataset.index = y_dataset.index.to_period('M')\n",
    "\n",
    "# Set Target Variable\n",
    "y  =  y_dataset.loc[:, [\"CPIAUCSL_h1\"]]\n",
    "X  =  x_dataset.drop(\"CPIAUCSL\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Selection of Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, X_train, X_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    X_train  =  X_train / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(X_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ X_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat[0])\n",
    "    Q         =  - 2 * diag_mat + X_train.transpose() @ X_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    # Initialize BQM\n",
    "    bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    bqm  =  bqm.from_qubo(Q)\n",
    "    \n",
    "    # Solve\n",
    "    solver     =  SimulatedAnnealingSampler()\n",
    "    #solver     =  SteepestDescentSolver()\n",
    "    #solver     =  TabuSampler()\n",
    "    #solver     =  TreeDecompositionSolver()\n",
    "\n",
    "    sampleset  =  solver.sample(bqm, num_reads = n_times)\n",
    "    solution   =  list(sampleset.first[0].values())\n",
    "    \n",
    "    # Prediction \n",
    "    pred       = solution @ X_pred\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series-Cross-Validation to Tune Parameter k\n",
    "def cv_ts(Y, X, splits, k_seq, alpha, n_times):\n",
    "\n",
    "    # Set up TS-Split\n",
    "    tscv    =  TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Set up Result-Matrix\n",
    "    cv_res  =  np.zeros((splits, len(k_seq)))\n",
    "\n",
    "    # Loop over Train-Test-Splits\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        # Split Data\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index],   Y.iloc[test_index]\n",
    "\n",
    "        # Cross-Validation k\n",
    "        for k in k_seq:\n",
    "\n",
    "            # Selected Predictors\n",
    "            solution    =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[1]\n",
    "\n",
    "            # Prediction\n",
    "            prediction  =  solution @ X_test.transpose()\n",
    "\n",
    "            # MSE\n",
    "            cv_res[i, k-1]  =  np.mean((Y_test.squeeze() - prediction) ** 2)\n",
    "\n",
    "            # Select k with smalltest average MSE\n",
    "            k  =  cv_res.mean(axis=0).argmin() + 1\n",
    "            \n",
    "    # Return \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate Forecasts\n",
    "cand_forecasts  =  results\n",
    "\n",
    "# Target Variable\n",
    "target_var  =  y_dataset.loc[:, [\"CPIAUCSL_h1\"]]\n",
    "\n",
    "# Get Dates\n",
    "dates  =  cand_forecasts.index.values\n",
    "\n",
    "# Match Time Frames\n",
    "target_var  =  target_var.loc[dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "alpha    =  1\n",
    "n_mods   =  cand_forecasts.shape[1]\n",
    "\n",
    "# Set Vector & Matrices\n",
    "X     =  cand_forecasts.copy()\n",
    "Y     =  target_var.copy()\n",
    "\n",
    "# Set Time Parameter\n",
    "init    =  int(12 * 5)\n",
    "final   =  len(Y)\n",
    "\n",
    "# Set up Empty Array\n",
    "predictions  =  np.zeros(final)\n",
    "predictions.fill(np.nan)\n",
    "\n",
    "# Loop\n",
    "for t in tqdm(range(init, final)):\n",
    "    \n",
    "    # Train Data\n",
    "    X_train  =  X.iloc[:t, ]\n",
    "    Y_train  =  Y.iloc[:t, ]\n",
    "    \n",
    "    # Prediction Data\n",
    "    X_pred   =  X.iloc[t, ]\n",
    "    \n",
    "    # Cross-Validation\n",
    "    splits   =  5\n",
    "    n_times  =  500\n",
    "    k_seq    =  [1, 2, 3, 4, 5]\n",
    "    k_opt    =  cv_ts(Y_train, X_train, splits, k_seq, alpha, n_times)\n",
    "    \n",
    "    # Prediction \n",
    "    n_times  =  500\n",
    "    predictions[t]  =  bssf(Y_train, X_train, X_pred, alpha, k_opt, n_times)[0]\n",
    "\n",
    "# Convert to Pandas Series\n",
    "predictions  =  pd.Series(predictions, name = \"qubo\", index = Y.index).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Target Variable\n",
    "target_var      =  pyreadr.read_r(path + '/Data/Results/Target_Var/target_var.RDS')[None]\n",
    "\n",
    "# Load all Candidate Forecasts\n",
    "cand_forecasts  =  pd.DataFrame()\n",
    "files           =  os.scandir(path + '/Data/Results/Candidate_Forecasts')\n",
    "\n",
    "# Loop\n",
    "for file in files:\n",
    "    if (file.path.endswith(\".RDS\")):\n",
    "        aux  =  pyreadr.read_r(file)[None]\n",
    "        cand_forecasts  =  pd.concat([cand_forecasts, aux], axis = 1)\n",
    "        \n",
    "# Drop Na\n",
    "cand_forecasts  =  cand_forecasts.dropna()\n",
    "\n",
    "# Get Dates\n",
    "dates           =  cand_forecasts.index.values\n",
    "\n",
    "# Match Time Frames\n",
    "target_var      =  target_var.loc[dates]\n",
    "\n",
    "# Dimensions\n",
    "print(cand_forecasts.shape)\n",
    "print(target_var.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Q-Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Selection of Forecasts\n",
    "def bssf(Y_train, X_train, X_pred, alpha, n_sub, n_times):\n",
    "    \n",
    "    # Adapt X-Matrix\n",
    "    X_train  =  X_train / n_sub\n",
    "    \n",
    "    # Generate Q-Matrix\n",
    "    ivec      =  np.mat(np.ones(X_train.shape[1])).transpose()\n",
    "    aux_mat   =  np.array(Y_train.transpose() @ X_train + alpha * n_sub)\n",
    "    diag_mat  =  np.diag(aux_mat[0])\n",
    "    Q         =  - 2 * diag_mat + X_train.transpose() @ X_train + alpha * ivec @ ivec.transpose()\n",
    "\n",
    "    # Initialize BQM\n",
    "    bqm  =  BinaryQuadraticModel('BINARY')\n",
    "    bqm  =  bqm.from_qubo(Q)\n",
    "    \n",
    "    # Solve\n",
    "    solver     =  SimulatedAnnealingSampler()\n",
    "    #solver     =  SteepestDescentSolver()\n",
    "    #solver     =  TabuSampler()\n",
    "    #solver     =  TreeDecompositionSolver()\n",
    "\n",
    "    sampleset  =  solver.sample(bqm, num_reads = n_times)\n",
    "    solution   =  list(sampleset.first[0].values())\n",
    "    \n",
    "    # Prediction \n",
    "    pred       = solution @ X_pred\n",
    "    \n",
    "    # Return \n",
    "    return(pred, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-Series-Cross-Validation to Tune Parameter k\n",
    "def cv_ts(Y, X, splits, k_seq, alpha, n_times):\n",
    "\n",
    "    # Set up TS-Split\n",
    "    tscv    =  TimeSeriesSplit(n_splits = splits)\n",
    "    \n",
    "    # Set up Result-Matrix\n",
    "    cv_res  =  np.zeros((splits, len(k_seq)))\n",
    "\n",
    "    # Loop over Train-Test-Splits\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "\n",
    "        # Split Data\n",
    "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
    "        Y_train, Y_test = Y.iloc[train_index],   Y.iloc[test_index]\n",
    "\n",
    "        # Cross-Validation k\n",
    "        for k in k_seq:\n",
    "\n",
    "            # Selected Predictors\n",
    "            solution    =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[1]\n",
    "\n",
    "            # Prediction\n",
    "            prediction  =  solution @ X_test.transpose()\n",
    "\n",
    "            # MSE\n",
    "            cv_res[i, k-1]  =  np.mean((Y_test.squeeze() - prediction) ** 2)\n",
    "\n",
    "            # Select k with smalltest average MSE\n",
    "            k  =  cv_res.mean(axis=0).argmin() + 1\n",
    "            \n",
    "    # Return \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameter\n",
    "alpha    =  1\n",
    "n_mods   =  cand_forecasts.shape[1]\n",
    "\n",
    "# Set Vector & Matrices\n",
    "X     =  cand_forecasts.copy()\n",
    "Y     =  target_var.copy()\n",
    "\n",
    "# Set Time Parameter\n",
    "init    =  int(12 * 5)\n",
    "final   =  len(Y)\n",
    "\n",
    "# Set up Empty Array\n",
    "predictions  =  np.zeros(final)\n",
    "predictions.fill(np.nan)\n",
    "\n",
    "# Loop\n",
    "for t in range(init, final):\n",
    "    \n",
    "    # Train Data\n",
    "    X_train  =  X.iloc[:t, ]\n",
    "    Y_train  =  Y.iloc[:t, ]\n",
    "    \n",
    "    # Prediction Data\n",
    "    X_pred   =  X.iloc[t, ]\n",
    "    \n",
    "    # Cross-Validation\n",
    "    splits   =  5\n",
    "    n_times  =  500\n",
    "    k_seq    =  [1, 2, 3, 4, 5]\n",
    "    k        =  cv_ts(Y_train, X_train, splits, k_seq, alpha, n_times)\n",
    "    \n",
    "    # Prediction \n",
    "    n_times  =  500\n",
    "    predictions[t]  =  bssf(Y_train, X_train, X_pred, alpha, k, n_times)[0]\n",
    "\n",
    "# Convert to Pandas Series\n",
    "predictions  =  pd.Series(predictions, name = \"qubo\", index = Y.index).to_frame()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Eval-Function (OOS-R2)\n",
    "def oos_r2(observation, prediction, benchmark):\n",
    "    \n",
    "    # Squared Error Model\n",
    "    se1  =  (observation - prediction) ** 2\n",
    "    \n",
    "    # Squared Error Benchmark\n",
    "    se2  =  (observation - benchmark) ** 2\n",
    "    \n",
    "    # Out-of-Sample R2\n",
    "    oos_r2  =  (1 - sum(se1) / sum(se2)) * 100\n",
    "    \n",
    "    # Return \n",
    "    return(oos_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OOS-Period\n",
    "eval_start  =  \"1974-12-01\"\n",
    "eval_end    =  \"2020-12-01\"\n",
    "\n",
    "# Keep only OOS-Period\n",
    "oos_target_var      =  target_var.loc[eval_start:eval_end].squeeze()\n",
    "oos_benchmark       =  cand_forecasts[eval_start:eval_end][\"pred_hist_mean\"]\n",
    "oos_bssf            =  predictions.loc[eval_start:eval_end].squeeze()\n",
    "oos_cand_forecasts  =  cand_forecasts[eval_start:eval_end]\n",
    "\n",
    "# Evaluate Best Subset Selection of Forecasts\n",
    "oos_r2(oos_target_var, oos_bssf, oos_benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Candidate Models\n",
    "eval_cand_mods  =  oos_cand_forecasts.apply(lambda x: oos_r2(oos_target_var, x, oos_benchmark), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cand_mods.sort_values(ascending = False).to_frame(\"OOS-R2\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OOS-Period\n",
    "eval_start  =  \"1974-12-01\"\n",
    "eval_end    =  \"2020-12-01\"\n",
    "\n",
    "# Keep only OOS-Period\n",
    "oos_target_var      =  target_var.loc[eval_start:eval_end].squeeze()\n",
    "oos_benchmark       =  cand_forecasts[eval_start:eval_end][\"pred_hist_mean\"]\n",
    "oos_cand_forecasts  =  cand_forecasts[eval_start:eval_end]\n",
    "\n",
    "# Evaluate Candidate Models\n",
    "eval_cand_mods  =  oos_cand_forecasts.apply(lambda x: oos_r2(oos_target_var, x, oos_benchmark), axis = 0)\n",
    "\n",
    "# Show Results\n",
    "print(eval_cand_mods.filter(like = \"XGB\"))\n",
    "print(eval_cand_mods.filter(like = \"GBM\"))\n",
    "print(eval_cand_mods.filter(like = \"pcr\"))\n",
    "print(eval_cand_mods.filter(like = \"glm\"), n =)\n",
    "#eval_cand_mods.sort_values(ascending = False).to_frame(\"OOS-R2\").head(n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BQM\n",
    "qubo  =  { (i,j) : Q.iloc[i,j] for i in range(0, 11) for j in range(0, 11) }\n",
    "bqm   =  BinaryQuadraticModel('BINARY')\n",
    "bqm   =  bqm.from_qubo(Q)\n",
    "\n",
    "# Initialize BQM\n",
    "bqm = BinaryQuadraticModel('BINARY')\n",
    "\n",
    "# Add Linear Coefficients\n",
    "for i in range(0,11):\n",
    "    lin_coef  =  Q.iloc[i,i]\n",
    "    bqm.add_linear(i, lin_coef)\n",
    "    \n",
    "# Add Quadratic Coefficients\n",
    "for i in range(0,11):\n",
    "    for j in range(0,11):\n",
    "        if i != j:\n",
    "            quad_coef  =  Q.iloc[i,j]\n",
    "            bqm.add_quadratic(i, j, quad_coef)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
